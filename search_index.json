[["index.html", "Tidy Modeling with R Book Club Welcome", " Tidy Modeling with R Book Club The R4DS Online Learning Community 2023-09-05 Welcome This is a companion for the book Tidy Modeling with R by Max Kuhn and Julia Silge. This companion is available at r4ds.io/tmwr. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book. This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["software-for-modeling.html", "Chapter 1 Software for modeling", " Chapter 1 Software for modeling Learning objectives: Recognize the principles around which the {tidymodels} packages were designed. Classify models as descriptive, inferential, and/or predictive. Define descriptive model. Define inferential model. Define predictive model. Differentiate between supervised and unsupervised models. Differentiate between regression and classification models. Differentiate between quantitative and qualitative data. Understand the roles that data can have in an analysis. Apply the data science process. Recognize the phases of modeling. The utility of a model hinges on its ability to be reductive. The primary influences in the data can be captured mathematically in a useful way, such as in a relationship that can be expressed as an equation. There are two reasons that models permeate our lives today: an abundance of software exists to create models and it has become easier to record data and make it accessible. "],["the-pit-of-success.html", "1.1 The pit of success", " 1.1 The pit of success {tidymodels} aims to help us fall into the Pit of Success: The Pit of Success: in stark contrast to a summit, a peak, or a journey across a desert to find victory through many trials and surprises, we want our customers to simply fall into winning practices by using our platform and frameworks. Avoid confusion: Software should facilitate proper usage. Avoid mistakes: Software should make it easy for users to do the right thing. Examples of creating a pit of success (discussed in more details later) internal consistency sensible defaults fail with meaningful error messages rather than silently producing incorrect results "],["types-of-models.html", "1.2 Types of models", " 1.2 Types of models Descriptive models: Describe or illustrate characteristics of data. Inferential models: Make some statement of truth regarding a predefined conjecture or idea. Inferential techniques typically produce some type of probabilistic output, such as a p-value, confidence interval, or posterior probability. Usually delayed feedback between inference and actual result. Predictive models: Produce the most accurate possible prediction for new data. Estimation (“How much?”) rather than inference (“Will it?”). Mechanistic models are derived using first principles to produce a model equation that is dependent on assumptions. Depend on the assumptions that define their model equations. Unlike inferential models, it is easy to make data-driven statements about how well the model performs based on how well it predicts the existing data Empirically driven models have more vague assumptions, and are derived directly from the data. No theoretical or probabilistic assumptions are made about the equations or the variables The primary method of evaluating the appropriateness of the model is to assess its accuracy using existing data 1. Broader discussions of these distinctions can be found in Breiman (2001b) and Shmueli (2010) "],["terminology.html", "1.3 Terminology", " 1.3 Terminology Unsupervised models are used to understand relationships between variables or sets of variables without an explicit relationship between variables and an outcome. Examples: PCA, clustering, autoencoders. Supervised models have an outcome variable. Examples: linear regression, decision trees, neural networks. Regression: numeric outcome Classification: ordered or unordered qualitative values. Quantitative data: numbers. Qualitative (nominal) data: non-numbers. Qualitative data still might be coded as numbers, e.g. one-hot encoding or dummy variable encoding Data can have different roles in analyses: Outcomes (labels, endpoints, dependent variables): the value being predicted in supervised models. Predictors (independent variables): the variables used to predict the outcome. Identifiers Choosing a model type will depend on the type of question we want to answer / problem to solve and on the available data, among other things. "],["the-data-analysis-process.html", "1.4 The data analysis process", " 1.4 The data analysis process Cleaning the data: investigate the data to make sure that they are applicable to the project goals, accurate, and appropriate Understanding the data: often referred to as exploratory data analysis (EDA). EDA brings to light how the different variables are related to one another, their distributions, typical ranges, and other attributes. “How did I come by these data?” “Is the data relevant?” Develop clear expectations of the goal of your model and how performance will be judged (Chapter 9) “What is/are the performance metrics or realistic goal/s of what can be achieved?” The data science process (from R for Data Science by Wickham and Grolemund. "],["the-modeling-process.html", "1.5 The modeling process", " 1.5 The modeling process The modeling process. Exploratory data analysis: Explore the data to see what they might tell you. (See previous) Feature engineering: Create specific model terms that make it easier to accurately model the observed data. Covered in Chapter 6. Model tuning and selection: Generate a variety of models and compare performance. Some models require hyperparameter tuning Model evaluation: Use EDA-like analyses and compare model performance metrics to choose the best model for your situation. The final model may be used for a conclusion and/or produce predictions on new data. "],["meeting-videos.html", "1.6 Meeting Videos", " 1.6 Meeting Videos 1.6.1 Cohort 1 Meeting chat log 00:10:57 Andrew G. Farina: Sorry guys, I have a sleeping baby in the room, so I am stuck with only chat tonight. Looking forward to the discussion though. 00:11:10 mayagans: Hi baby!!! 00:11:31 mayagans: (Hi everyone else too — Im also in a loud house right now super stoked for this!) 00:11:38 Jim Gruman: Hello everyone 00:11:39 Tony ElHabr: the chat is where all of the fun happens anyways! 00:11:59 Tan Ho: Obviously! 00:12:01 Scott Nestler: It&#39;s been way too long since I&#39;ve seen many of you. Hope everyone is doing well. I&#39;m excited for this. 00:12:04 Jeremy: Yep, I’ve got a puppy who believes she’s an attack dog going crazy so I’ll probably mute for a while 00:12:36 Tyler Grant Smith: on kid bath duty for the start of this 00:15:16 Yoni Sidi: It’s a gitbook! 00:15:37 Tan Ho: It&#39;s a book about a book 00:15:40 Tan Ho: classic Jon 00:15:46 Joe Sydlowski: Metabook 00:15:54 Scott Nestler: Very meta. 00:16:03 Tony ElHabr: presentation + book seems like it is prime for a package 00:16:17 Tony ElHabr: counting on jon to jump on that idea 00:26:44 shamsuddeen: The utility of a model hinges on its ability to be reductive. What is the meaning of this from the book? 00:28:03 Tony ElHabr: I think that means &quot;a model should be interpretable&quot; 00:28:28 Tony ElHabr: yeah, &quot;simpler&quot; is a better word 00:28:41 Yoni Sidi: Sparse model means less overfitting 00:28:41 shamsuddeen: sure 00:29:15 Gabriela Palomo: Perhaps it may also mean that a model uses a bunch of data and simplifies it in an equation or model? 00:29:29 Jacob Miller: As someone who is an intermediate user of caret, how useful would it be to switch completely over to tidymodels and not revert back to caret? Or are there benefits to using both consistently? 00:29:56 Gabriela Palomo: So in a way it&#39;s simpler to understand as well vs seeing all the raw data 00:30:09 Tony ElHabr: i feel you have much more &quot;low-level&quot; control with tidymodels 00:30:36 Scott Nestler: I had a similar question to Jacob&#39;s, but with regard to mle &amp; mle3. 00:31:01 Tan Ho: Caret is broader but tidymodels is deeper (see yesterday&#39;s xkcd :P) 00:31:10 Arjun’s iPhone: you can mix tidymodels and caret.... preprocess using tidymodels and feed it to caret 00:32:12 Scott Nestler: TYPO ALERT. I meant mlr and mlr3. 00:32:18 Asmae Toumi: Agreed with David. For example, weighted RMSE stuff is only on caret (for now) and there’s a GitHub issue reply by max basically saying its too hard to add to tidy models right now. Either way tidy models seems the way to go to not be behind in say, 2 years, when its well developed 00:32:20 Conor Tompkins: My understanding is that caret is deprecated. It still works, but tidymodels is where its at now. Like dplyr in 2015. Not 100% coverage compared to base R or data.table, but heading in the right direction fast. 00:32:25 Maria: Yes, usemodels is great! 00:32:38 Conor Tompkins: Yeah not officially depreacted 00:34:30 mayagans: My only comment is that I love how many people are here!!!! I can only imagine the range of domain expertise in this “room” - I HATE ice breakers but do people want to throw in the chat what domain they want to write models in/why they’re reading this book? Im a pharma person but Im also obsessed with music analytics :) I look forward to seeing how presenters apply their chapters! 00:34:43 Connor Krenzer: The book says in the Empirically driven models section: &quot;No theoretical or probabilistic assumptions are made about the sales numbers or the variables that are used to define similarity. In fact, the primary method of evaluating the appropriateness of the model is to assess its accuracy using existing data. If the structure of this type of model was a good choice, the predictions would be close to the actual values.&quot; How does the significance of the model&#39;s variables play into the above? Let&#39;s take linear regression for example. Does this mean we are only supposed to care about R-square instead of p-values? 00:34:51 Yoni Sidi: https://github.com/topepo/workflowsets 00:35:41 Asmae Toumi: Sure Maya, great idea, my domain is healthcare/medtech and for fun, sports analytics! 00:36:44 Jordan Krogmann: Live and die by the pun :) 00:37:04 Kevin Kent: @Maya - I originally learned ML stuff in sklearn but do 80% of my work in R, so I’d like to move that all over to tidy models. I work in healthcare technology, in the devops area 00:37:26 Tony ElHabr: good question Connor. i think it depends on your intention. for an inferential model, the variables and p values matter more, but that&#39;s not to say that your model&#39;s R2 is &quot;allowed&quot; to be really bad. for a predictive model, it would all be about maximizing R2 00:37:38 Yoni Sidi: Modeling and simulation in pharma 00:37:59 Conor Tompkins: I don’t use modeling professionally, would like to get there. I use R to avoid using Excel. Very interested in sports data and civic hacking 00:38:12 Scott Nestler: I guide many students in capstone projects building models in all kinds of domains. Much of my own work is in sports analytics, either for fun or with some of our teams here on campus. 00:38:24 Tony ElHabr: electricity markets. sports for fun 00:38:36 Jonathan Trattner: Undergrad studying computational neuroscience! 00:38:43 Jonathan Trattner: I can volunteer for the tidyverse primer! 00:38:45 Jim Gruman: Im in industry/agriculture - marketing/geospatial/IoT events/survival 00:38:46 Maria: I also in healthcare/research 00:38:58 Tyler Grant Smith: im a predictive modeling actuary working in p&amp;c insurance 00:39:02 Vasant M: @Connor Krenzer yes, the less you use p-values as a metric to assess models the better. R-square is one metric, but not always the most reliable one to use. For instance r-square doesn’t mean anything for non-linear models. I would rather depend on model accuracy to guide model building 00:39:05 Jonathan Trattner: Sounds good (: 00:39:08 Tan Ho: I work in homebuilding, so finance-ish data - and work on fantasy football data as well 00:39:13 Stephen - Computer - No Mic: Degree in Health Data Analytics - currently working on an automated trading algorithm (which is built with tidymodels) 00:39:29 Aashish Cheruvu: I’m a student and I’m interesting in healthcare analytics and tech 00:39:38 Miles Ott (he/him/his): Hi everyone! Excited to be here :)I am a stats/data science prof at Smith College and my work/research stuff is in social network analysis and sampling applied to public health 00:39:43 Vasant M: I am Bioinformatician - Work in Biomedical research, currently doing Lipidomics in Sleep Mediccine 00:39:51 shamsuddeen: Student interested in natural language processing 00:39:55 Andrew G. Farina: I am a grad candidate currently, trying to build a solid base in modeling to use in the future. 00:39:56 Stephen - Computer - No Mic: I have been using R to run a text messaging campaign for the Senate run-offs in Georgia recently 00:40:13 Tim Moloney: I work in environmental consulting, do a lot of geospatial and/or statistics analyses with R 00:40:19 Adrienne St Clair: Hi all, I&#39;m a botanist and work in plant conservation in public parks. I am a nascent data nerd and want to learn all I can about data analysis. 00:40:21 Conor Tompkins: I am currently using tidymodels to build a model to predict house sale prices in Pittsburgh 00:40:34 Jonathan Leslie: I work in data science consulting...I work with businesses/government agencies to design data science projects. 00:40:43 Vasant M: @Stephen that’s very cool. 00:40:45 ErickKnackstedt: Business intelligence developer in the mental health/mindfulness space, no real modeling experience really excited to learn tidymodels 00:40:53 Andrew G (he/him): I work in App Analytics. Will be starting a new gig in app/game analytics soon. Historically modeling on the job has been few and far between so I’m looking forward to understanding best practices, workflow, etc… 00:41:23 Ben Gramza: Hi I&#39;m Ben, I just graduated with a stats degree (and thus am unemployed and without a domain). I&#39;ve done some work with COVID survey data and redistricting/gerrymandering in the past. I also keep up with the sports analytics scene in my free time. 00:41:53 Giovani Ferreira: Tech Team Leader here, data hobbyist, usually very interested in NLP and Topic Modelling, decided to use this bookclub to level my modelling skills 00:42:25 Jacob Miller: Senior studying stats, done actuarial consulting internships, and planning on grad school in stats. Sports analytics is the hobby/passion 00:43:01 mayagans: Aaaahhh so many cool domains!! Everyone is a bad ass wow - I selfishly hope everyone talks ties in the content with their passions and maybe Ill even know something about #SPORTS by the time we’re done LOL 00:44:00 Stephen - Computer - No Mic: Thanks @ Vasant ! 00:44:21 Conor Tompkins: Deployment seems very domain specific 00:44:38 Conor Tompkins: Tech stack = domain 00:45:57 David Severski: Oh, do I have thoughts on cloudy… ;P 00:46:04 David Severski: S/cloudy/cloudyr/ 00:46:32 Jordan Krogmann: https://github.com/wlandau/targets 00:46:39 tim: To get some more background in machine learning, in addition to learning tidymodels, any suggestions for books? I was thinking Applied Predictive Modeling - but keep changing my mind and need something to stick to. I guess it uses caret too? So that might be useful. 00:47:36 mayagans: ……Is Yoni in an aquarium of pizzas? 00:48:07 Tan Ho: asking the important questions :D 00:48:16 Scott Nestler: Responding to Tim&#39;s question … I&#39;m currently working my way through Machine Learning with R, the tidyverse, and mlr (Rhys). 00:48:25 Vasant M: @Tim Statistical Learning PDF link http://www.ime.unicamp.br/~dias/Intoduction%20to%20Statistical%20Learning.pdf 00:48:34 Connor Krenzer: @tim I hear Introduction to Statistical Learning is a classic 00:49:06 Vasant M: @Tim if you like a course https://www.edx.org/course/statistical-learning 00:49:15 Tony ElHabr: i&#39;m just glad we won&#39;t have to nag people to volunteer to do presentations since we have so many participants lol 00:49:21 ErickKnackstedt: https://dtkaplan.github.io/SM2-bookdown/preface-to-this-electronic-version.html 00:49:38 tim: Thanks, this is awesome! Now I just need to pick something and stick with it, haha 00:49:38 ErickKnackstedt: That book is legit 00:49:59 Jonathan Leslie: @Tim I second the recommendation for Introduction to Statistical Learning. It’s a great overview of different modelling approaches and how to interpret model outputs. 00:50:18 Miles Ott (he/him/his): nice to meet you all! 00:50:19 David Severski: Have a great one, everyone! 00:50:24 Jordan Krogmann: thanks take it ea y 00:50:28 Yoni Sidi: Bye and thanks for all the fish 00:50:29 Tan Ho: Cheers gang! 00:50:32 Aashish Cheruvu: Bye everyone and thank you 00:50:33 mayagans: Thanks Jon!! 00:50:36 Maria: Cheers! 00:51:03 Arjun’s iPhone: p 1.6.2 Cohort 2 Meeting chat log 00:06:50 Stephen Holsenbeck: https://docs.google.com/spreadsheets/d/1vD4LG4_nhsxSAxXiBi42iKIvZXQtNxgB5C_PUkIZ0wo/edit#gid=0 00:08:48 Amélie Gourdon-Kanhukamwe: Questions (if forgotten): name (pronunciation), location, fun fact about yourself, why are you here? 00:21:23 Carmen Santana: when the pandemic is over you should come to Portugal, great wine here!! 00:21:50 Kevin Kent: There was a wine ratings tidytuesday a while ago https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-05-28 00:21:50 shamsuddeen: Hahhaha, yes. Specially Porto wine -:) 00:22:30 Layla Bouzoubaa: Yes to portugal 00:22:37 Layla Bouzoubaa: Thanks, Kevin! 00:26:36 Stephen Holsenbeck: https://github.com/r4ds/bookclub-tmwr 00:28:15 Amélie Gourdon-Kanhukamwe: I love Portugal too, but other fun fact: I am non-wine drinking French. 00:28:49 Layla Bouzoubaa: 😱 01:02:53 Kevin Kent: I really like fast ai’s chapter on ethics - https://github.com/fastai/fastbook/blob/master/03_ethics.ipynb (free by the way) 01:03:14 Carmen Santana: thanx 01:03:44 Layla Bouzoubaa: Thank you, kevin 01:04:14 Layla Bouzoubaa: Yes, let’s add it to google doc 01:07:34 Amélie Gourdon-Kanhukamwe: Possibly after FE too? 01:08:45 Layla Bouzoubaa: After 01:08:48 Layla Bouzoubaa: Agree with auust 01:08:51 Kevin Kent: After is ok with me 01:11:03 Layla Bouzoubaa: There was also rstudio conf from last year from googlebrain for ethics to predict lending 01:14:55 shamsuddeen: Cheers guys 1.6.3 Cohort 3 Meeting chat log 00:13:08 Morgan Grovenburg: Daniel I love your Blue&#39;s Clues background! 00:13:34 Daniel Chen: ( : #steve 00:13:56 Morgan Grovenburg: I miss Steve! 00:17:58 priyanka gagneja: nice 00:36:34 Daniel Chen: e.g., mechanistic model is maybe how gasses are absorbed in our cells at depth (in scuba diving) -- buhlmann decompression algorithm: https://en.wikipedia.org/wiki/B%C3%BChlmann_decompression_algorithm 00:37:00 Daniel Chen: ^ not sure, but that was the first thing that came to mind about something that is purely based on physical properties 00:39:13 Chris Martin: Similarly - mechanistic models made me think of models (may be systems of differential equations ...) of the chemical reactions taking place in the Earth&#39;s atmosphere. 00:41:01 Daniel Chen: for those who do not know &quot;one-hot encoding&quot; is the engineering/computer science term for &quot;dummy variables&quot; in statistics -- that took me a long time to realize 00:41:36 Morgan Grovenburg: Thank you Daniel! I had no idea! 00:42:10 priyanka gagneja: I need to run early today . See you all later 00:43:24 Chris Martin: Thanks, yes a new term to me too! 00:46:53 Daniel Chen: one-hot encoding is what they use in sci-kit learn in python. it confused the heck out of me when I was trying to learn how to fit models there 00:57:33 Morgan Grovenburg: I got to go. Thanks Ildiko! 00:59:22 Daniel Chen: https://github.com/r4ds/bookclub-tmwr 01:00:32 Daniel Chen: i can do next week 01:00:40 Ildiko Czeller: https://github.com/r4ds/bookclub-tmwr/blob/main/README.md 01:02:58 Daniel Chen: see/talk to everyone on slack :) 1.6.4 Cohort 4 Meeting chat log 00:32:07 Federica Gazzelloni: https://www.tmwr.org/index.html 00:32:34 Federica Gazzelloni: https://www.tidymodels.org/ 00:34:44 Federica Gazzelloni: https://www.youtube.com/c/JuliaSilge 00:37:03 Federica Gazzelloni: https://github.com/r4ds/bookclub-tmwr Meeting chat log 00:09:51 Brandon Hurr: https://docs.google.com/spreadsheets/d/1-S1UbKWay_TeR5n9LkztZY2XXrMjZr3snl1srPvTvH4/edit#gid=0 00:45:33 Ryan Metcalf: What if we were to augment / change / modify the current variables and then repredict? 00:46:12 Brandon Hurr: Happens all the time in my world. You try and explain with the least amount of variables as you can and then dig more when you don’t cover all the variation in the data that you want. 00:46:36 Brandon Hurr: Even the imperfect models are considered “mechanistic” 01:03:01 Isabella Velásquez: And I’m in the US ! In Central time right now but will be in Pacific time soon - thank you so much for pushing back the time a bit 🙌 "],["a-tidyverse-primer.html", "Chapter 2 A tidyverse primer", " Chapter 2 A tidyverse primer Learning objectives: List the tidyverse design principles. Explain what it means for the tidyverse to be designed for humans. Describe how reusing existing data structures can make functions easier to work with. Explain what it means for a set of functions to be designed for the pipe. Explain what it means for function to be designed for functional programming. List some differences between a tibble and a base data.frame. Recognize how to use the tidyverse to read and wrangle data. "],["tidyverse-design-principles.html", "2.1 Tidyverse design Principles", " 2.1 Tidyverse design Principles The tidyverse has four core design principles: Human centered: Designed to promote human usability. Consistent: Learning how to use one function or package is as similar as another. Composable: Easily breakdown data challenges into smaller components with exploratory tools to find the best solution. Inclusive: Fostering a community of like-minded users (e.g. #rstats) "],["design-for-humans---overview.html", "2.2 Design for Humans - Overview", " 2.2 Design for Humans - Overview “Programs must be written for people to read, and only incidentally for machines to execute.” - Hal Abelson Credit: Nielson Norman Group Motivation - Avoiding Norman Doors What are the equivalent of Norman Doors in programming? "],["design-for-humans.html", "2.3 Design for Humans and the Tidyverse", " 2.3 Design for Humans and the Tidyverse The tidyverse offers packages that are easily readable and understood by humans. It enables them to more easily achieve their programming goals. Consider the mtcars dataset, which comprises fuel consumption and 10 aspects of autombile design and performance from 1973-1974. Previewing the first six rows of the data, we see: ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 If we wanted to arrange these in ascending order based on the mpg and gear variables, how could we do this? The function arrange(), in the dplyr package of the tidyverse, takes a data frame and column names as such: arrange(.data = mtcars, gear, mpg) arrange(), and other tidyverse functions, use names that are descriptive and explicit. For general methods, there is a focus on verbs, as seen with the functions pivot_longer() and pivot_wider() in the tidyr package. "],["reusing-existing-data-structures.html", "2.4 Reusing existing data structures", " 2.4 Reusing existing data structures “You don’t have to reinvent the wheel, just attach it to a new wagon.” - Mark McCormack There are many different data types in R, such as matrices, lists, and data frames.1 A typical function would take in data of some form, conduct an operation, and return the result. tidyverse functions most often operate on data structures called tibbles. Traditional data frames can represent different data types in each column, and multiple values in each row. Tibbles are a special data frame that have additional properties helpful for data analysis. Example: list-columns boot_samp &lt;- rsample::bootstraps(mtcars, times = 3) boot_samp ## # Bootstrap sampling ## # A tibble: 3 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [32/12]&gt; Bootstrap1 ## 2 &lt;split [32/10]&gt; Bootstrap2 ## 3 &lt;split [32/12]&gt; Bootstrap3 class(boot_samp) ## [1] &quot;bootstraps&quot; &quot;rset&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; The above example shows how to create bootstrap resamples of the data frame mtcars. It returns a tibble with a splits column that defines the resampled data sets. This function inherits data frame and tibble methods so other functions that operate on those data structures can be used. For a more detailed discussion, see Hadley Wickham’s Advanced R↩︎ "],["designed-for-the-pipe.html", "2.5 Designed for the pipe", " 2.5 Designed for the pipe The pipe operator, %&gt;%, comes from the magrittr package by Stefan Milton Bache, and is used to chain together a sequence of R functions. More specifically, the pipe operator uses the value of the object on the left-hand side of the operator as the first argument on the operator’s right-hand side. The pipe allows for highly readable code. Consider wanting to sort the mtcars dataset by the number of gears (gear) and then select the first ten rows. How would you do that? cars_arranged &lt;- arrange(mtcars, gear) cars_selected &lt;- slice(cars_arranged, 1:10) # more compactly cars_selected &lt;- slice(arrange(mtcars, gear), 1:10) Using the pipe to substitute the left-hand side of the operator with the first argument on the right-hand side, we can get the same result as follows: cars_selected &lt;- mtcars %&gt;% arrange(gear) %&gt;% slice(1:10) This approach with the pipe works because all the functions return the same data structure (a tibble/data frame) which is the first argument of the next function. Whenever possible, create functions that can be incorporated into a pipeline of operations. "],["designed-for-functional-programming.html", "2.6 Designed for Functional Programming", " 2.6 Designed for Functional Programming Functional Programming is an approach to replace iterative (i.e. for) loops. Consider the case where you may want two times the square root of the mpg for each car in mtcars. You could do this with a for loop as follows: n &lt;- nrow(mtcars) roots &lt;- rep(NA_real_, n) for (car in 1:n) { roots[car] &lt;- 2 * sqrt(mtcars$mpg[car]) } You could also write a function to do the computations. In functional programming, it’s important that the function does not have any side effects and the output only depends on the inputs. For example, the function my_sqrt() takes in a car’s mpg and a weight by which to multiply the square root. my_sqrt &lt;- function(mpg, weight) { weight * sqrt(mpg) } Using the purrr package, we can forgo the for loop and use the map() family of functions which use the basic syntax of map(vector, function). Below, we are applying the my_sqrt() function, with a weight of 2, to the first three elements of mtcars$mpg. User supplied functions can be declared by prefacing it with ~ (pronounced “twiddle”). By default, map() returns a list. If you know the class of a function’s output, you can use special suffixes. A character output, for example, would used by map_chr(), a double by map_dbl(), and a logical by map_lgl(). map( .x = head(mtcars$mpg, 3), ~ my_sqrt( mpg = .x, weight = 2 ) ) ## [[1]] ## [1] 9.165151 ## ## [[2]] ## [1] 9.165151 ## ## [[3]] ## [1] 9.549869 map() functions can be used with 2 inputs, by specifying map2() Requires arguments .x and .y map2( .x = head(mtcars$mpg, 3), .y = c(1,2,3), ~ my_sqrt( mpg = .x, weight = .y ) ) ## [[1]] ## [1] 4.582576 ## ## [[2]] ## [1] 9.165151 ## ## [[3]] ## [1] 14.3248 "],["tibbles-vs.-data-frames.html", "2.7 Tibbles vs. Data Frames", " 2.7 Tibbles vs. Data Frames A tibble is a special type of data frame with some additional properties. Specifically: Tibbles work with column names that are not syntactically valid variable names. data.frame(`this does not work` = 1:2, oops = 3:4) ## this.does.not.work oops ## 1 1 3 ## 2 2 4 tibble(`this does work, though` = 1:2, `woohoo!` = 3:4) ## # A tibble: 2 × 2 ## `this does work, though` `woohoo!` ## &lt;int&gt; &lt;int&gt; ## 1 1 3 ## 2 2 4 Tibbles prevent partial matching of arguments to avoid accidental errors df &lt;- data.frame(partial = 1:5) tbbl &lt;- tibble(partial = 1:5) df$part ## [1] 1 2 3 4 5 tbbl$part ## Warning: Unknown or uninitialised column: `part`. ## NULL Tibbles prevent dimension dropping, so subsetting data into a single column will never return a vector. df[, &quot;partial&quot;] ## [1] 1 2 3 4 5 tbbl[, &quot;partial&quot;] ## # A tibble: 5 × 1 ## partial ## &lt;int&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 Tibbles allow for list-columns, which can be a powerful tool when working with the purrr package. template_list &lt;- list(a = 1, b = 2, c = 3, d = 4, e = 5) data.frame(col = 1:5, list_col = template_list) ## col list_col.a list_col.b list_col.c list_col.d list_col.e ## 1 1 1 2 3 4 5 ## 2 2 1 2 3 4 5 ## 3 3 1 2 3 4 5 ## 4 4 1 2 3 4 5 ## 5 5 1 2 3 4 5 tibble(col = 1:5, list_col = template_list) ## # A tibble: 5 × 2 ## col list_col ## &lt;int&gt; &lt;named list&gt; ## 1 1 &lt;dbl [1]&gt; ## 2 2 &lt;dbl [1]&gt; ## 3 3 &lt;dbl [1]&gt; ## 4 4 &lt;dbl [1]&gt; ## 5 5 &lt;dbl [1]&gt; "],["how-to-read-and-wrangle-data.html", "2.8 How to read and wrangle data", " 2.8 How to read and wrangle data The following example shows how to use the tidyverse to read in data (with the readr package) and easily manipulate it (using the dplyr and lubridate packages). We will walk through these steps during our meeting. library(tidyverse) library(lubridate) url &lt;- &quot;http://bit.ly/raw-train-data-csv&quot; all_stations &lt;- # Step 1: Read in the data. readr::read_csv(url) %&gt;% # Step 2: filter columns and rename stationname dplyr::select(station = stationname, date, rides) %&gt;% # Step 3: Convert the character date field to a date encoding. # Also, put the data in units of 1K rides dplyr::mutate(date = lubridate::mdy(date), rides = rides / 1000) %&gt;% # Step 4: Summarize the multiple records using the maximum. dplyr::group_by(date, station) %&gt;% dplyr::summarize(rides = max(rides), .groups = &quot;drop&quot;) head(all_stations, 10) ## # A tibble: 10 × 3 ## date station rides ## &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2001-01-01 18th 0 ## 2 2001-01-01 35-Bronzeville-IIT 0.448 ## 3 2001-01-01 35th/Archer 0.318 ## 4 2001-01-01 43rd 0.211 ## 5 2001-01-01 47th-Dan Ryan 0.787 ## 6 2001-01-01 47th-South Elevated 0.427 ## 7 2001-01-01 51st 0.364 ## 8 2001-01-01 54th/Cermak 0 ## 9 2001-01-01 63rd-Dan Ryan 1.37 ## 10 2001-01-01 69th 2.37 “This pipeline of operations illustrates why the tidyverse is popular. A series of data manipulations is used that have simple and easy to understand user interfaces; the series is bundled together in a streamlined and readable way. The focus is on how the user interacts with the software. This approach enables more people to learn R and achieve their analysis goals, and adopting these same principles for modeling in R has the same benefits.” - Max Kuhn and Julia Silge in Tidy Modeling with R "],["further-reading.html", "2.9 Further Reading", " 2.9 Further Reading Design of Everyday Things - Don Norman Tidyverse Design Principles - The Tidyverse Team Visualization Analysis and Design - A really great primer on visualization design from a human-centered perspective. Draws on research in cognitive science and presents a high-level framework for designing visualizations to support decision making. From Tidyverse Design Principles Chapter 2: The Unix philsophy The Zen of Python Design Principles Behind Smalltalk "],["meeting-videos-1.html", "2.10 Meeting Videos", " 2.10 Meeting Videos 2.10.1 Cohort 1 Meeting chat log 00:07:43 Daniel Chen: hi Jon. hi everyone. first time here :) 00:08:03 Jon Harmon (jonthegeek): Welcome! 00:09:27 KakShA Ekam DallasSK: the gang is here..except for tyler 00:09:31 KakShA Ekam DallasSK: and his kid 00:10:01 David Severski: The 4 seasons of hex bin stickers. LOL 00:10:11 Tan Ho: MiniTyler doesn&#39;t need help with the tidyverse 00:10:44 KakShA Ekam DallasSK: that&#39;s right..he just wanted to learn bangbang 00:11:11 Tan Ho: hey, there&#39;s Tyler! 00:11:23 pavitra: yay!! 00:12:36 Daniel Chen: i feel like more people have cameras on and respond here than any other zoom meeting i&#39;ve been in all year... 00:12:50 Tony ElHabr: haha I second that 00:13:03 Tan Ho: we&#39;re all frens here :) 00:17:25 Tony ElHabr: i will not hate on using mtcars i will not hate on using mtcars i will not hate on using mtcars 00:17:30 Tan Ho: Ooh! A use case for the &lt;details&gt; block 00:17:41 Tan Ho: shush, no dataset snobbery 00:17:54 Darya Vanichkina: Yup, details is so helpful for easily hiding stuff in Rmd 00:18:06 Jon Harmon (jonthegeek): Ooooh, good call on details, let&#39;s make it clear how to do that in the instructions! (I&#39;m still partly here). 00:18:25 Yoni Sidi: Shameless plug {details} https://github.com/yonicd/details 00:18:48 Tan Ho: ohhh dang 00:18:53 Tan Ho: installing now, I use that all the time 00:19:24 mayagans: Ooooooh it makes a collapsible div?? That is sick nasty! 00:19:29 pavitra: wow, details is very cool 00:19:34 David Severski: Funny, I was just exposed to details in GH issues earlier today. :) 00:20:03 Yoni Sidi: Seamless integration to rmd with it’s own knitter chunk engine…ok I’m done plugging 00:20:15 Vasant M: Ah yes details! That’s why I recall your name @yoni Sidi 00:20:31 Tony ElHabr: lol i have also been having this weird data frame print out issue 00:20:55 Darya Vanichkina: I wonder why… 00:21:27 Tan Ho: must be new tibble package related issue 00:22:17 Yoni Sidi: What print problem? 00:22:22 Daniel Chen: fyi: not all things that work on tibbles will work on data.frame 00:22:36 Tony ElHabr: things like &quot;[90m&quot; get printed out instead of &quot;#&quot; 00:22:53 Gabriela Palomo: and viceversa data.frame -&gt; tibbles 00:22:55 Tan Ho: https://r4ds.github.io/bookclub-tmwr/reusing-existing-data-structures.html 00:23:39 Tony ElHabr: jonathan brought the jokes tonight 00:23:42 Tony ElHabr: always love that 00:23:47 Jon Harmon (jonthegeek): Thinking of %&gt;% as &quot;and then&quot; is very useful. Learning to read code in a way that makes sense is super super helpful! 00:24:24 Yoni Sidi: Ironically Tony You can look at the print method for data.frames in details to see how I got around it 00:24:38 Tony ElHabr: hmm will do Yoni 00:25:08 Tyler Grant Smith: for small loops and especially if youve preallocated the for loop is probably faster 00:25:10 Daniel Chen: to fix the &quot;[90m&quot; stuff you need to put: options(crayon.enabled = FALSE) either in your renvion or on the top of the knited file at the 00:25:25 Yoni Sidi: :+1: 00:25:36 Tony ElHabr: thx y&#39;all. will try that out! 00:26:03 Darya Vanichkina: To evaluate the speed you can use microbenchmark::microbenchmark(&lt;code here&gt;) - I tend to use it when picking from stackoverflow 00:26:23 Tan Ho: ope TIL that&#39;s called a twiddle 00:26:28 Vasant M: i thought ~ is tilde 00:26:34 Jon Harmon (jonthegeek): bench::mark is another option (by Jim Hester) 00:26:38 Tan Ho: today-I-learned 00:27:03 Daniel Chen: yes it&#39;s an annononyoms fuunction 00:27:05 Vasant M: Statistician here and I call it tilde 00:27:07 Tan Ho: I always called it a lambda 00:27:09 Daniel Chen: the twittle is a shortcut for the lambda 00:27:27 Tan Ho: Hadley renames things outta nowhere for no reason 00:27:28 Tan Ho: grah 00:27:32 Jon Harmon (jonthegeek): Good to hear that, Vasant! But it isn&#39;t just Hadley! 00:27:34 Tan Ho: insert pooh meme 00:27:49 mayagans: And twittle is writing a model in twitter 00:27:51 Vasant M: Ya! I think it’s the atlantic/pacific divide 00:28:01 Tony ElHabr: zoom really missed the boat by not allowing images to be embedded in chat 00:28:08 Darya Vanichkina: .x is also hidden from the global environment (i.e. it exists but is not visible in the environment, from memory (?) 00:28:11 Jon Harmon (jonthegeek): https://hsm.stackexchange.com/questions/7999/why-do-mathematicians-call-twiddle#:~:text=Tilde%20is%20from%20Spanish%20tildar,pretty%20apt%20name%20for%20it. 00:28:30 mayagans: Im still trying to find the JB quote about loops 00:28:32 Tyler Grant Smith: i prefer giving arguments in the ... to map when possible rather than making a partial lambda. opinions? 00:28:33 Yoni Sidi: Wait … isn’t ~ $/sim$? 00:28:59 Darya Vanichkina: Tyler, can you give an example? 00:29:05 Tan Ho: &gt; JB quote about loops someone has to write a for loop, but it doesn&#39;t have to be you! 00:29:13 mayagans: Lol yes thank you tan so good 00:29:20 Tony ElHabr: yea tyler. ellipses as the default, and partial only minimally 00:29:57 Tan Ho: yeah, where possible I write map, map-args, function, static args, but not always possible with argument order 00:30:04 Tyler Grant Smith: so for the given example. map(1:5, my_sqrt, weight = 2) 00:30:25 Tan Ho: tbh Tyler started it 00:32:15 Kevin Kent: Yeah I like the … static arg style too. 00:32:54 Darya Vanichkina: And if we have more than one arg would it be (ex_ weight = 2, height = 3, randomarg = 4)? 00:33:03 Daniel Chen: to tyler, re ... : the ... works, but you can&#39;t map a vector of values with it (can&#39;t do: map(1:5, my_sqrt, weight = 11:15) ). that&#39;s what map2 and pmap are for. 00:33:16 Tan Ho: LIVE CODE 00:33:18 Tan Ho: YAY 00:33:26 Joe Sydlowski: The correct window setup too! 00:33:29 Tony ElHabr: insert environment pane comment 00:33:32 Tan Ho: BOOO 00:33:40 Tan Ho: BOOOOOOOOO 00:33:41 Tyler Grant Smith: yep daniel 00:34:05 Tyler Grant Smith: and Darya, yes 00:34:09 David Severski: Source goes on the right. Console on left. Change my mind. ;P 00:34:17 Tan Ho: oh nooooo 00:34:18 mayagans: Thats chaos 00:34:25 atoumi: this is absolutely the correct layout 00:34:30 mayagans: ^^^ 00:34:32 Tan Ho: where&#39;s daryn I need backup 00:34:34 Darya Vanichkina: @atoumi I agree 00:34:36 mayagans: Queen has spoken 00:34:37 Kevin Kent: haha. My eyes would get crossed with source on the right 00:34:40 Tyler Grant Smith: that hurts my brain to think about 00:34:40 Daniel Chen: i have to say it&#39;s the &quot;wrong env pane setup&quot; when you&#39;re teaching (new learners) becuase students ask &quot;mine don&#39;t look like that&quot;. unless you take out time from the lesson to show them how to customize it 00:34:57 Darya Vanichkina: @Daniel I actually do that at the start of every workshop 00:35:13 David Severski: Now it’s time for the RMarkdown notebook output to console/in-line debate. :D 00:35:26 Darya Vanichkina: Because that allows me to stick EVERYTHING into the bottom right pane, and have source take up the entire left of the screen 00:35:28 Tan Ho: i assume we don&#39;t need to have a lightmode darkmode fight? 00:35:39 Tony ElHabr: i think we are all on the dark side, right? 00:35:43 atoumi: Output to console and darkmode because we care about our eyes 00:35:44 Scott Nestler: That might be something we all agree on. 00:35:46 Tony ElHabr: RIGHT Y&#39;ALL? 00:35:47 Darya Vanichkina: Also, I think it’s a discussion, not a fight … 00:35:59 Tan Ho: shush 00:36:00 David Severski: For teaching, Studio Cloud or a dockerized version is my goto setup. 00:36:08 Tan Ho: we don&#39;t pull punches with dark mode 00:36:50 David Severski: Yay! Rsthemes! 00:36:53 mayagans: Blindeddd by the lightttt 00:37:07 Darya Vanichkina: I’m Carpentries-trained/based, so I still tend to want people to walk away with something they can work with on their own data later 00:37:11 Darya Vanichkina: Although it does take time 00:37:21 Tony ElHabr: makes sense Darya 00:37:40 Daniel Chen: i run into horizontal space issues when i make the font bigger when i have to teach, but that usually ends up being workshop dependent. 00:37:43 Joe Sydlowski: I never noticed that before! 00:38:25 Darya Vanichkina: Surprisingly, I recently had a horrific workshop (geospatial python with DL, via zoom - install issues every session), and the post-workshop survey learners STILL said they wanted us to fix installs (instead of a docker-based VM solution) 00:38:43 Darya Vanichkina: [I am still setting up a viable docker/VM for next year] 00:39:11 Daniel Chen: for python, can you use a binder instance for workshops? 00:39:27 Darya Vanichkina: I needed GPUs 00:39:33 Daniel Chen: :( 00:40:00 Daniel Chen: conda environment.yml files aren&#39;t able to install the binaries? 00:40:28 Connor Krenzer: Could you use the map() function on this dataset? 00:40:29 mayagans: Your python talk is gonna get you booted from this chat (lol jk xoxo) 00:40:32 David Severski: I kinda hate the warning messages the new dplyr spits out about groups (yes, I know they’re configurable). 00:40:34 Jordan Krogmann: Great job! 00:40:37 Tony ElHabr: discussion of python install issues in an R meeting? how fitting 00:40:54 Yoni Sidi: Great job Jonathan! 00:40:54 Tan Ho: i threw the option into my rprofile once and have never had problems again 00:41:02 Darya Vanichkina: @mayagans/@Tony I think it’s more a teaching issue 00:41:08 Darya Vanichkina: And I definitely use/teach both 00:41:10 Tony ElHabr: yeah, global Rprofile option ftw 00:41:19 Tony ElHabr: don&#39;t get me wrong, i love python too 00:41:26 Tony ElHabr: i just like to joke haha 00:41:28 David Severski: I do to, but I use a lot of renv these days, so I’m always going into environments where my rprofile isn’t active 00:45:14 Vasant M: Especially when you apply a lot of different functions (models) , being explicit is better 00:45:19 Tan Ho: https://imgur.com/WdcWnKz 00:45:54 Connor Krenzer: Is there a reason why you would use map() instead of vapply()? 00:46:52 Daniel Chen: vapply lets you specify the output types and how many elements in each list element. map only returns a list. 00:47:28 Daniel Chen: that&#39;s map vs map_* map will return a list 00:48:25 David Severski: And the easy path to parallelization with `furrr` is so nice. 00:49:01 Darya Vanichkina: @David agreed 00:49:56 Yoni Sidi: https://github.com/hrbrmstr/freebase 00:51:02 David Severski: He really called it freebase? :P 00:51:06 Darya Vanichkina: Thanks, everyone! 00:51:08 David Severski: Bye! 2.10.2 Cohort 2 Meeting chat log 00:08:44 Carmen Santana: Hello, my connection is terrible 00:08:47 Carmen Santana: but Ana told me that she need to work today and will join next week 00:09:12 Stephen Holsenbeck: ok ✅ thanks for letting us know! 00:22:45 Layla Bouzoubaa: Can you send the url of this thread? 00:23:01 Layla Bouzoubaa: I’m going to use this for teaching :D 00:23:38 Kevin Kent: https://twitter.com/andrewheiss/status/1359583543509348356?s=20 00:23:45 Layla Bouzoubaa: Thank you!! 00:26:59 shamsuddeen: arrange(.data = mtcars, gear, mpg), why do we have .? before data 00:28:59 Stephen Holsenbeck: that&#39;s just how the argument is specified in arrange. R allows periods in object names, so you can have an object named simply . if you wanted objects named with a preceding period are considered &quot;hidden&quot;. they won&#39;t show up in the environment inspector but they are actually present in the environment and be called from the console 00:32:57 Luke Shaw: In reality do you try to always use the namespace &quot;::&quot; syntax when calling functions? I sometimes do, but often don&#39;t... 00:33:38 Stephen Holsenbeck: I do 00:33:53 Carmen Santana: I only use it when I know I have functions with the same name in different packages 00:34:16 Roberto Villegas-Diaz: I personally like to use `::`, as I build packages for my research group and some verbs (function names) are very similar to others 00:34:21 August: yes, I think its good practice. Practicularly if you are using a lot of packages 00:34:40 Amélie Gourdon-Kanhukamwe: I tend to if I know this is code I will share, so that helps the collaborator / reader to know which of the many libraries I have loaded, it is coming from. 00:36:07 Roberto Villegas-Diaz: I use this line a lot `%&gt;%` &lt;- magrittr::`%&gt;%` 😅 00:36:19 Stephen Holsenbeck: 🙂 same 00:41:13 Roberto Villegas-Diaz: Thanks for the tip on snippets! 🎉 00:42:51 Stephen Holsenbeck: ✅ 00:48:52 Layla Bouzoubaa: booo 00:53:24 Amélie Gourdon-Kanhukamwe: That sounds about right. 00:59:50 shamsuddeen: arrange(.data = x, .by_group = ) 01:07:04 shamsuddeen: dplyr::select(station = stationname, date, rides) , do you prefer this approach or use library(packagename) ? 01:09:21 August: I&#39;ve got to go, hope you all enjoy the rest of your Sundays :D 01:09:43 Stephen Holsenbeck: likewise August! happy v day! 01:10:52 Layla Bouzoubaa: Bye everyone!! Need to hop off, happy valentines! &lt;3 01:11:08 Stephen Holsenbeck: bye, happy Valentine&#39;s to you too! 01:12:23 Roberto Villegas-Diaz: Thanks for presenting Kevin! Stay safe everyone! 01:12:29 Amélie Gourdon-Kanhukamwe: Yes, thank you! 01:12:35 Carmen Santana: Thanks Kevin! 01:12:40 shamsuddeen: Thanks Kevin !! 01:12:51 shamsuddeen: See you 2.10.3 Cohort 3 Meeting chat log 00:13:03 Daniel Chen (he/him): https://r4ds.github.io/bookclub-tmwr/a-tidyverse-primer.html 00:20:33 Jake Scott: I used to have to read the docs every single time for gather and spread. The pivot verbs have saved my life many times! 00:21:40 priyanka gagneja: I somehow see the shared screen too small, is there a way to make it bigger ? 00:22:14 Jake Scott: You can click on the three dots at the corner of it, and then click &quot;pin&quot;. At least that&#39;s what I did! 00:22:25 edgar zamora: ^^same 00:22:33 priyanka gagneja: Perfect thanks 00:22:38 Chris Martin: I was confused by gather and spread too! I didn&#39;t realise they were the same as the pivot functions even though I use them! 00:39:14 Jake Scott: That is going to save me so much time, I had no idea about that! 00:39:19 Morgan Grovenburg: Same!!! 00:40:05 Chris Martin: Yeah, I had wondered how to get round that issue ... 00:40:16 Chris Martin: Now I know! 00:40:35 Jake Scott: I always commented out the pipe above too, which wasn&#39;t the worst in the world, but def slowed things down 00:44:10 Ildiko Czeller: the conflicted package can be used for explicit preferences for some functions 00:44:29 Jake Scott: MASS versus dplyr is always an epic battle between which filter() and select() I want to use, since they mask each other 00:45:37 Hannah: If you want to see the order in which packages were loaded you can check `searchpaths()`. 00:46:01 Toryn Schafer: Advanced R has a section on the order of namespaces/package environments: https://adv-r.hadley.nz/environments.html#special-environments 00:49:11 Ildiko Czeller: %T&gt;% 00:49:20 Ildiko Czeller: those are not exported I think 00:49:29 jiwan: %in% ? 00:49:47 Ildiko Czeller: %in% is a base I thing I think 00:49:56 Hannah: %in% is a base operator 00:50:07 jiwan: gotcha 00:50:09 Ildiko Czeller: %% notes any &quot;infix&quot; operator 00:51:48 Hannah: well done! 00:52:41 jiwan: thx for presenting! 00:54:16 Morgan Grovenburg: Are those DOTA 2 mouse pads? 00:54:54 Daniel Chen (he/him): yes :) 00:55:30 Morgan Grovenburg: Sweet 01:03:01 Hannah: I haven&#39;t used it yet but recently came across this package for working with pipelines: https://github.com/MilesMcBain/breakerofchains - it lets you run the code up to the line of the cursor 01:04:24 Ildiko Czeller: I think data.frame is only &quot;preferred&quot; if your script relies on one of the behaviors that tibble &quot;corrects&quot; 2.10.4 Cohort 4 Meeting chat log 00:01:48 Isabella Velásquez: Morning everybody. Eating breakfast 🙂 00:18:29 Federica Gazzelloni: nest and unnest: https://tidyr.tidyverse.org/reference/nest.html 00:25:55 Isabella Velásquez: I had a similar question about library(package) and install.packages(“package”)… but I don’t remember the answer so I did not internalize the learning 🙃 00:55:38 Esmeralda Cruz: oh yes, that happen with group bye and then summarize 00:58:10 Anna-Leigh Brown: mtcars %&gt;% rownames_to_column(&#39;car&#39;) %&gt;% group_by(cyl,gear) %&gt;% mutate(disp_mean = mean(disp)) 01:00:38 Isabella Velásquez: Thanks all - I got to drop off ^_^ Great walkthrough, Brandon! 01:01:09 Esmeralda Cruz: Thanks and Meery christmas "],["a-review-of-r-modeling-fundamentals.html", "Chapter 3 A review of R modeling fundamentals", " Chapter 3 A review of R modeling fundamentals Learning objectives: Specify model terms using the R formula syntax. List conveniences for modeling that are supported by the R formula syntax. Use anova() to compare models. Use summary() to inspect a model. Use predict() to generate new predictions from a model. List the three purposes that the R model formula serves. Recognize how the design for humans rubric is applied to {tidymodels} packages. Use broom::tidy() to standardize the structure of R objects. Use the {tidyverse} along with base modeling functions like lm() to produce multiple models at once. "],["r-formula-syntax.html", "3.1 R formula syntax", " 3.1 R formula syntax We’ll use the trees data set provided in {modeldata} (loaded with {tidymodels}) for demonstration purposes. Tree girth (in inches), height (in feet), and volume (in cubic feet) are provided. (Girth is somewhat like a measure of diameter.) library(tidyverse) library(tidymodels) theme_set(theme_minimal(base_size = 14)) data(trees) trees &lt;- as_tibble(trees) trees ## # A tibble: 31 × 3 ## Girth Height Volume ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 8.3 70 10.3 ## 2 8.6 65 10.3 ## 3 8.8 63 10.2 ## 4 10.5 72 16.4 ## 5 10.7 81 18.8 ## 6 10.8 83 19.7 ## 7 11 66 15.6 ## 8 11 75 18.2 ## 9 11.1 80 22.6 ## 10 11.2 75 19.9 ## # ℹ 21 more rows Note that there is an analytical way to calculate tree volume from measures of diameter and height. We observe that Girth is strongly correlated with Volume trees %&gt;% corrr::correlate() ## # A tibble: 3 × 4 ## term Girth Height Volume ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Girth NA 0.519 0.967 ## 2 Height 0.519 NA 0.598 ## 3 Volume 0.967 0.598 NA Shame on you 😉 if you didn’t guess I would make a scatter plot given a data set with two variables. trees %&gt;% ggplot(aes(x = Girth, y = Height)) + geom_point(aes(size = Volume)) We can fit a linear regression model to predict Volume as a function of the other two features, using the formula syntax to save us from some typing. reg_fit &lt;- lm(Volume ~ ., data = trees) reg_fit ## ## Call: ## lm(formula = Volume ~ ., data = trees) ## ## Coefficients: ## (Intercept) Girth Height ## -57.9877 4.7082 0.3393 How would you write this without the formula syntax? If we want to get fancy with our pipes (%&gt;%), then we should wrap our formula with formula(). This due to the way . is interpreted by (%&gt;%). The (%&gt;%) passes the object on the left-hand side (lhs) to the first argument of a function call on the right-hand side (rhs). Often you will want lhs to the rhs call at another position than the first. For this purpose you can use the dot (.) as placeholder. For example, y %&gt;% f(x, .) is equivalent to f(x, y) and z %&gt;% f(x, y, arg = .) is equivalent to f(x, y, arg = z). - magrittr/pipe This would be confusing since within lm(), the . is interpreted as “all variables aside from the outcome”. This is why we explicitly call formula(). This allows us to pass the data object (trees) with the pipe to the data argument, below, not to the actual formula. trees %&gt;% lm(formula(Volume ~ .), data = .) ## ## Call: ## lm(formula = formula(Volume ~ .), data = .) ## ## Coefficients: ## (Intercept) Girth Height ## -57.9877 4.7082 0.3393 Interaction terms are easy to generate. inter_fit &lt;- lm(Volume ~ Girth * Height, data = trees) inter_fit ## ## Call: ## lm(formula = Volume ~ Girth * Height, data = trees) ## ## Coefficients: ## (Intercept) Girth Height Girth:Height ## 69.3963 -5.8558 -1.2971 0.1347 Same goes for polynomial terms. The use of the identity function, I(), allows us to apply literal math to the predictors. poly_fit &lt;- lm(Volume ~ Girth + I(Girth^2) + Height, data = trees) poly_fit ## ## Call: ## lm(formula = Volume ~ Girth + I(Girth^2) + Height, data = trees) ## ## Coefficients: ## (Intercept) Girth I(Girth^2) Height ## -9.9204 -2.8851 0.2686 0.3764 poly_fit3 &lt;- lm(Volume ~ (.)^2, data = trees) poly_fit3 ## ## Call: ## lm(formula = Volume ~ (.)^2, data = trees) ## ## Coefficients: ## (Intercept) Girth Height Girth:Height ## 69.3963 -5.8558 -1.2971 0.1347 # There are only two predictors in this model so this produces the same results as # inter_fit but if there were 3 there would be three individual # effects and the combination of those effects as interaction depending on if the third # variable is continuous or categorical Excluding columns is intuitive. no_height_fit &lt;- lm(Volume ~ . - Height, data = trees) no_height_fit ## ## Call: ## lm(formula = Volume ~ . - Height, data = trees) ## ## Coefficients: ## (Intercept) Girth ## -36.943 5.066 The intercept term can be removed conveniently. This is just for illustrative purposes only. Removing the intercept is rarely done. In this particular case, it may make sense as it is impossible for a tree to have negative volume no_intercept_fit &lt;- lm(Volume ~ . + 0, data = trees) no_intercept_fit ## ## Call: ## lm(formula = Volume ~ . + 0, data = trees) ## ## Coefficients: ## Girth Height ## 5.0440 -0.4773 To illustrate another convenience provided by formulas, let’s add a categorical column. trees2 &lt;- trees set.seed(42) trees2$group = sample(toupper(letters[1:4]), size = nrow(trees2), replace = TRUE) trees2 ## # A tibble: 31 × 4 ## Girth Height Volume group ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 8.3 70 10.3 A ## 2 8.6 65 10.3 A ## 3 8.8 63 10.2 A ## 4 10.5 72 16.4 A ## 5 10.7 81 18.8 B ## 6 10.8 83 19.7 D ## 7 11 66 15.6 B ## 8 11 75 18.2 B ## 9 11.1 80 22.6 A ## 10 11.2 75 19.9 D ## # ℹ 21 more rows Encoding the categories as separate features is done auto-magically with the formula syntax. dummy_fit &lt;- lm(Volume ~ ., data = trees2) dummy_fit ## ## Call: ## lm(formula = Volume ~ ., data = trees2) ## ## Coefficients: ## (Intercept) Girth Height groupB groupC groupD ## -55.2921 4.6932 0.3093 -1.8367 -0.0497 0.6462 Under the hood, this is done by model.matrix(). model.matrix(Volume ~ ., data = trees2) %&gt;% head(10) ## (Intercept) Girth Height groupB groupC groupD ## 1 1 8.3 70 0 0 0 ## 2 1 8.6 65 0 0 0 ## 3 1 8.8 63 0 0 0 ## 4 1 10.5 72 0 0 0 ## 5 1 10.7 81 1 0 0 ## 6 1 10.8 83 0 0 1 ## 7 1 11.0 66 1 0 0 ## 8 1 11.0 75 1 0 0 ## 9 1 11.1 80 0 0 0 ## 10 1 11.2 75 0 0 1 To visualize the inclusion of a polynomial: dummy_fit3 &lt;- lm(Volume ~ (.)^3, data = trees2) dummy_fit3 ## ## Call: ## lm(formula = Volume ~ (.)^3, data = trees2) ## ## Coefficients: ## (Intercept) Girth Height ## 60.087182 -5.435685 -1.137632 ## groupB groupC groupD ## -17.115928 281.578642 -57.794217 ## Girth:Height Girth:groupB Girth:groupC ## 0.126224 0.838817 -20.583875 ## Girth:groupD Height:groupB Height:groupC ## 7.434342 0.152739 -3.639388 ## Height:groupD Girth:Height:groupB Girth:Height:groupC ## 0.536103 -0.005086 0.266657 ## Girth:Height:groupD ## -0.078695 3.1.1 Recap Purposes of R model formula: The formula defines the columns that are used by the model. The standard R machinery uses the formula to encode the columns into an appropriate format. The roles of the columns are defined by the formula. "],["inspecting-and-developing-models.html", "3.2 Inspecting and developing models", " 3.2 Inspecting and developing models Being the sound analysts that we are, we should check if the assumptions of linear regression are violated. The plot() generic function has a specific method for lm objects that generates various diagnostic plots. A short recap of the 4 main diagnostic plots produced by plot() of a model object. Residuals vs Fitted - to see if residuals have non-linear patterns. Good sign if you see equally spread residuals around a horizontal line without distinct patterns normal Q-Q plot to see if both sets of residuals are identical, if the line is straight then sets come from normal distributions Scale Location plot to see if residuals are spread evenly along ranges of predictors good to check for assumptions of homoscedasticity (equal variance) Residual vs Leverage plot helps to identify an influential cases (cases that don’t get along with the trend of the majority). these are identified by where residuals are located off the Cook’s distance line. par(mfrow = c(1, 2)) plot(reg_fit, which = c(1, 2)) The second plot does not show any strong violation of the normality assumption. However, the first plot shows a violation of the linearity assumption (that there is a linear relationship between the response variable and the predictors). If the assumption were satisfied, the smooth red line would be like a straight horizontal line at y=0. Note that there is a {ggplot2} way to generate the same plots. library(ggfortify) autoplot(reg_fit, which = c(1, 2)) But what about the coefficients? summary(reg_fit) ## ## Call: ## lm(formula = Volume ~ ., data = trees) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.4065 -2.6493 -0.2876 2.2003 8.4847 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -57.9877 8.6382 -6.713 2.75e-07 *** ## Girth 4.7082 0.2643 17.816 &lt; 2e-16 *** ## Height 0.3393 0.1302 2.607 0.0145 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.882 on 28 degrees of freedom ## Multiple R-squared: 0.948, Adjusted R-squared: 0.9442 ## F-statistic: 255 on 2 and 28 DF, p-value: &lt; 2.2e-16 Use {broom} for a tidy version. library(broom) reg_fit %&gt;% tidy() ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -58.0 8.64 -6.71 2.75e- 7 ## 2 Girth 4.71 0.264 17.8 8.22e-17 ## 3 Height 0.339 0.130 2.61 1.45e- 2 reg_fit %&gt;% glance() %&gt;% glimpse() ## Rows: 1 ## Columns: 12 ## $ r.squared &lt;dbl&gt; 0.94795 ## $ adj.r.squared &lt;dbl&gt; 0.9442322 ## $ sigma &lt;dbl&gt; 3.881832 ## $ statistic &lt;dbl&gt; 254.9723 ## $ p.value &lt;dbl&gt; 1.071238e-18 ## $ df &lt;dbl&gt; 2 ## $ logLik &lt;dbl&gt; -84.45499 ## $ AIC &lt;dbl&gt; 176.91 ## $ BIC &lt;dbl&gt; 182.6459 ## $ deviance &lt;dbl&gt; 421.9214 ## $ df.residual &lt;int&gt; 28 ## $ nobs &lt;int&gt; 31 {purrr} and {dplyr} can help you scale up your modeling process. We can compare all of the models we made before. list( &#39;reg&#39; = reg_fit, &#39;inter&#39; = inter_fit, &#39;poly&#39; = poly_fit, &#39;no_height&#39; = no_height_fit, &#39;no_intercept&#39; = no_intercept_fit ) %&gt;% map_dfr(glance, .id = &#39;id&#39;) %&gt;% select(id, adj.r.squared) %&gt;% arrange(desc(adj.r.squared)) ## # A tibble: 5 × 2 ## id adj.r.squared ## &lt;chr&gt; &lt;dbl&gt; ## 1 poly 0.975 ## 2 inter 0.973 ## 3 no_intercept 0.968 ## 4 reg 0.944 ## 5 no_height 0.933 We observe that the polynomial fit is the best. We can create models for each group in trees2. reg_fits &lt;- trees2 %&gt;% group_nest(group) %&gt;% mutate( fit = map(data, ~ lm(formula(Volume ~ .), data = .x)), # converts model object&#39;s coefficients to a dataframe tidied = map(fit, tidy), # row summary of a model glanced = map(fit, glance), # returns a tibble of additional metrics like Cooks distance, lower and upper bounds # of fitted values, standard errors of fitted values augmented = map(fit, augment) ) .select_unnest &lt;- function(data, ...) { data %&gt;% select(group, ...) %&gt;% unnest(...) } reg_fits %&gt;% .select_unnest(tidied) ## # A tibble: 12 × 6 ## group term estimate std.error statistic p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A (Intercept) -44.6 17.5 -2.55 0.0312 ## 2 A Girth 4.21 0.477 8.83 0.00000998 ## 3 A Height 0.250 0.294 0.849 0.418 ## 4 B (Intercept) -66.1 13.9 -4.74 0.0178 ## 5 B Girth 4.16 0.704 5.91 0.00969 ## 6 B Height 0.520 0.123 4.24 0.0240 ## 7 C (Intercept) -86.4 90.5 -0.954 0.410 ## 8 C Girth 4.83 0.747 6.47 0.00748 ## 9 C Height 0.680 1.20 0.567 0.611 ## 10 D (Intercept) -46.3 14.8 -3.14 0.0349 ## 11 D Girth 6.03 0.372 16.2 0.0000852 ## 12 D Height -0.0268 0.214 -0.125 0.906 reg_fits %&gt;% .select_unnest(glanced) ## # A tibble: 4 × 13 ## group r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 0.960 0.951 3.81 107. 5.27e-7 2 -31.3 70.7 72.6 ## 2 B 0.935 0.891 2.20 21.5 1.66e-2 2 -11.2 30.3 29.5 ## 3 C 0.946 0.910 4.06 26.2 1.26e-2 2 -14.8 37.7 36.8 ## 4 D 0.990 0.985 2.80 194. 1.04e-4 2 -15.2 38.4 38.2 ## # ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; reg_fits %&gt;% .select_unnest(augmented) ## # A tibble: 31 × 10 ## group Volume Girth Height .fitted .resid .hat .sigma .cooksd .std.resid ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 10.3 8.3 70 7.81 2.49 0.252 3.91 0.0645 0.758 ## 2 A 10.3 8.6 65 7.82 2.48 0.283 3.90 0.0780 0.769 ## 3 A 10.2 8.8 63 8.17 2.03 0.395 3.93 0.103 0.687 ## 4 A 16.4 10.5 72 17.6 -1.18 0.134 4.01 0.00566 -0.332 ## 5 A 22.6 11.1 80 22.1 0.500 0.534 4.03 0.0142 0.192 ## 6 A 19.1 12 75 24.6 -5.54 0.123 3.45 0.113 -1.56 ## 7 A 22.2 12.9 74 28.2 -5.99 0.0837 3.38 0.0823 -1.64 ## 8 A 36.3 14.5 74 34.9 1.37 0.116 4.00 0.00643 0.383 ## 9 A 38.3 16 72 40.8 -2.45 0.330 3.90 0.101 -0.787 ## 10 A 55.7 17.5 82 49.6 6.13 0.255 3.16 0.397 1.87 ## # ℹ 21 more rows "],["more-of-base-and-stats.html", "3.3 More of {base} and {stats}", " 3.3 More of {base} and {stats} R’s {base} and {stats} libraries have lots of built-in functions that help perform statistical analysis. For example, anova() can be used to compare two regression models quickly. anova(reg_fit, poly_fit) ## Analysis of Variance Table ## ## Model 1: Volume ~ Girth + Height ## Model 2: Volume ~ Girth + I(Girth^2) + Height ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 28 421.92 ## 2 27 186.01 1 235.91 34.243 3.13e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We observe that the second order term for Girth does indeed provide significant explanatory power to the model. (Formally, we reject the null hypothesis that the second order term for Girth is zero.) What is ANOVA? Use base R statistical function when someone tries to test your statistics knowledge. Question: If \\(U_1\\) and \\(U_2\\) are i.i.d. (independent and identically distributed) \\(Unif(0,1)\\) random variables, what is the distribution of \\(U_1 + U_2\\)? set.seed(42) n &lt;- 10000 u_1 &lt;- runif(n) u_2 &lt;- runif(n) .hist &lt;- function(x, ...) { hist(x, probability = TRUE,...) lines(density(x), col = &quot;blue&quot;, lwd = 2, ...) } layout(matrix(c(1,2,3,3), 2, 2, byrow = TRUE)) .hist(u_1) .hist(u_2) .hist(u_1 + u_2) Answer: Evidently it’s triangular. There are probably lots of functions that you didn’t know you even needed. add_column &lt;- function(data) { # Whoops! `df` should be `data` df %&gt;% mutate(dummy = 1) } trees %&gt;% add_column() ## Error in UseMethod(&quot;mutate&quot;): no applicable method for &#39;mutate&#39; applied to an object of class &quot;function&quot; df() is the density function for the F distribution with df1 and df2 degrees of freedom df ## function (x, df1, df2, ncp, log = FALSE) ## { ## if (missing(ncp)) ## .Call(C_df, x, df1, df2, log) ## else .Call(C_dnf, x, df1, df2, ncp, log) ## } ## &lt;bytecode: 0x55bd86818080&gt; ## &lt;environment: namespace:stats&gt; "],["why-tidy-principles-and-tidymodels.html", "3.4 Why Tidy Principles and {tidymodels}?", " 3.4 Why Tidy Principles and {tidymodels}? The {tidyverse} has four guiding principles which {tidymodels} shares. It is human centered, i.e. the {tidyverse} is designed specifically to support the activities of a human data analyst. Functions use sensible defaults, or use no defaults in cases where the user must make a choice (e.g. a file path). {recipes} and {parnsip} enable data frames to be used every where in the modeling process. Data frames are often more convenient than working with matrices/vectors. It is consistent, so that what you learn about one function or package can be applied to another, and the number of special cases that you need to remember is as small as possible. Object orientated programming (mainly S3) for functions such as predict() provide a consistent interface to the user. broom::tidy() output is in a consistent format (data frame). List outputs provided by package-specific functions vary. It is composable, allowing you to solve complex problems by breaking them down into small pieces, supporting a rapid cycle of exploratory iteration to find the best solution. {recipes}, {parsnip}, {tune}, {dials}, etc are separate packages used in a tidy machine learning development workflow. It may seem inconvenient to have so many packages to perform specific tasks, but such a paradigm is helpful for decomposing the whole model design process, often making problems feel more manageable. It is inclusive, because the tidyverse is not just the collection of packages, but it is also the community of people who use them. Although the {tidyverse} and {tidymodels} are opinionated in their design, the developers are receptive to public feedback. "],["meeting-videos-2.html", "3.5 Meeting Videos", " 3.5 Meeting Videos 3.5.1 Cohort 1 Meeting chat log 00:06:11 Jonathan Trattner: I second that 00:08:03 Tan Ho: Time to find and send memez 00:09:11 Tan Ho: You all have seen Hadley cat vibing, right? 00:09:18 Jon Harmon (jonthegeek): https://www.vivino.com/mcpherson-cellars-la-herencia/w/2270344?ref=nav-search&amp;cart_item_source=text-search 00:10:00 Jon Harmon (jonthegeek): https://twitter.com/RCoderWeb/status/1351282600086810634 00:10:18 David Severski: Any chance you could get a bit closer to your mic, Tony? Audio is a bit poor on my end. 00:10:43 Jon Harmon (jonthegeek): I muted the one that was crackling, I think. 00:10:50 Maya Gans: Friendly reminder to please mute :) 00:10:54 Maya Gans: Oh you beat me thanks 00:13:16 Jon Harmon (jonthegeek): New base pipe makes this all... weird. But they have a way to do it now, at least! 00:13:53 Jon Harmon (jonthegeek): trees |&gt; my_data =&gt; lm(Volume ~ ., data = my_data) 00:13:55 Jordan Krogmann: I haven&#39;t played around with the base pipe yet 00:14:17 Tan Ho: =&gt; ??? 00:14:24 Jordan Krogmann: that is going to take some re-learning 00:14:27 David Severski: Is there a comprehensive guide to R’s formula syntax? Always found learning I(), + , *, etc. kind of scattered through bits of documentation. 00:14:28 Jon Harmon (jonthegeek): Kinda a lambda function... thing. 00:14:31 Jordan Krogmann: &quot;=&gt;&quot;? 00:14:53 Maya Gans: I thought `I` was “asis” - is it different inside the context of lm? 00:14:56 Jon Harmon (jonthegeek): But don&#39;t get hung up on that &#39;cuz it&#39;s still in development, RStudio will make it clear when it&#39;s time :) 00:15:36 Jon Harmon (jonthegeek): @David: Hmm, I haven&#39;t seen a formula cheat sheet, but there HAS to be one out there... 00:17:35 Tyler Grant Smith: https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf page 57 00:18:12 Yoni Sidi: There is a Belgian flavored cheatsheet, but not going to share anything from said site 00:19:20 Jon Harmon (jonthegeek): Yeahhhh, no thanks! The Paradis book has a good guide in there, it&#39;d be nice to pull that out... so let&#39;s do that in the notes for this chapter! 00:20:45 David Severski: Shout out to gglm for better lm plots with ggplot! http://graysonwhite.com/gglm/ 00:21:02 luifrancgom: Also ggplot2::geom_qq(): https://ggplot2.tidyverse.org/reference/geom_qq.html 00:21:24 Tan Ho: Belgian...flavoured... 00:21:37 Tan Ho: ohhhhh 00:21:39 Jon Harmon (jonthegeek): Reference to a litigious website. 00:21:43 Tan Ho: I was thinking of waffles 00:21:58 tim: Now I&#39;m thinking about waffles 00:22:15 Yoni Sidi: +1 on waffles 00:22:30 Jon Harmon (jonthegeek): https://github.com/hrbrmstr/waffle 00:22:39 David Severski: Gotta be a minority here but pancakes &gt; waffles. ;) 00:22:55 Jon Harmon (jonthegeek): My wife&#39;s pancakes &gt; waffles. 00:23:07 Tan Ho: crepes? 00:23:10 Maya Gans: Insert {stacks} pun 00:23:20 Yoni Sidi: blinches&gt;pancakes 00:24:32 Conor Tompkins: What is the difference between modeling each subgroup separately vs. adding the subgroup as a term in a model that contains all the subgroups? 00:24:32 Yoni Sidi: Side note, you can pass arguments to the broom arguments on the res of the function in map 00:24:41 Yoni Sidi: *rhs 00:27:30 Andrew G. Farina: I think the biggest difference is robustness. modeling each subgroup lets you know how well the model fits that subgroup. Adding in a variable (subgroup) will tell you how much variance is accounted for by the subgroup. Does that make sense? 00:28:05 DX: Hi I am new to the book club. Where can we find this book? 00:28:24 Ben Gramza: https://www.tmwr.org/base-r.html 00:28:28 DX: Thanks 00:28:31 Scott Nestler: I used to get confused about &#39;anova&#39; and &#39;aov&#39; but they are very different. &#39;aov&#39; fits a model (by calling lm) whereas &#39;anova&#39; is a generic function that analyzes a fitted model (or models). 00:28:36 Joe Sydlowski: One reason I&#39;ve used subgroups instead of a categorical variable is when I want to do feature selection with the subgroups. If you use a categorical variable than each level is confined to the same predictors 00:29:00 Maya Gans: Thats a super useful distinction @Scott ! 00:31:03 Conor Tompkins: Andrew, Joe, thanks. I think I see what the benefit is. 00:31:51 Jon Harmon (jonthegeek): If someone wants to pipe up and summarize what Andrew and Joe explained, that&#39;d be great :D 00:32:22 luifrancgom: jajajaja 00:32:40 Jon Harmon (jonthegeek): btw data is also a function, so... be careful :) 00:33:08 Yoni Sidi: Last week me can confirm 00:33:15 Maya Gans: I love using single letters and t always gets me too 00:33:18 Scott Nestler: Many years ago, a common way to estimate a standard normal distribution was to sum 12 Uniform(0,1) distributions and then subtract 6. 00:33:19 luifrancgom: interesting from utils (data) 00:33:23 Jon Harmon (jonthegeek): For following along: r4ds.io/tmwr is this study guide... thing... that we&#39;re making 00:34:09 Jon Harmon (jonthegeek): And I use extremely specific variable names. RStudio autocomplete means you only have to type the full thing once, and then people know what you mean. 00:34:48 Tan Ho: trying to grok joe&#39;s comments for my non-statsy self: fitting one model with a categorical variable means you have one coefficient for each feature and the difference between categories is explained by the coefficient for the category. Fitting one model for each level of the categorical variable means you get different coefficients for a feature based on the subgroup of data 00:35:31 Yoni Sidi: It also depends if you incl the intercept 00:36:23 Yoni Sidi: w and wo it changes the meaning of the coefficients 00:36:48 Conor Tompkins: Sounds like subgrouping lets you turn non-tree models into a model with more tree-ish logic 00:38:02 Andrew G. Farina: I think that is a good way to think about it. If you had multiple models and were comparing fit. A linear model may fit one group better then the others, while a poly model may fit another group better. 00:38:12 luifrancgom: Thank you Tony 00:38:13 Jordan Krogmann: nice work! 00:38:16 Andrew G. Farina: Tony that was great 00:38:17 Scott Nestler: I learned a new term from reading this chapter. In the Advanced R book, we talked about R being &quot;lazy,&quot; but didn&#39;t know what the opposite of that was. Apparently, the word is &quot;eager.&quot; 00:38:40 Maya Gans: Thanks Tony!!! 00:38:40 darynr: Good job, Tony 00:38:48 Tan Ho: YAY TONY 00:38:50 Jonathan Trattner: thanks Tony! 00:39:01 Jonathan Leslie: Thanks, Tony! 00:39:01 Jim Gruman: thank you Tony 00:39:03 Conor Tompkins: Thanks Tony! 00:39:14 caroline: Thank you Tony 00:43:12 Tan Ho: &quot;There are houses&quot; /fin 00:43:40 Scott Nestler: There are actually a bunch of Ames housing data set memes out there. 00:44:21 Jonathan Trattner: https://gallery.shinyapps.io/ames-explorer/ 00:44:44 Jonathan Trattner: doesn&#39;t map stuff though 00:44:54 Jonathan Trattner: so leaflet is still an option! 00:45:32 David Severski: Gotta run here. Thanks everyone! 00:45:59 Jordan Krogmann: Thanks jon and tony later! 3.5.2 Cohort 2 Meeting chat log 00:08:16 Kevin Kent: Welcome Carlo! Where are you living now? 00:08:29 Carlo Medina: im based in nyc currently 00:08:44 Kevin Kent: Nice! I grew up there. Currently in Boston 00:26:54 Kevin Kent: Yeah I was thinking along the lines of August’s explanation as well. I bet it captures the formula expression in the context of the data environment. 00:27:16 Kevin Kent: Like a quosure 00:27:38 August: You know we&#39;re just reading that chapter xD 00:27:47 Kevin Kent: :) advr you mean? 00:28:39 August: yep, Monday class. Its getting cognitively complex. 00:28:57 Kevin Kent: Sweet! I had to diagram that out to make any sense of it 00:29:46 Kevin Kent: Not sure if this is 100% accurate, but if you all could improve on it that would be awesome. Helped me think through it https://docs.google.com/drawings/d/1cSQreGTUabMhRy9Vx2hYGJdJTdVRqTzOUtaJJvvb3mw/edit?usp=sharing 00:32:02 Stephen Holsenbeck: looks right 00:38:42 Luke Shaw: reminds me of Simpsons paradox 00:41:23 August: This blog explains interactions pretty well: https://statisticsbyjim.com/regression/interaction-effects/ 00:41:32 Carmen: thanks! 00:41:39 Amélie Gourdon-Kanhukamwe (she/they): Page 27 of the this pre-print, my collaborator has reported an interaction as Kevin suggests: https://psyarxiv.com/ajv4q/ 00:44:36 Stephen Holsenbeck: 💡 thank you! 00:44:46 Stephen Holsenbeck: thanks August! 00:53:10 Kevin Kent: That groupB, c etc binary coding is a good example of one hot encoding 00:53:31 Kevin Kent: (From our discussion last week) 00:53:56 Kevin Kent: N - 1 binary variables for n levels in the factor 00:54:18 Stephen Holsenbeck: Yes! 👆 00:55:50 Carlo Medina: might be a little late on the interaction stuff and why we have negative betas for girth and height: one way to think about regression is Kevin&#39;s point is to have other variables fixed. thus given the regression result of V=69-5.85G =1.2H + 0.13GH (1) to do this, we can do differentiation with respect to a variable: say dV/dH to check how change in H affects change in V. when we derive eqn (1) we get dV/dH=-1.2+0.13G this implies that H&#39;s effect on Volume is &quot;dependent&quot; on the value of the girth you are currently at. (e.g. if you have a girth of 10 vs a girth of 20, a unit increase in height translates to 0.1 volume per length and 1.4 volume per length, respectively given that our starting girth is 8.3, the dV/dH should be positive even if we have a -1.2 fixed term. sorry might be a little too long, but i hope that helps. :) 00:57:54 Stephen Holsenbeck: That does! a good refresher on differentiation and a reminder that it underlies the mathematics of regression 01:00:08 Kevin Kent: Broom is an incredible package 01:05:39 Carlo Medina: is group_nest syntactic sugar for group_by() %&gt;% nest() ? :O #TIL 01:09:01 Kevin Kent: I think that’s right Carlo. I have never used it - I find the tibble of tibbles approach interesting but I usually split into a list for this type of thing.. But I think this could make it easier to run multiple models and retain original metadata 01:09:24 Kevin Kent: I want to try it this way and see if it makes the workflow cleaner 01:11:10 Kevin Kent: I often lose the name of the group when I map across a list of dfs and then do other transformations to each output. 01:16:12 Luke Shaw: The .hist function was a good example of what shamsuddeen found last time - it would have been bad to call it &quot;hist&quot; as it would have conflicted with the function that already existed 01:16:36 Kevin Kent: Ah ah. That’s an awesome connection. Thanks Luke 01:17:06 Kevin Kent: I think I usually call things like “my_hist” or something like that. This seems more professional or something haha 01:17:26 Shamsuddeen: Yah, luke. Thats good connection 01:19:38 Amélie Gourdon-Kanhukamwe (she/they): This may answer: https://stackoverflow.com/questions/802050/what-is-opinionated-software 01:19:50 Stephen Holsenbeck: 👆 01:20:22 Shamsuddeen: Oh nice 01:21:06 Kevin Kent: Yeah, I think its largely about, as august mentioned, clear and comprehensive set of design standards that are “enforced” in each of the packages in the tidyverse 01:21:45 Kevin Kent: And those standards emanate from a philosophy about how programs should be designed. Basically agreeing with august 3.5.3 Cohort 3 Meeting chat log 00:12:03 Ildiko Czeller: not modeling related, but girth was a new word for me :) 00:22:04 Hannah Frick: it&#39;s doing 2-way interactions for all in the () 00:23:57 Toryn Schafer: lm(mpg ~ (.)^2, data = mtcars) 00:25:57 Hannah Frick: and then all 3-way interactions: m(mpg ~ (.)^3, data = mtcars) 00:26:08 Hannah Frick: *lm 00:36:24 Chris Martin: I think this was the example for + 0 00:37:11 Jake Scott: Thank you! I think seeing it like that makes it make more sense, at least for me. Also thanks to Toryn for explaining! 00:37:40 Ildiko Czeller: +1! thank you both. (it is a shame we cannot like posts here :) ) 00:43:57 Hannah Frick: you specify like a matix with c(rows, columns) 00:56:15 Ildiko Czeller: does anova makes sense if one models predictors are not a superset of the other model predictors? 00:58:41 Jake Scott: I think it would make sense just insofar as we care whether the fit is different between model 1 or model 2. Like maybe model 1 is Y ~ X and model 2 is Y ~ Z. I believe we could use ANOVA to see we get different fits. Not sure it is the primary way to discern that, but I think it&#39;d work no? I may have to hit the books to refresh myself! 00:59:23 Hannah Frick: anova is for nested models only if I recall correctly 00:59:31 Toryn Schafer: Would have to double check, but no, I think the models need to be nested (predictors a subset of the larger model) 01:00:15 Ildiko Czeller: i need to refresh a lot of stats :) it is not clear to me when /why you would use anova or just compare r^2 01:00:28 Toryn Schafer: ANOVA has the p-value haha 01:00:37 Ildiko Czeller: or maybe anova tells you if a smaller r^2 is smaller enough 01:01:15 Jake Scott: The mighty p-value. But yes I am rusty so not sure- if you say it has to be nested, my guess is you&#39;re correct! 01:01:27 Chris Martin: Thanks Edgar 01:02:51 Toryn Schafer: Ildiko, yes it will tell you if the increase in R^2 is significant (remember R^2 always increases with more predictors) 01:04:10 Ildiko Czeller: thank you Toryn, makes sense! 3.5.4 Cohort 4 Meeting chat log 00:12:59 Isabella Velásquez: Hi everybody, getting my headphones ready. 00:13:15 Federica Gazzelloni: Hi Isabella 00:42:56 Laura Rose: I need to jump off for another meeting. Nice explanations, AL! See you all next week. 00:46:44 Isabella Velásquez: Such a good quotation!!! https://twitter.com/fermatslibrary/status/1235582404388982785?lang=en 00:48:25 Isabella Velásquez: dat all the way 😄 "],["the-ames-housing-data.html", "Chapter 4 The Ames housing data", " Chapter 4 The Ames housing data Learning objectives: Explain why exploratory data analysis is an essential component of any modeling project. Recognize the Ames housing data - variables, context, and past cleaning. Explain when it makes sense to log-transform data. "],["pittsburgh-a-parallel-real-world-example.html", "4.1 Pittsburgh: a parallel real world example", " 4.1 Pittsburgh: a parallel real world example Conor Tompkins presented a fantastic overview of home sale price modeling by taking us through his recent project on Pittsburgh home sale price modeling, including discussions about his exploratory data analysis, motivations behind log-transforming sale data, and thoughts about inflation-adjusting historical sale prices. You can check out the discussion and presentation in the Cohort 1 meeting video for this week! Code Repository here: https://github.com/conorotompkins/model_allegheny_house_sales Shiny app: https://conorotompkins.shinyapps.io/house_sale_estimator/ "],["meeting-videos-3.html", "4.2 Meeting Videos", " 4.2 Meeting Videos 4.2.1 Cohort 1 Meeting chat log 00:11:41 Tyler Grant Smith: does jon sound far away 00:11:44 Jonathan Trattner: yes 00:11:58 Yoni Sidi: Austin is far away 00:12:04 Tan Ho: very! 00:12:23 Conor Tompkins: https://conorotompkins.shinyapps.io/house_sale_estimator/ 00:12:26 Jon Harmon (jonthegeek): I ran out of USB slots temporarily so I&#39;m using my crappy microphone for a bit. 00:26:41 Yoni Sidi: what were the original motivations on cleaning the data, was it preset task driven or strictly data driven? 00:27:59 Tan Ho: oops sorry :P 00:28:35 Juan Guillermo: Hi everyone 00:28:42 Jon Harmon (jonthegeek): Welcome, Juan! 00:28:57 Juan Guillermo: thanks! 00:28:59 Darya Vanichkina: With school districts vs council districts, how does that work? I.e. can school districts span counties, and does it make sense to adjust house price based on whether or not the house is in a good school district? [not US-based, so not 100% sure how it works on the ground] 00:29:46 Jon Harmon (jonthegeek): I don&#39;t know about Pittsburgh, but school districts &amp; council districts are completely unrelated in Austin. 00:29:46 Jonathan Trattner: Did you save and document each of those iterations in the data cleaning set? Or were you just going through it looking for what you wanted? 00:30:06 Tan Ho: YAY JON 00:30:12 Jonathan Trattner: Congrats!! 00:30:38 Jon Harmon (jonthegeek): Thanks, it&#39;s exciting! 00:37:56 Tony ElHabr: Conor, did you think about including additional data sets, such as Zillow&#39;s forecasts? 00:38:04 Asmae Toumi: skimr is awesome, it can generate all sorts of summaries. you can also pipe it after a group_by 00:38:19 Darya Vanichkina: It also works on the command line! 00:38:32 Darya Vanichkina: Which is really impressive when working on cloud/HPC 00:39:12 David Severski: Zillow is pretty tight fisted about scraping their estimates. 00:39:28 pavitra: is esquisse comparable with skimr? 00:40:30 pavitra: cool.. thanks! 00:40:43 Kevin Kent: Ohh esquisse is sketch in French. That makes sense 00:41:05 Jon Harmon (jonthegeek): And now French speakers can laugh at me, Pavitra, AND Yoni&#39;s pronunciation! 00:41:44 pavitra: well, I pronounced it like a total desi - &quot;eskqueeeeez&quot;..you cannot top that, Jon! 00:42:11 Asmae Toumi: Really happy I tuned it tonight, didn’t know of priceR package to inflation adjust prices. Ive been doing it manually lol 00:42:24 Jon Harmon (jonthegeek): yeah, that&#39;s great even on its own for sure! 00:42:45 Jon Harmon (jonthegeek): He said GitHub so we need to make him give us the URL so we can put it in the book. 00:42:48 Tan Ho: I&#39;ve been making use of CANSIM to access stats Canada data on stuff, i&#39;m sure there&#39;s something comparable 00:43:03 pavitra: does this dataset include demographics also? 00:43:06 Tan Ho: https://github.com/stevecondylios/priceR 00:43:27 Joe Sydlowski: I feel like I put a lot of blind trust in packages like that. How much do you validate the functions when you find a new package? 00:43:31 Jon Harmon (jonthegeek): @pavitra I don&#39;t think this one did. 00:43:42 Asmae Toumi: Speaking of hockey bruins currently kicking Pittsburgh’s ass right now 00:43:55 Tan Ho: boston home prices kicking everyone&#39;s ass rn 00:44:00 Asmae Toumi: lmaooooo 00:44:07 Yoni Sidi: def check the code and the level of unit testing 00:44:39 Tyler Grant Smith: wouldnt neighborhod effect vary qith year of sale....gentrification etc 00:44:42 Jonathan Trattner: It is on CRAN for whatever that’s worth 00:44:49 Yoni Sidi: that&#39;s not worth much 00:44:54 Tan Ho: He&#39;s also in the R4DS slack channel 00:44:58 Yoni Sidi: on CRAN means they passed cmd check 00:45:05 Jonathan Trattner: Well yeah 00:45:09 Jonathan Trattner: But it also has some nice tests 00:45:12 Tony ElHabr: yoni is going to need to interview him before he approves of the package 00:45:15 David Severski: A little basic looking. https://github.com/stevecondylios/priceR/blob/master/R/adjust_for_inflation.R#L298-L321 00:45:22 Jonathan Trattner: Using api for world bank 00:46:08 David Severski: I tend to use indices direct from FRED for a lot of my own inflation conversion work. 00:46:31 Jon Harmon (jonthegeek): Yeah, it definitely depends how important exact numbers are to you. 00:46:50 Kevin Kent: I guess inflation would be a feature you’d have to forecast out if you wanted to get predictions for the future? But still noodling on that. 00:47:07 Tyler Grant Smith: it definitely is 00:47:29 Jon Harmon (jonthegeek): It looks like that package allowed for future inflation. 00:47:41 Jon Harmon (jonthegeek): (Conor commented out that part, but it showed in his code) 00:48:53 Kevin Kent: Oh nice. I feel like I run into that a lot in forecasting contexts - needing to be careful about the features and how uncertain they are in the future. 00:49:41 Scott Nestler: An aside since we have some sports fans in the group. Pine-Richland is where Phil Jurkovic, the former ND backup QB (who&#39;s now the starter at BC) is from. 00:49:54 Asmae Toumi: nice 00:52:29 Tyler Grant Smith: id definitely consider esp since a lot of pricing is done as $/sqft 00:52:38 David Severski: I wonder if lot sizes would discretize cleanly. Lot sizes tend to bin, right? 00:52:44 Tyler Grant Smith: did you engineer something like that 00:52:58 Jarad Jones: To go along with Jon’s question about log transforming…..how would you all decide to do that or not? 00:53:38 Scott Nestler: How did you collapse factor variables? With fct_lump_n() or something else? 00:53:46 David Severski: Jarad - Plotting out the distributions is something I try to do consistently, then look to transforms to get close-er to a normal distribution. 00:54:18 Tyler Grant Smith: not just log transform but box cox transforms more generally 00:54:22 Kevin Kent: I’d say it also depends on the assumptions of the model, and if they require normally distributed features. 00:54:22 Darya Vanichkina: Yes, like David - eyeball it :( 00:54:34 Tony ElHabr: yup. non negative is big use case 00:54:38 Asmae Toumi: In the words of the iconic Andrew German, “Log transform, kids. And don’t listen to people who tell you otherwise.” 00:54:45 Darya Vanichkina: Box Cox or Yeo Johnson 00:54:46 Asmae Toumi: link:https://statmodeling.stat.columbia.edu/2019/08/21/you-should-usually-log-transform-your-positive-data/ 00:54:56 Tyler Grant Smith: i would talk but i have lots of loud kids around me 00:55:05 David Severski: “And trust me about the sunscreen…” ;) 00:55:33 Asmae Toumi: Don’t forget the two finger rule for sunscreen also 00:55:37 Darya Vanichkina: Kevin, I think that because we could be comparing models which do/do not require normally distributed residuals I’d transform (and then compare) 00:55:42 Arjun Paudel: anytime you have a big tail 00:55:55 Tony ElHabr: also, if you&#39;re combining two predictions, I think log-transformed has good theoretical properties 00:56:13 Kevin Kent: Yeah that makes sense 00:56:47 Jarad Jones: That’s helpful, thanks! 00:56:52 Scott Nestler: That student was trying to maximize their leverage. 00:57:05 Jon Harmon (jonthegeek): They left and came back to the assignment but it&#39;s sometimes hard to see that. 00:57:13 Darya Vanichkina: It doesn’t - but you’re usually comparing the performance of the two 00:57:18 Darya Vanichkina: Right? 00:57:43 Tony ElHabr: yeah, I don&#39;t think it really needs it. but never hurts to try multiple methods 00:57:52 Tony ElHabr: tyler is the truth teller 00:58:00 Tony ElHabr: he got the kids to calm down for long enough 00:58:27 Tyler Grant Smith: yes that is one reason to do it 00:58:34 Tyler Grant Smith: theyre in the bath now 00:58:39 pavitra: for scientific assays, the dilutions are so large in range, I absolutely need to log-transform the data to make any sense of it 00:58:46 Jon Harmon (jonthegeek): ^^^ 00:59:00 Tony ElHabr: also, you look smarter if you log transform 00:59:04 Darya Vanichkina: LOL 00:59:07 Tony ElHabr: your audience will think you know what you&#39;re doing 00:59:10 shamsuddeen: lol 00:59:16 Kevin Kent: Lol fantastic 00:59:33 pavitra: john murdoch 00:59:33 Darya Vanichkina: I loved the RStudio conf talk where the FT journalist pros/cons of it 00:59:47 Darya Vanichkina: Yes, there are also “normal people”… 01:00:23 Tyler Grant Smith: lognormal people 01:01:35 Scott Nestler: 1 Full and 7 Half ??? 01:02:13 Jonathan Trattner: Maybe they’re complementary halves? 01:02:13 Tyler Grant Smith: shower in the bedroom but i cant be bothered to go to another room for #2 01:03:39 Tan Ho: doesn&#39;t your house have a three-urinal men&#39;s washroom separate from a three-stall women&#39;s washroom? 01:03:40 Darya Vanichkina: If possible, please, I’d love some documentation for all of the .R scripts on GitHub to make your thought process/prototyping clearer…. 01:03:43 Asmae Toumi: Is this on GitHub? I have a small aesthetic suggestion for the leaflet map so that the labels are on top of the color 01:04:15 Jon Harmon (jonthegeek): I believe it is and we&#39;ll make him share it in the channel/in the bookdown :) 01:04:17 Darya Vanichkina: I think it’s here? https://github.com/conorotompkins/model_allegheny_house_sales 01:04:28 Tan Ho: Yup, that&#39;s the one 01:04:43 Asmae Toumi: thanks 01:05:00 Darya Vanichkina: Sorry, need to run - thank you, everyone! 01:05:13 pavitra: has connor already developed any models on this data? 01:05:13 Jon Harmon (jonthegeek): See ya Darya! 01:05:26 Tan Ho: https://github.com/conorotompkins/model_allegheny_house_sales/tree/main/scripts/model @pavitra 01:05:28 Jon Harmon (jonthegeek): @pavitra: Yup! he predicts the price based on those settings. 01:05:47 pavitra: oh boy! neat 01:06:33 David Severski: Gotta run now. Thanks for the talk and the conversation! 01:07:30 Jon Harmon (jonthegeek): A numeric value between 0 and 1 or an integer greater or equal to one. If it&#39;s less than one then factor levels whose rate of occurrence in the training set are below threshold will be &quot;othered&quot;. If it&#39;s greater or equal to one then it&#39;s treated as a frequency and factor levels that occur less then threshold times will be &quot;othered&quot;. 01:09:13 Jon Harmon (jonthegeek): A logical. Should the step be skipped when the recipe is baked by bake.recipe()? While all operations are baked when prep.recipe() is run, some operations may not be able to be conducted on new data (e.g. processing the outcome variable(s)). Care should be taken when using skip = TRUE as it may affect the computations for subsequent operations 01:11:30 Jim Gruman: thankyou Conor 01:11:40 Kevin Kent: Nice job! Was helpful to talk through code and the concepts represented in it. 01:11:44 Tan Ho: thank you! that was awesome! 01:11:48 Jonathan Leslie: Thanks, Conor! 01:11:51 caroline: Thank you Conor 01:11:52 Joe Sydlowski: Thanks Conor! 01:11:56 Laurens Put: Thank you 01:11:59 Asmae Toumi: Conor that was awesome. I hope it ends up on tidytuesday 01:12:00 Jarad Jones: Nice job Conor, the whole end product is pretty impressive for a first shiny app! 01:12:00 pavitra: thanks a lot, Connor..i think you finished the purpose of the book 4.2.2 Cohort 2 Meeting chat log 00:07:46 Janita Botha: I am new here! My name is Janita (she/they) and I am a PhD student in sensory science. Not a programmer but need Tidy models to work for me... I&#39;ve caught up with the YouTube videos of the other cohorts 00:08:15 Janita Botha: Also, I am based it New Zealand. It is 7AM on a Monday morning here 00:08:24 Roberto Villegas-Diaz: Welcome! 00:08:33 Stephen Holsenbeck: wow! 00:09:05 shamsuddeen: Welcome Janita, you are welcome. 00:09:15 August: Hi Janita, Welcome to the best cohort :D 00:18:53 Luke Shaw: In answer to the question on differences between the Ames data sets, I think it&#39;s explained here: 00:18:57 Luke Shaw: https://github.com/topepo/AmesHousing/blob/master/R/make_ames.R 00:20:46 August: I really love dlookr for exploratory data analysis: https://github.com/choonghyunryu/dlookr and skimr for initial views of data structures https://cran.r-project.org/web/packages/skimr/vignettes/skimr.html. 00:21:35 Kevin Kent: Thanks august. I’ve never heard of dlookr 00:23:52 shamsuddeen: a logarithmic transform may also stabilize the variance in a way that makes inference more legitimate. 00:24:36 Janita Botha: As far as I understand it also reduces the impact of outliers on modelling 00:29:29 rahul bahadur: Yes, One assumption of linear regression is that variance is constant. If this is not met, a log transform might help in keeping the variance constant. 00:34:33 Kevin Kent: This is the code for the book https://github.com/tidymodels/TMwR 00:35:59 shamsuddeen: pathological distributions 00:36:41 Kevin Kent: “The Cauchy distribution is often used in statistics as the canonical example of a &quot;pathological&quot; distribution since both its expected value and its variance are undefined” 00:37:02 Kevin Kent: https://en.wikipedia.org/wiki/Cauchy_distribution 00:37:34 Kevin Kent: https://en.wikipedia.org/wiki/Pathological_(mathematics) 00:41:15 Kevin Kent: https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/ 00:46:01 Janita Botha: Yes! &quot;Getting used to it&quot; is such a good way of saying it 00:46:43 shamsuddeen: Yes, that’s right 00:50:58 Kevin Kent: Principal component analysis (PCA) would be one approach to reduce correlated predictors to a smaller group of predictors 00:51:19 Carlo M: specifically for multicollinearity: the easiest mental model I have is to take it to the &#39;extreme&#39; case. Suppose you have y ~ x1 + x2, and you have x1 and x2 perfectly correlated, e.g. x2 = 2*x1 (x2 is perfectly calculable with that relationship). then, it&#39;s possible to just do the following y ~ x1 + (2*x1) =&gt; y~3x1. That is, one can just do y ~ x1 (via model parsimony) 00:52:15 Carlo M: you&#39;re welcome :) 00:57:32 Janita Botha: All good 00:57:35 Janita Botha: :) 00:57:42 shamsuddeen: Thank you all 00:57:59 rahul bahadur: Thanks 00:58:43 Janita Botha: Thanks.... 4.2.3 Cohort 3 You can check out the EDA script presented by Jiwan Heo here. Meeting chat log 00:03:49 Daniel Chen: https://r4ds.github.io/bookclub-tmwr/ 00:08:31 jiwan: https://cran.r-project.org/web/packages/AmesHousing/AmesHousing.pdf 00:17:34 Daniel Chen: can you show qqplot of the transforemed plot? 00:18:27 Daniel Chen: it should be qqnorm(VALUES) 00:18:48 Daniel Chen: http://www.sthda.com/english/wiki/qq-plots-quantile-quantile-plots-r-base-graphs 00:24:18 Chris Martin: No, audio or video from me today I&#39;m afraid … 00:33:15 Toryn Schafer: Thanks for the demo, Jiwan, have to head out early! 00:47:41 Hannah Frick: I also need to leave a littel early, thanks for the live demo - always interesting to see people explore datasets! 00:53:53 Chris Martin: Thanks for the demo! 00:56:44 Chris Martin: I some times end up coming back to EDA after doing some modelling. So, try not to worry too much about covering everything in an initial EDA. 00:58:48 Chris Martin: Thanks everyone 4.2.4 Cohort 4 Meeting chat log 00:23:01 Federica Gazzelloni: book club notes: https://r4ds.github.io/bookclub-tmwr/the-ames-housing-data.html 00:50:24 Laura Prado de Assis: here&#39;s what&#39;s in the book reg_ames &lt;- lm(Sale_Price ~ Neighborhood, ames) 00:50:38 Laura Prado de Assis: sorry, I said lat+long, but it was neighborhood 00:51:05 Laura Prado de Assis: ames &lt;- ames %&gt;% mutate(Sale_Price = log10(Sale_Price)) 01:01:56 Stephen Charlesworth: I have to drop off, too, thanks all! "],["spending-our-data.html", "Chapter 5 Spending our data", " Chapter 5 Spending our data Learning objectives: Use {rsample} to split data into training and testing sets. Identify cases where stratified sampling is useful. Understand the difference between rsample::initial_time_split() and rsample::initial_split(). Understand the trade-offs between too little training data and too little testing data. Define a validation set of data. Explain why data should be split at the independent experimental unit level. "],["spending-our-data-1.html", "5.1 Spending our data", " 5.1 Spending our data The task of creating a useful model can be daunting. Thankfully, one can do so step-by-step. It can be helpful to sketch out your path, as Chanin Nantasenamat has done so: We’re going to zoom into the data splitting part. As the diagram shows, it is one of the earliest considerations in a model building workflow. The training set is the data that the model(s) learns from. It’s usually the majority of the data (~ 80-70% of the data), and you’ll be spending the bulk of your time working on fitting models to it. The test set is the data set aside for unbiased model validation once a candidate model(s) has been chosen. Unlike the training set, the test set is only looked at once. Why is it important to think about data splitting? You could do everything right, from cleaning the data, collecting features and picking a great model, but get bad results when you test the model on data it hasn’t seen before. If you’re in this predicament, the data splitting you’ve employed may be worth further investigation. "],["common-methods-for-splitting-data.html", "5.2 Common methods for splitting data", " 5.2 Common methods for splitting data Choosing how to conduct the split of the data into training and test sets may not be a trivial task. It depends on the data and the purpose. The most common type of sampling is known as random sampling and it is done readily in R using the rsample package with the initial_split()function. For the Ames housing dataset, the call would be: library(tidymodels) set.seed(123) data(ames) ames_split &lt;- initial_split(ames, prop = 0.80) ames_split ## &lt;Training/Testing/Total&gt; ## &lt;2344/586/2930&gt; The object ames_split is an rsplit object. To get the training and test results you can call on training() and test(): ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) "],["class-imbalance.html", "5.3 Class imbalance", " 5.3 Class imbalance In many instances, random splitting is not suitable. This includes datasets that contain class imbalance, meaning one class is dominated by another. Class imbalance is important to detect and take into consideration in data splitting. Performing random splitting on a dataset with severe class imbalance may cause the model to perform badly at validation. You want to avoid allocating the minority class disproportionately into the training or test set. The point is to have the same distribution across the training and test sets. Class imbalance can occur in differing degrees: Splitting methods suited for datasets containing class imbalance should be considered. Let’s consider a #Tidytuesday dataset on Himalayan expedition members, which Julia Silge recently explored here using {tidymodels}. library(tidyverse) library(skimr) members &lt;- read_csv(&quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-22/members.csv&quot;) skim(members) Table 5.1: Data summary Name members Number of rows 76519 Number of columns 21 _______________________ Column type frequency: character 10 logical 6 numeric 5 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace expedition_id 0 1.00 9 9 0 10350 0 member_id 0 1.00 12 12 0 76518 0 peak_id 0 1.00 4 4 0 391 0 peak_name 15 1.00 4 25 0 390 0 season 0 1.00 6 7 0 5 0 sex 2 1.00 1 1 0 2 0 citizenship 10 1.00 2 23 0 212 0 expedition_role 21 1.00 4 25 0 524 0 death_cause 75413 0.01 3 27 0 12 0 injury_type 74807 0.02 3 27 0 11 0 Variable type: logical skim_variable n_missing complete_rate mean count hired 0 1 0.21 FAL: 60788, TRU: 15731 success 0 1 0.38 FAL: 47320, TRU: 29199 solo 0 1 0.00 FAL: 76398, TRU: 121 oxygen_used 0 1 0.24 FAL: 58286, TRU: 18233 died 0 1 0.01 FAL: 75413, TRU: 1106 injured 0 1 0.02 FAL: 74806, TRU: 1713 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist year 0 1.00 2000.36 14.78 1905 1991 2004 2012 2019 ▁▁▁▃▇ age 3497 0.95 37.33 10.40 7 29 36 44 85 ▁▇▅▁▁ highpoint_metres 21833 0.71 7470.68 1040.06 3800 6700 7400 8400 8850 ▁▁▆▃▇ death_height_metres 75451 0.01 6592.85 1308.19 400 5800 6600 7550 8830 ▁▁▂▇▆ injury_height_metres 75510 0.01 7049.91 1214.24 400 6200 7100 8000 8880 ▁▁▂▇▇ Let’s say we were interested in predicting the likelihood of survival or death for an expedition member. It would be a good idea to check for class imbalance: library(janitor) members %&gt;% tabyl(died) %&gt;% adorn_totals(&quot;row&quot;) ## died n percent ## FALSE 75413 0.98554607 ## TRUE 1106 0.01445393 ## Total 76519 1.00000000 We can see that nearly 99% of people survive their expedition. This dataset would be ripe for a sampling technique adept at handling such extreme class imbalance. This technique is called stratified sampling, in which “the training/test split is conducted separately within each class and then these subsamples are combined into the overall training and test set”. Operationally, this is done by using the strata argument inside initial_split(): set.seed(123) members_split &lt;- initial_split(members, prop = 0.80, strata = died) members_train &lt;- training(members_split) members_test &lt;- testing(members_split) 5.3.1 Stratified sampling simulation With simulation we can see the effect of stratification: we expect that the expected value does not change with stratification but the variance is lower. simulate_stratified_sampling &lt;- function(prop_in_dataset, n_resample = 50, n_rows = 1000, seed = 45678) { set.seed(seed) data_to_split &lt;- tibble(died = c( rep(1, floor(n_rows * prop_in_dataset)), rep(0, floor(n_rows * (1 - prop_in_dataset))) )) samples &lt;- map_dfr(seq_len(n_resample), ~{ initial_split(data_to_split) %&gt;% testing() %&gt;% summarize(died_pct = mean(died)) }) stratified_samples &lt;- map_dfr(seq_len(n_resample), ~{ initial_split(data_to_split, strata = died) %&gt;% testing() %&gt;% summarize(died_pct = mean(died)) }) rbind( samples %&gt;% mutate(stratified = FALSE), stratified_samples %&gt;% mutate(stratified = TRUE) ) %&gt;% group_by(stratified) %&gt;% summarize(mean = mean(died_pct), var = var(died_pct)) } rsample does not stratify if class imbalance is more extreme than 10% simulate_stratified_sampling(0.09) ## # A tibble: 2 × 3 ## stratified mean var ## &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 FALSE 0.0910 0.000240 ## 2 TRUE 0.0917 0.000257 Stratified sampling happens: simulate_stratified_sampling(0.11) ## # A tibble: 2 × 3 ## stratified mean var ## &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 FALSE 0.110 0.000257 ## 2 TRUE 0.112 0 "],["continuous-outcome-data.html", "5.4 Continuous outcome data", " 5.4 Continuous outcome data For continuous outcome data (e.g. costs), a stratified random sampling approach would involve conducting a 80/20 split within each quartile and then pool the results together. For the Ames housing dataset, the call would look like this: set.seed(123) ames_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) "],["time-series-data.html", "5.5 Time series data", " 5.5 Time series data For time series data where you’d want to allocate data to the training set/test set depending on a sorted order, you can use initial_time_split() which works similarly to initial_split(). The prop argument can be used to specify what proportion of the first part of data should be used as the training set. data(drinks) drinks_split &lt;- initial_time_split(drinks) train_data &lt;- training(drinks_split) test_data &lt;- testing(drinks_split) The lag argument can specify a lag period to use between the training and test set. This is useful if lagged predictors will be used during training and testing. drinks_lag_split &lt;- initial_time_split(drinks, lag = 12) train_data_lag &lt;- training(drinks_lag_split) test_data_lag &lt;- testing(drinks_lag_split) c(max(train_data_lag$date), min(test_data_lag$date)) ## [1] &quot;2011-03-01&quot; &quot;2010-04-01&quot; "],["multi-level-data.html", "5.6 Multi-level data", " 5.6 Multi-level data It’s important to figure out what the independent experimental unit is in your data. In the Ames dataset, there is one row per house and so houses and their properties are considered to be independent of one another. In other datasets, there may be multiple rows per experimental unit (e.g. as in patients who are measured multiple times across time). This has implications for data splitting. To avoid data from the same experimental unit being in both the training and test set, split along the independent experimental units such that X% of experimental units are in the training set. Example: # data source: http://www.bristol.ac.uk/cmm/learning/mmsoftware/data-rev.html#oxboys child_heights &lt;- read_delim(here::here(&quot;data/Oxboys.txt&quot;), col_names = FALSE, delim = &quot; &quot;) %&gt;% purrr::set_names(c(&quot;child_id&quot;, &quot;age_norm&quot;, &quot;height&quot;, &quot;measurement_id&quot;, &quot;season&quot;)) head(child_heights) ## # A tibble: 6 × 5 ## child_id age_norm height measurement_id season ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -1 140. 1 7.32 ## 2 1 -0.747 143. 2 9.36 ## 3 1 -0.463 145. 3 0.84 ## 4 1 -0.164 147. 4 4.32 ## 5 1 -0.002 148. 5 6.24 ## 6 1 0.247 150. 6 9.36 Depending on the modeling problem we may want to split the data to train and test set in a way that data points for children remain together. You can do this with the following code. child_heights_split &lt;- child_heights %&gt;% group_nest(child_id) %&gt;% initial_split() child_heights_train &lt;- training(child_heights_split) %&gt;% unnest(data) For other task it might be more suitable to split along measurement id and all childrens’ last measurement will be the test set. "],["what-proportion-should-be-used.html", "5.7 What proportion should be used?", " 5.7 What proportion should be used? https://twitter.com/asmae_toumi/status/1356024351720689669?s=20 Some people said the 80/20 split comes from the Pareto principle/distribution or the power law. Some said because it works nicely with 5-fold cross-validation (which we will see in the later chapters). I believe the point is to use enough data in the training set to allow for solid parameter estimation but not too much that it hurts performance. 80/20 or 70/30 seems reasonable for most problems at hand, as it’s what is widely used. Max Kuhn notes that a test set is almost always a good idea, and it should only be avoided when the data is “pathologically small”. "],["summary.html", "5.8 Summary", " 5.8 Summary Data splitting is an important part of a modeling workflow as it impacts model validity and performance. The most common splitting technique is random splitting. Some data, such as time-series or multi-level data require a different data splitting technique called stratified sampling. The rsample package contains many functions that can perform random splitting and stratified splitting. We will learn more about how to remedy certain issues such as class imbalance, bias and overfitting in Chapter 10. 5.8.1 References Tidy modeling with R by Max Kuhn and Julia Silge: https://www.tmwr.org/splitting.html Feature Engineering and Selection: A Practical Approach for Predictive Models by Max Kuhn and Kjell Johnson: https://bookdown.org/max/FES/ Handle class imbalance in #TidyTuesday climbing expedition data with tidymodels: https://juliasilge.com/blog/himalayan-climbing/ Data preparation and feature engineering for machine learning: https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data How to Build a Machine Learning Model by Chanin Nantasenamat: https://towardsdatascience.com/how-to-build-a-machine-learning-model-439ab8fb3fb1 "],["meeting-videos-4.html", "5.9 Meeting Videos", " 5.9 Meeting Videos 5.9.1 Cohort 1 Meeting chat log 00:09:24 Jon Harmon (jonthegeek): Oops typo! Tan shouldn&#39;t have accepted that! 00:10:05 Tan Ho: umm 00:10:18 Tyler Grant Smith: I never take notes, but if I did, I wish I would take them like this 00:10:24 Jon Harmon (jonthegeek): Certainly can&#39;t be MY fault for TYPING it. 00:10:32 Tan Ho: what typo are we talking about? 00:10:36 Tony ElHabr: you don&#39;t need docs if you got diagrams like this 00:10:43 Jon Harmon (jonthegeek): &quot;too list training data&quot; 00:10:55 Tan Ho: also...is &quot;data spending&quot; a hadleyism? 00:10:57 Tyler Grant Smith: I also object to this order 00:11:11 Jonathan Trattner: @Jon My professor always gets mad at R for doing what she tells it to instead of what she wants it to do 00:11:12 Jon Harmon (jonthegeek): I think it&#39;s a Maxim. And I like it. 00:11:17 Tyler Grant Smith: preprocessing needs to be done after the split 00:11:31 Tyler Grant Smith: some of it does anyway... 00:11:40 Jonathan Trattner: Does PCA Tyler? 00:11:58 Tyler Grant Smith: yes, I would say so 00:12:35 Jonathan Trattner: 👍🏼 00:12:47 Jon Harmon (jonthegeek): We&#39;ll talk about processing in the next chapter :) 00:12:56 Tony ElHabr: pre-processing is done after the splitting in the normal tidy workflow. I guess the diagram was just &quot;wrong&quot;? 00:13:38 Jon Harmon (jonthegeek): It can make sense to do the processing before splitting if you don&#39;t have a nice system like recipes to make sure they&#39;re processed the same. 00:14:07 Tyler Grant Smith: it can make sense to be wrong too :) 00:14:15 Jonathan Trattner: Also if you can reduce the dimensionality of it before hand, would it not make sense to do that first and split the simpler data? 00:14:29 Jon Harmon (jonthegeek): The idea is you should treat your test data the same as you&#39;d treat new data. 00:14:54 Jon Harmon (jonthegeek): If you do it before the split, you might do something that&#39;s hard to do or might include it in an ~average, etc, and thus leak into the training data. 00:15:12 Jonathan Trattner: That makes sense, thanks! 00:15:16 Jarad Jones: Class imbalance, perfect! I was hoping to go over how to decide between upsampling or downsampling 00:15:39 Jon Harmon (jonthegeek): We won&#39;t do much there yet, he goes into it more in 10 I think. 00:15:59 Jon Harmon (jonthegeek): But feel free to ask Asmae about it! 00:16:12 Jarad Jones: Haha, shoot, will have to wait a bit then 00:17:02 Tyler Grant Smith: question for later: for what types models is upsampling/downsampling suggested/necessary? I find in xgboost, for example, that I rarely need to do it. or at least that it doesn&#39;t make the model results any better 00:18:09 Maya Gans: +1 this question ^^^ 00:18:13 Conor Tompkins: Tabyl is such a useful function 00:18:29 Tyler Grant Smith: janitor as a whole is fantastic 00:18:45 Jordan Krogmann: janitor::clean_names() mvp 00:18:56 Jonathan Trattner: Huge facts ^^ 00:18:58 Tyler Grant Smith: ^ 00:19:03 Jon Harmon (jonthegeek): Correction: He briefly mentions upsampling in the next chapter. 00:19:09 arjun paudel: is it prob or prop? I thought the argument for initial_split was prop 00:19:25 Scott Nestler: Yes! We recently did a &quot;Blue Collar Data Wrangling&quot; class with coverage of janitor and plumber. 00:19:29 Tony ElHabr: the upsampling/downsampling question is a good one. I think frameworks that use boosting/bagging may not need it, but it&#39;s always worth testing 00:20:07 Tony ElHabr: the downside is not using stratification 00:20:36 Tan Ho: always log, always stratify 00:20:37 Tan Ho: got it 00:22:14 Tan Ho: *looks around nervously* 00:22:51 Jordan Krogmann: I mean youre not going to not log 00:23:12 Jordan Krogmann: *waiting for the number of counter articles* 00:24:00 Jon Harmon (jonthegeek): Woot, I have a PR accepted in this book now (for a minor typo at the end of this chapter) :) 00:24:01 Tyler Grant Smith: I gotta imagine that stratified sampling and random sampling converge as n-&gt;inf 00:24:23 Tony ElHabr: law of large numbers 00:24:25 Tyler Grant Smith: and it happens probably pretty quickly 00:24:43 Jon Harmon (jonthegeek): Yeah, I guess a downside would be if you stratify so much that it doesn&#39;t make sense and causes rsample to complain. 00:25:12 Jon Harmon (jonthegeek): There&#39;s a minor change starting next chapter, not yet merged: https://github.com/tidymodels/TMwR/pull/106/files 00:27:55 Tyler Grant Smith: i frequently work with data like this 00:28:18 Conor Tompkins: It would be interesting to have a table of model types and how they react to things like missingness, class imbalance, one-hot encoding etc. so you can choose the appropriate model for the specific weirdness of your data. 00:28:36 Tony ElHabr: so at what point do you use longitudinal model over something else 00:29:31 Jordan Krogmann: student re-enrollment cycle... how does the last term impact future terms 00:31:14 Tony ElHabr: memes in the wild 00:31:17 Tony ElHabr: i&#39;m here for it 00:31:20 Jon Harmon (jonthegeek): Yup! And there&#39;s a whole thing about the fact that each question a student answers technically influences the next one, even if they don&#39;t get feedback. 00:32:57 Scott Nestler: I recall learning (many years ago) about using 3 sets -- Training, Test, and Validation. Training to train/build models, Validation to assess the performance of different (types of) models on data not used to train them, and then Test to fine-tune model parameters once you have picked one. The splits were usually something like 70/15/15 or 80/10/10. This didn&#39;t seem to be discussed in this chapter. Any idea why? 00:33:37 Jon Harmon (jonthegeek): We&#39;ll talk about validation later, I think. There&#39;s a minute of it. Gonna talk about this out loud in a sec... 00:34:43 Tyler Grant Smith: 5.3 What about a validation set? 00:35:49 Tony ElHabr: If you do cross-validation, the CV eval metric is effectively your validation 00:35:50 Jonathan Trattner: What about cross-validation on the training set? Is that different than what we’re discussing now? 00:35:53 Tony ElHabr: and your training 00:36:10 Tyler Grant Smith: ya...split first train-validate and test and then split train-validate into train and validate 00:36:42 Jarad Jones: I think cross-validation is used during model training on the training set 00:37:08 Ben Gramza: I actually watched a &quot;deep-learning&quot; lecture on this today. The guy said that a validation set is used to select your parameters/hyperparameters, then you test your tuned model on the test set. 00:40:11 Tony ElHabr: validation makes more sense when you&#39;re comparing multiple model frameworks too. the best one on the validation set is what is ultimately used for the test set 00:41:45 Jordan Krogmann: i think it comes into play when you are hyperparameter tuning for a single model 00:44:21 Ben Gramza: yeah, for example if you are using a K-nearest neighbor model, you use the validation set on your models with K=1, 2, 3, … . You select the best performing K from the validation set, then test that on the test set. 00:46:22 Joe Sydlowski: Good question! 00:46:28 Jordan Krogmann: i do it on all of it 00:46:45 Jordan Krogmann: annnnnnnnnnd i am probably in the wrong lol 00:50:20 Jordan Krogmann: yuup otherwise you will cause leakage 00:57:41 Tyler Grant Smith: i suppose I need to add inviolate to my day-to-day vernacular 00:58:52 Jon Harmon (jonthegeek): I&#39;m noticing myself say that over and over and I don&#39;t know why! 00:59:50 Tony ElHabr: i had to google that 01:05:17 Conor Tompkins: Great job asmae! 01:05:22 Jonathan Trattner: ^^^ 01:05:28 Tony ElHabr: Pavitra getting ready for recipes 01:05:37 Jordan Krogmann: great job! 01:05:42 Joe Sydlowski: Thanks Asmae! 01:05:46 Andy Farina: That was great Asmae, thank you! 01:05:47 Pavitra Chakravarty: 🤣🤣🤣🤣 01:05:56 caroline: Thank you Asmae :) 01:05:59 Pavitra Chakravarty: great presentation Asmae 5.9.2 Cohort 2 Meeting chat log 00:07:17 Janita Botha: Sorry I&#39;m late... Been slow booting up... 00:08:46 Amélie Gourdon-Kanhukamwe (she/they): https://supervised-ml-course.netlify.app/ 00:08:58 shamsuddeen: Thank you 00:09:07 Stephen Holsenbeck: thanks! 00:22:06 Janita Botha: Just a side warren... I find the focus on testing vs training data in tidymodels very frustrating since the field that I am in focusses more on inferential statistics because we tend to have relatively small sample sizes for the large amount of variance we encounter 00:22:52 Janita Botha: In other words in my field my data is ususally better &quot;spent&quot; as traininig data... 00:38:09 Louis Carlo Medina: Thanks Rahul! yeah, I think I conflated oversampling with strata. I think I remember the strata now, where you actually sample within groups as opposed to the group as a whole. 00:38:29 shamsuddeen: Yes, ! 00:41:00 rahul bahadur: No worries. 00:42:26 Mikhael Manurung: It should be random regardless whether strata is specified or not 00:43:00 rahul bahadur: For stratified random sampling, the strata are created first and then random samples are taken from it 00:43:31 shamsuddeen: Why don’t we stratified all the time? 00:44:14 rahul bahadur: it is not needed when the data is balanced. However, you can 00:44:16 shamsuddeen: The book says: “There is very little downside to using stratified sampling. “ 00:44:40 shamsuddeen: Ah, I see. Stratified is for imbalance data 00:44:52 shamsuddeen: Thanks raul. 00:45:01 Stephen Holsenbeck: stratification should basically be the default 00:45:06 shamsuddeen: *rahul 00:45:36 Stephen Holsenbeck: If you go completely random, your classes in the test set may not match the category distributions in the dataset 00:45:45 August: https://otexts.com/fpp2/accuracy.html 00:45:48 Stephen Holsenbeck: same with the training set 00:46:02 August: this is a good diagram for time series cross validation 00:46:34 August: Training test at top of section 00:46:46 Janita Botha: @Amelie that is a really good question - you should add that to the questions for Julia and Max 00:47:11 Louis Carlo Medina: ^+1. Hyndman et al&#39;s texts for timeseries stuff are really good 00:47:43 rahul bahadur: Yes, Hyndman has good timeseries 00:49:43 shamsuddeen: From the book. 00:49:44 shamsuddeen: Too much data in the training set lowers the quality of the performance estimates. 00:49:52 shamsuddeen: too much data in the test set handicaps the model’s ability to find appropriate parameter estimates. 00:50:14 shamsuddeen: What is difference between performance estimates and parameter estimates.? 00:51:26 Janita Botha: https://otexts.com/fpp3/ 00:51:35 rahul bahadur: Parameter estimates, in case of regression for example, would be the beta estimates 00:51:38 Amélie Gourdon-Kanhukamwe (she/they): Performance estimates = grossly statistics assessing the quality of the model, such as MSE, percentage correct, area under the curve 00:51:45 rahul bahadur: Performance estimates = MSE 00:51:47 Amélie Gourdon-Kanhukamwe (she/they): And yes, as Rahul 00:52:08 Janita Botha: I shared the link above 00:52:13 Kevin Kent: Modeltime - https://cran.r-project.org/web/packages/modeltime/vignettes/getting-started-with-modeltime.html 00:52:32 Amélie Gourdon-Kanhukamwe (she/they): Thanks Janitha 00:55:56 Kevin Kent: https://physionet.org/ 00:59:34 Amélie Gourdon-Kanhukamwe (she/they): https://cran.r-project.org/web/packages/anomalize/vignettes/anomalize_quick_start_guide.html 00:59:38 August: https://cran.r-project.org/web/packages/anomalize/anomalize.pdf 00:59:58 August: https://cran.r-project.org/web/packages/anomalize/vignettes/anomalize_methods.html 5.9.3 Cohort 3 Meeting chat log 00:23:04 Daniel Chen (he/him): I don&#39;t know why it&#39;s 10% 00:23:23 Daniel Chen (he/him): it seems like a good heuristic? 00:29:33 Daniel Chen (he/him): (Strata below 10% of the total are pooled together.) 00:54:00 Ildiko Czeller: https://spatialsample.tidymodels.org/reference/spatialsample.html 5.9.4 Cohort 4 Meeting chat log 00:17:06 Isabella Velásquez: Need an R2Cobalt package... 00:17:16 Laura Rose: yes! 00:28:03 Isabella Velásquez: https://juliasilge.com/blog/ 00:29:36 Isabella Velásquez: Supervised Machine Learning for Text Analysis in R: https://smltar.com/ 00:29:45 Isabella Velásquez: ISLR tidymodels Labs: https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/index.html 00:31:46 Isabella Velásquez: A couple more posts! https://www.danielphadley.com/bootstrap_tutto/ 00:31:51 Isabella Velásquez: https://www.rebeccabarter.com/blog/2020-03-25_machine_learning/ 00:32:10 Stephen Charlesworth: Here’s a spreadsheet w/ different tidymodels uses: https://docs.google.com/spreadsheets/d/1PzbtanwieCCpaUt9G6XMhKXQ_0cBWfq_9xG9bH33Occ/edit#gid=652338894 00:33:34 Federica Gazzelloni: https://www.tidymodels.org/ 00:52:38 Stephen Charlesworth: https://twitter.com/towards_ai/status/1332567246011555840 00:53:06 Laura Rose: 😆 00:55:13 Isabella Velásquez: https://en.wikipedia.org/wiki/Pareto_principle 01:05:39 Laura Rose: cross validation using modeltime https://cran.r-project.org/web/packages/modeltime.resample/vignettes/getting-started.html "],["fitting-models-with-parsnip.html", "Chapter 6 Fitting models with parsnip", " Chapter 6 Fitting models with parsnip Learning objectives: Identify ways in which model interfaces can differ. x Specify a model in {parsnip}. x Fit a model with parsnip::fit() and parsnip::fit_xy(). x Describe how {parsnip} generalizes model arguments. x Use broom::tidy() to convert model objects to a tidy structure. x Use dplyr::bind_cols() and the predict() methods from {parsnip} to make tidy predictions. Find interfaces to other models in {parsnip}-adjacent packages. Modeling Map modeling flow Chapter Setup Below # load parsnip, recipes, rsample, broom... library(tidymodels) library(AmesHousing) # attach data data(ames) # log scale price ames &lt;- mutate(ames, Sale_Price = log10(Sale_Price)) # train/test set.seed(123) ames_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) "],["create-a-model.html", "6.1 Create a Model", " 6.1 Create a Model 6.1.1 Different Model Interfaces Different-interfaces Model Interfaces Different Implementations = Different Interfaces Linear Regression can be implemented in many ways Ordinary Least Squares Regularized Linear Regression … {stats} takes formula uses data.frame lm(formula, data, ...) {glmnet} Has x/y interface Uses a matrix glmnet(x = matrix, y = vector, family = &quot;gaussian&quot;, ...) 6.1.2 Model Specification model specification {tidymodels}/{parsnip} - Philosophy is to unify &amp; make interfaces more predictable. Specify model type (e.g. linear regression, random forest …) linear_reg() rand_forest() Specify engine (i.e. package implementation of algorithm) set_engine(\"some package's implementation\") declare mode (e.g. classification vs linear regression) use this when model can do both classification &amp; regression set_mode(\"regression\") set_mode(\"classification\") Bringing it all together lm_model_spec &lt;- linear_reg() %&gt;% # specify model set_engine(&quot;lm&quot;) # set engine lm_model_spec ## Linear Regression Model Specification (regression) ## ## Computational engine: lm 6.1.3 Model Fitting From above we will use our existing model specification fit() any nominal or categorical variables will be split out into dummy columns most formula methods also turn do the same thing fit_xy delays creating dummy variable and has underlying model function # create model fit using formula lm_form_fit &lt;- lm_model_spec %&gt;% fit(Sale_Price ~ Longitude + Latitude, data = ames_train) # create model fit using x/y lm_xy_fit &lt;- lm_model_spec %&gt;% fit_xy( x = ames_train %&gt;% select(Longitude, Latitude), y = ames_train %&gt;% pull(Sale_Price) ) 6.1.4 Generalized Model Arguments Like the varying interfaces, model parameters differ from implementation to implementation two level of model arguments main arguments - Parameters aligned with the mathematical vehicle engine arguments - Parameters aligned with the package implementation of the mathematical algorithm argument ranger randomForest sparklyr sampled predictors mtry mtry feature_subset_strategy trees num.tress ntree num_trees data points to split min.node.size nodesize min_instances_per_node argument parsnip sampled predictors mtry trees trees data points to split min_n Parsnip in Action The translate() provides the mapping from the parsnips interface to the each individual package’s implementation of the algorithm. # stats implementation linear_reg() %&gt;% set_engine(&quot;lm&quot;) %&gt;% translate() ## Linear Regression Model Specification (regression) ## ## Computational engine: lm ## ## Model fit template: ## stats::lm(formula = missing_arg(), data = missing_arg(), weights = missing_arg()) # glmnet implementation linear_reg(penalty = 1) %&gt;% set_engine(&quot;glmnet&quot;) %&gt;% translate() ## Linear Regression Model Specification (regression) ## ## Main Arguments: ## penalty = 1 ## ## Computational engine: glmnet ## ## Model fit template: ## glmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), ## family = &quot;gaussian&quot;) "],["use-model-results.html", "6.2 Use Model results", " 6.2 Use Model results Now that we have a fitted model we will need to pull some summary information from it we will use two extremely fun functions from the {broom} package to help us out (tidy() &amp; glance()). tidy() - Has a bunch of versatility, but for our context it can take our model object and return our model coefficients into a nice tibble. broom::tidy(lm_form_fit) %&gt;% knitr::kable() term estimate std.error statistic p.value (Intercept) -300.250929 14.5815154 -20.59120 0 Longitude -2.013412 0.1296788 -15.52615 0 Latitude 2.781713 0.1816642 15.31239 0 glance() - allows us in this context to convert our model’s summary statistics into a tibble broom::glance(lm_form_fit) %&gt;% knitr::kable() r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs 0.1639281 0.1632132 0.1613844 229.3031 0 2 950.0754 -1892.151 -1869.116 60.91909 2339 2342 "],["make-predictions.html", "6.3 Make Predictions", " 6.3 Make Predictions Rules to Live by: Returns a tibble Column names are … erh Predictable Return the same number of rows as are in the data set some predict functions omit observations with NA values. Which is great if that’s what you intend, but if you aren’t expecting that behavior you would have to find out the hard way. # create example test set ames_test_small &lt;- ames_test %&gt;% slice(1:5) # predict on test set predict(lm_form_fit, new_data = ames_test_small) %&gt;% knitr::kable() .pred 5.223697 5.221967 5.284242 5.239113 5.314692 Combining bind_cols with our predict function we can merge our predictions back to the test set. # add predictions together with actuals ames_test_small %&gt;% select(Sale_Price) %&gt;% bind_cols(predict(lm_form_fit, ames_test_small)) %&gt;% # Add 95% prediction intervals to the results: bind_cols(predict(lm_form_fit, ames_test_small, type = &quot;pred_int&quot;)) %&gt;% knitr::kable() Sale_Price .pred .pred_lower .pred_upper 5.021189 5.223697 4.907039 5.540355 5.235528 5.221967 4.905310 5.538625 5.278525 5.284242 4.967563 5.600922 5.060698 5.239113 4.922441 5.555785 5.596808 5.314692 4.997995 5.631388 "],["tidymodels-adjacent-packages.html", "6.4 {tidymodels}-Adjacent Packages", " 6.4 {tidymodels}-Adjacent Packages Opinions can be shared, other modeling packages can use the same opinion to replicate a workflow. The {discrim}2 package adds a new set of mathematical models to our arsenal of tools. discrim_flexible() %&gt;% - Mathematical Model or if we are using my terrible analogy the car body set_engine(\"earth\") - The package we want to approximate our discriminat analysis # devtools::install_github(&quot;tidymodels/discrim&quot;) # to install # load package library(discrim) # create dummy data parabolic_grid &lt;- expand.grid(X1 = seq(-5, 5, length = 100), X2 = seq(-5, 5, length = 100)) # fit model from discrim fda_mod &lt;- discrim_flexible(num_terms = 3) %&gt;% set_engine(&quot;earth&quot;) %&gt;% fit(class ~ ., data = parabolic) # assigning predictions to data frame parabolic_grid$fda &lt;- predict(fda_mod, parabolic_grid, type = &quot;prob&quot;)$.pred_Class1 # plotting prediction library(ggplot2) ggplot(parabolic, aes(x = X1, y = X2)) + geom_point(aes(col = class), alpha = .5) + geom_contour(data = parabolic_grid, aes(z = fda), col = &quot;black&quot;, breaks = .5) + theme_bw() + theme(legend.position = &quot;top&quot;) + coord_equal() Discrim Package Link↩︎ "],["summary-1.html", "6.5 Summary", " 6.5 Summary Create a Common Interface - All models are comprised of some core components mathematical model engine implementation mode if needed Arguments Main - algorithm specific (trees, mtry, penalty) Engine - Package/Engine specific (e.g. verbose, num.threads, …) Predictable Behavior tibble in, tibble out same number of observations returned for predict() "],["meeting-videos-5.html", "6.6 Meeting Videos", " 6.6 Meeting Videos 6.6.1 Cohort 1 Meeting chat log 00:15:40 Tan Ho: YESSS 00:15:58 Tan Ho: (@ the drake meme) 00:21:51 Tyler Grant Smith: space advantage 00:23:32 Asmae : what&#39;s that 00:24:28 Jim Gruman: http://www.feat.engineering/categorical-trees.html on dummies or no-dummies with trees 00:24:37 Tony ElHabr: amazing meme 00:24:48 Tony ElHabr: I am physically applauding 00:25:03 Scott Nestler: Some additional discussion regarding fit_xy() at tidyverse.org/blog/2019/04/parsnip-internals, related to possible range of mtry variables when you don&#39;t know number of predictors before recipe is prepped. 00:25:56 Scott Nestler: Is there a typo in the book after tables toward end of 7.1 where it talks about common argument names? It mentions num_n but I think they meant min_n. What do others think? Or is num_n actually used? 00:26:30 Conor Tompkins: Scott I think Jon made a PR to fix that typo 00:26:53 Scott Nestler: Thx. I hadn&#39;t checked yet. Just caught it in a quick read right before we started. 00:31:15 Scott Nestler: There are actually 30 different model types and engines at https://www.tidymodels.org/find/parsnip/ that work with parsnip. 00:43:55 Tony ElHabr: yay volunteers 00:44:48 Andy Farina: Thanks Jordan, excellent presentation 6.6.2 Cohort 2 Meeting chat log 00:12:53 shamsuddeen: https://torch.mlverse.org 00:13:21 shamsuddeen: Book: https://mlverse.github.io/torchbook_materials/ 00:16:18 Janita Botha: I&#39;m in the meeting twice because my sound is not working this morning 00:26:10 August: fit_xy doesn&#39;t do the contrast coding automatically, fit does. 00:28:04 Luke Shaw: Is fit_xy a &#39;safer&#39; option? 00:28:33 rahul bahadur: @August. If you were to have categorical predictors in `lm()` would fit_xy() be able to model it? `lm()` automatically does contrast coding internally 00:30:55 Luke Shaw: https://xkcd.com/927/ 00:33:10 Amélie Gourdon-Kanhukamwe (she/they): Nice one Luke! 00:34:44 rahul bahadur: I hope tidymodels doesn&#39;t end up becoming just another standard :P 00:34:50 August: I&#39;m not sure about safer, it depends I suppose if you have done a recipe with the coding before hand. 00:35:39 Kevin Kent: I wonder if future packages that tidy models references will be built with a tidy models integration in mind. Instead of tidy models having to do so much translation 00:36:34 Luke Shaw: Thanks August. Think I need to get my hands dirty using it to really help me understand! 00:37:34 Luke Shaw: Yeah I&#39;m only joking with the xkcd but does feel relevant. Kinda hoping tidymodels can be the one standard I get v. familiar with and has a long life 00:40:30 Kevin Kent: I think modeltime is an example of a modeling package done with tidymodels in mind. Kind of designed first for tidy models instead of having to do a bunch of translation to fit into the framework. Hopefully more do development like this 00:41:21 Kevin Kent: But I guess that one is doing a similar job to tidymodels in the sense of pulling in functionality from other packages as engines 00:41:45 August: Absolutely. 00:42:44 August: btw I have mostly used fit() rather than fit_xy() although the latter by be preferable 00:42:54 Kevin Kent: I love these templating packages..excited to use them 00:43:18 Kevin Kent: rstudio add in looks fantastic 00:44:23 rahul bahadur: Yes, fit_xy(), though requires more code, would not lead to unknown behaviour. I believe. 00:44:41 Kevin Kent: Yeah less “magic” happening 00:46:19 Luke Shaw: This feels very scary about using &quot;fit&quot;: &quot;If the data were preprocessed in any way, incorrect predictions will be generated (sometimes, without errors).&quot; 00:47:56 Luke Shaw: ^^^ that&#39;s not about the function fit() - my bad 00:48:09 Kevin Kent: Or if you accidentally read in a numeric column as categorical and then run fit it won’t throw an error even though its not meaningful as a categorical variable 6.6.3 Cohort 3 Meeting chat log 00:15:22 Ildiko Czeller: &gt; linear_reg() Linear Regression Model Specification (regression) &gt; linear_reg() %&gt;% set_mode(&quot;classification&quot;) Linear Regression Model Specification (classification) 00:17:00 Edgar Zamora: https://www.tidymodels.org/find/parsnip/ 00:22:51 Toryn Schafer (she/her): model.matrix 00:26:52 Ildiko Czeller: Hi Daniel! good to see you 00:27:14 Daniel Chen: hello hello! 00:29:02 Daniel Chen: I forgot there was an article/help page of all the supported engines 00:30:04 Daniel Chen: ooh they moved the things around: this is the page to help you find arguments: https://www.tidymodels.org/find/parsnip/ 00:30:40 Daniel Chen: that page has the model/engine parsnip param name and original param name 00:31:03 Ildiko Czeller: thanks! 00:34:36 Daniel Chen: you just need to be careful with the column names after `tidy` since they don&#39;t always mean the same thing for each model output. but it makes the column names consistent so you can combine tables 00:45:18 jiwan: Error in parsnip::rand_forest(verbose = TRUE) : unused argument (verbose = TRUE) 00:45:48 Daniel Chen: unused arguments usually get ignored. maybe you get a warning? 00:54:29 Daniel Chen: bye all! 6.6.4 Cohort 4 Meeting chat log 00:56:18 Laura Rose: https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/linear-model-selection-and-regularization.html#forward-and-backward-stepwise-selection 00:58:07 Ryan Metcalf: https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/index.html 00:58:52 Ryan Metcalf: https://avehtari.github.io/ROS-Examples/ 01:01:31 Federica Gazzelloni: https://arxiv.org/pdf/1502.06988.pdf "],["ai-ethics.html", "AI Ethics", " AI Ethics Cohort 2 had a discussion of AI Ethics at this point in the club. "],["meeting-videos-6.html", "6.7 Meeting Videos", " 6.7 Meeting Videos 6.7.1 Cohort 2 Meeting chat log 00:04:27 Luke Shaw: Hey all :) My internet is a bit unstable so will be off-video 00:17:45 Amélie Gourdon-Kanhukamwe: If you have not seen it, Coded Bias is now available to stream, at least in the US, possibly in the UK too: https://www.codedbias.com/ (feaures Joy Buolamwini and many others, Klein too - Data Feminism). 00:18:45 Stephen Holsenbeck: 🌟🙏 awesome! thank you for the recommendation! 00:20:34 Amélie Gourdon-Kanhukamwe: That paper for example found exactly what Kevin is describing, classifying on gender although it was not in the data, because of underlying relationships: https://arxiv.org/abs/2004.07173 00:21:10 rahul bahadur: To add to @Kevin&#39;s point Apple Card&#39;s algo had the problem of providing lower credit limits to women. In their defense they didn&#39;t include &#39;sex&#39; in their model. But, the effects were latent in other variables. 00:21:40 Amélie Gourdon-Kanhukamwe: 👆🏻 00:23:24 Layla Bouzoubaa: 🤘🏼 00:32:27 Amélie Gourdon-Kanhukamwe: Thanks Shamsuddeen, Timnit Gebru is someone I enjoy the critical and ethical perspective of, so that looks great! 00:33:37 Kevin Kent: Should Layla or Luke share next? 00:33:53 Layla Bouzoubaa: Oh sorry, I just went ahead 00:33:59 Layla Bouzoubaa: Luke did I just cut you lol 00:34:02 Kevin Kent: Haha oh that’s what I thought would happen 00:34:09 Amélie Gourdon-Kanhukamwe: Left is a euphemism Shamsuddeen :-) 00:34:29 Layla Bouzoubaa: :) 00:34:30 Kevin Kent: No worries, cool. Thanks! Just didn’t want to take up the whole time 00:34:30 Luke Shaw: You didn&#39;t cut me out :) 00:35:03 Kevin Kent: brb 00:36:28 Kevin Kent: back 00:39:29 shamsuddeen: can we synthetic data to closed any representation bias? 00:39:57 Amélie Gourdon-Kanhukamwe: No worries 00:40:09 Amélie Gourdon-Kanhukamwe: * Ignore above 00:40:35 Kevin Kent: I thought this was a such a cool histogram 00:40:54 Kevin Kent: Images in the binned bars 00:49:09 Kevin Kent: I loved this simulation - one thing I was thinking of too is that there are long-term costs to biased algorithms and aren’t just about immediate costs and revenue. Like if the company is perceived as being discriminatory, people might boycott the products. 00:49:36 Luke Shaw: yeah this is cool 00:50:57 Amélie Gourdon-Kanhukamwe (she/they): I was reading on decolonizing STEM education today, and incidentally found this which is relevant for today&#39;s discussion, although I only managed to skim it: https://www.nature.com/articles/d41586-020-00274-3 It does resonate with some of the points in Layla&#39;s notes (eg, counterfactuals) 00:51:43 Kevin Kent: Thanks for sharing! 00:52:03 Amélie Gourdon-Kanhukamwe (she/they): I llike that it has specific steps. 00:52:11 Kevin Kent: I’ve also been meaning to read the for the longest time https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418815 00:52:56 Amélie Gourdon-Kanhukamwe (she/they): And &quot;Algorithms of oppression&quot; too (also need to find time for it 00:54:14 Amélie Gourdon-Kanhukamwe (she/they): https://nyupress.org/9781479837243/algorithms-of-oppression/ 00:54:25 Kevin Kent: 👍 00:59:14 Kevin Kent: I think a lot of these models are trained using Wikipedia as the corpus so you can definitely see how a dataset that is created in a crowdsourced method can have bias in it 00:59:29 Luke Shaw: I have to go on the hour (a few mins) - really fascinating conversations! We don&#39;t need to go the stuff I mentioned - didn&#39;t have anything to present just were there for a discussion. 01:01:43 Kevin Kent: Sorry about that Luke, I kind of lost track of time in my part. But Im definitely going to read your reocmendatio 01:02:18 Luke Shaw: Thanks for the discussion all, see you next week. Can any recommendations in the slack or google sheets please :D 01:02:55 Stephen Holsenbeck: Will do, did you have stuff you wanted to present on Data Feminism? 01:04:52 shamsuddeen: Layla, you working on ethics ? 01:04:58 Janita Botha: This was awesome! 01:05:06 Amélie Gourdon-Kanhukamwe (she/they): Just heard of this in relation to white methods: https://www.zedbooks.net/shop/book/decolonizing-methodologies/ 01:05:52 Amélie Gourdon-Kanhukamwe (she/they): Uni of Bristol has a data ethics journal club too, open outside. Will fetch the link. 01:07:29 Kevin Kent: Depthless is a true auto-antonym 01:07:36 Amélie Gourdon-Kanhukamwe (she/they): http://www.bristol.ac.uk/golding/events/2021/data-ethics-club---february-3.html 01:07:55 Stephen Holsenbeck: same 01:08:07 Amélie Gourdon-Kanhukamwe (she/they): Same same 01:08:19 Stephen Holsenbeck: share in the slack 🙂 01:08:36 Kevin Kent: Please :) 01:10:40 Amélie Gourdon-Kanhukamwe (she/they): Sorry for the plug, but if you have Twitter, I have a critical data science list: https://twitter.com/i/lists/1321784589464068097 01:12:37 Amélie Gourdon-Kanhukamwe (she/they): Angela Saini&#39;s Superior is a good general one on how science can be used in racist ways, for awareness (including the IBM nazi past). 01:12:59 Janita Botha: Thank you! 01:16:22 rahul bahadur: Thanks Everyone 01:16:28 Janita Botha: bye! "],["a-model-workflow.html", "Chapter 7 A model workflow", " Chapter 7 A model workflow Learning objectives: Explain why a model workflow includes preprocessing, fitting, and post-processing. Describe parts of the modeling process that occur before the model is fit. Describe parts of the modeling process that occur after the model is fit. Use the {workflows} package to create a simple workflow. Add a model to a workflow. Add a formula to a workflow. Fit a workflow. Use a workflow to predict new data. Update a workflow. Use {recipes} with {workflows}. Add a recipe to a workflow. Use workflows::pull_*() to extract objects from fitted workflows. Describe how a workflow that uses a formula decides how to pre-process data. Describe how workflows using tree-based models pre-process factor predictors. Add a special formula to a workflow with the formula argument to workflows::add_model(). Describe workflow steps that are not yet included in {tidymodels}. "],["workflows.html", "7.1 Workflows", " 7.1 Workflows A single object to wrap the pre-processing and model fitting. Modeling steps without workflows Link to drawing Workflows help you manage fewer objects and call fewer functions to achieve your goal. "],["demonstration.html", "7.2 Demonstration", " 7.2 Demonstration library(tidyverse) library(tidymodels) tidymodels_prefer() set.seed(123) A previous Tidy Tuesday dataset is used for demonstration. References: Tidy Tuesday description Data source sf_trees &lt;- readr::read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-28/sf_trees.csv&#39;) kableExtra::kable(head(sf_trees, 10)) %&gt;% kableExtra::scroll_box(width = &#39;100%&#39;) tree_id legal_status species address site_order site_info caretaker date dbh plot_size latitude longitude 53719 Permitted Site Tree(s) :: 2963 Webster St 1 Sidewalk: Curb side : Cutout Private 1955-09-19 NA NA 37.79787 -122.4341 30313 Permitted Site Tree(s) :: 501 Arkansas St 3 Sidewalk: Curb side : Cutout Private 1955-10-20 NA NA 37.75984 -122.3981 30312 Permitted Site Tree(s) :: 501 Arkansas St 2 Sidewalk: Curb side : Cutout Private 1955-10-20 NA NA 37.75984 -122.3981 30314 DPW Maintained Pittosporum undulatum :: Victorian Box 501 Arkansas St 1 Sidewalk: Curb side : Cutout Private 1955-10-20 16 NA 37.75977 -122.3981 30315 Permitted Site Acacia melanoxylon :: Blackwood Acacia 1190 Sacramento St 5 Sidewalk: Curb side : Cutout Private 1955-10-24 NA NA 37.79265 -122.4124 30316 Permitted Site Acacia melanoxylon :: Blackwood Acacia 1190 Sacramento St 6 Sidewalk: Curb side : Cutout Private 1955-10-24 NA NA 37.79265 -122.4124 48435 Permitted Site Tree(s) :: 1190 Sacramento St 4 Sidewalk: Curb side : Cutout Private 1955-10-24 NA NA 37.79265 -122.4124 30319 Permitted Site Magnolia grandiflora :: Southern Magnolia 867 25th Ave 2 Sidewalk: Curb side : Cutout Private 1955-12-13 NA NA 37.77319 -122.4843 30318 Permitted Site Magnolia grandiflora :: Southern Magnolia 867 25th Ave 1 Sidewalk: Curb side : Cutout Private 1955-12-13 NA NA 37.77319 -122.4843 30320 Permitted Site Corymbia ficifolia :: Red Flowering Gum 867 25th Ave 3 Sidewalk: Curb side : Cutout Private 1955-12-13 NA NA 37.77319 -122.4843 The goal will be to predict dbh which means diameter at breast height. 7.2.1 Some data exploration and cleaning kableExtra::kable(skimr::skim(sf_trees)) %&gt;% kableExtra::scroll_box(width = &#39;100%&#39;) skim_type skim_variable n_missing complete_rate Date.min Date.max Date.median Date.n_unique character.min character.max character.empty character.n_unique character.whitespace numeric.mean numeric.sd numeric.p0 numeric.p25 numeric.p50 numeric.p75 numeric.p100 numeric.hist Date date 124610 0.3543088 1955-09-19 2020-01-25 2001-06-12 7404 NA NA NA NA NA NA NA NA NA NA NA NA NA character legal_status 54 0.9997202 NA NA NA NA 7 28 0 9 0 NA NA NA NA NA NA NA NA character species 0 1.0000000 NA NA NA NA 2 81 0 571 0 NA NA NA NA NA NA NA NA character address 1487 0.9922948 NA NA NA NA 1 40 0 85909 0 NA NA NA NA NA NA NA NA character site_info 0 1.0000000 NA NA NA NA 1 33 0 31 0 NA NA NA NA NA NA NA NA character caretaker 0 1.0000000 NA NA NA NA 3 23 0 22 0 NA NA NA NA NA NA NA NA character plot_size 50013 0.7408478 NA NA NA NA 1 23 0 524 0 NA NA NA NA NA NA NA NA numeric tree_id 0 1.0000000 NA NA NA NA NA NA NA NA NA 126529.214071 7.931704e+04 1.0000 52601.50000 120862.00000 202607.50000 261546.00000 ▇▆▆▆▇ numeric site_order 1634 0.9915331 NA NA NA NA NA NA NA NA NA 4.579118 1.251574e+01 -50.0000 1.00000 2.00000 4.00000 501.00000 ▇▁▁▁▁ numeric dbh 41819 0.7833066 NA NA NA NA NA NA NA NA NA 9.953767 2.936408e+01 0.0000 3.00000 7.00000 12.00000 9999.00000 ▇▁▁▁▁ numeric latitude 2832 0.9853254 NA NA NA NA NA NA NA NA NA 37.766260 2.497521e-01 37.5090 37.74032 37.76024 37.77964 47.27022 ▇▁▁▁▁ numeric longitude 2832 0.9853254 NA NA NA NA NA NA NA NA NA -122.445586 4.152907e-01 -138.2839 -122.45430 -122.43140 -122.41295 -122.36662 ▁▁▁▁▇ # DataExplorer::create_report(sf_trees) trees_cleaned &lt;- sf_trees %&gt;% rename(diam = dbh, date_planted = date) %&gt;% filter(!is.na(diam)) %&gt;% filter(!is.na(legal_status)) %&gt;% filter(latitude &lt;= 40 &amp; longitude &gt;= -125) %&gt;% filter(diam &lt;= 100 &amp; diam &gt; 0) %&gt;% filter(site_order &gt;= 0) %&gt;% select(-plot_size) kableExtra::kable(skimr::skim(trees_cleaned)) %&gt;% kableExtra::scroll_box(width = &#39;100%&#39;) skim_type skim_variable n_missing complete_rate Date.min Date.max Date.median Date.n_unique character.min character.max character.empty character.n_unique character.whitespace numeric.mean numeric.sd numeric.p0 numeric.p25 numeric.p50 numeric.p75 numeric.p100 numeric.hist Date date_planted 110867 0.2497378 1955-10-20 2020-01-25 2005-08-03 5990 NA NA NA NA NA NA NA NA NA NA NA NA NA character legal_status 0 1.0000000 NA NA NA NA 7 28 0 9 0 NA NA NA NA NA NA NA NA character species 0 1.0000000 NA NA NA NA 2 81 0 520 0 NA NA NA NA NA NA NA NA character address 3 0.9999797 NA NA NA NA 1 35 0 71086 0 NA NA NA NA NA NA NA NA character site_info 0 1.0000000 NA NA NA NA 1 33 0 26 0 NA NA NA NA NA NA NA NA character caretaker 0 1.0000000 NA NA NA NA 3 23 0 22 0 NA NA NA NA NA NA NA NA numeric tree_id 0 1.0000000 NA NA NA NA NA NA NA NA NA 138935.593229 8.043974e+04 1.0000 66037.5000 139248.00000 211074.50000 261546.00000 ▇▃▆▆▇ numeric site_order 0 1.0000000 NA NA NA NA NA NA NA NA NA 4.292466 8.770121e+00 0.0000 1.0000 2.00000 4.00000 168.00000 ▇▁▁▁▁ numeric diam 0 1.0000000 NA NA NA NA NA NA NA NA NA 9.946640 9.949371e+00 1.0000 3.0000 7.00000 12.00000 100.00000 ▇▁▁▁▁ numeric latitude 0 1.0000000 NA NA NA NA NA NA NA NA NA 37.760650 2.461270e-02 37.5090 37.7404 37.76128 37.78089 37.80902 ▁▁▁▅▇ numeric longitude 0 1.0000000 NA NA NA NA NA NA NA NA NA -122.434344 3.030750e-02 -122.5113 -122.4538 -122.43113 -122.41216 -122.36662 ▂▃▇▇▂ ggplot(trees_cleaned, aes(x = diam)) + geom_histogram() + scale_x_log10() ggplot(trees_cleaned, aes(x = date_planted, y = diam)) + geom_bin2d() + geom_smooth() "],["modeling-with-workflows.html", "7.3 Modeling with workflows", " 7.3 Modeling with workflows trees_split &lt;- initial_split(trees_cleaned %&gt;% mutate(diam = log10(diam)), prop = 0.8) trees_training &lt;- training(trees_split) trees_testing &lt;- testing(trees_split) trees_recipe &lt;- recipe(trees_training, diam ~ .) %&gt;% update_role(tree_id, address, new_role = &quot;id&quot;) %&gt;% step_indicate_na(date_planted) %&gt;% # really dummy imputation step_mutate(date_planted = if_else(!is.na(date_planted), date_planted, as.Date(&#39;1950-01-01&#39;))) %&gt;% step_other(all_nominal_predictors(), threshold = 0.01) %&gt;% step_dummy(all_nominal_predictors()) linear_model_spec &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) tree_workflow_lm &lt;- workflow() %&gt;% add_model(linear_model_spec) %&gt;% add_recipe(trees_recipe) fitted_workflow_lm &lt;- tree_workflow_lm %&gt;% fit(trees_training) tidy(extract_recipe(fitted_workflow_lm), 3) ## # A tibble: 35 × 3 ## terms retained id ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 legal_status DPW Maintained othe… ## 2 legal_status Permitted Site othe… ## 3 legal_status Undocumented othe… ## 4 species Acacia melanoxylon :: Blackwood Acacia othe… ## 5 species Arbutus &#39;Marina&#39; :: Hybrid Strawberry Tree othe… ## 6 species Callistemon citrinus :: Lemon Bottlebrush othe… ## 7 species Corymbia ficifolia :: Red Flowering Gum othe… ## 8 species Cupressus macrocarpa :: Monterey Cypress othe… ## 9 species Eriobotrya deflexa :: Bronze Loquat othe… ## 10 species Ficus microcarpa nitida &#39;Green Gem&#39; :: Indian Laurel Fig … othe… ## # ℹ 25 more rows tidy(extract_fit_parsnip(fitted_workflow_lm)) ## # A tibble: 41 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 6.35e+1 4.22e+0 15.1 3.57e- 51 ## 2 site_order -3.08e-3 1.15e-4 -26.9 1.66e-158 ## 3 date_planted -4.19e-5 4.83e-7 -86.8 0 ## 4 latitude 5.56e-1 4.03e-2 13.8 2.75e- 43 ## 5 longitude 6.78e-1 3.28e-2 20.7 6.99e- 95 ## 6 na_ind_date_planted -6.22e-1 9.62e-3 -64.7 0 ## 7 legal_status_Permitted.Site 1.07e-1 4.51e-3 23.8 4.47e-125 ## 8 legal_status_Undocumented 7.55e-2 6.51e-3 11.6 4.43e- 31 ## 9 legal_status_other 6.61e-2 9.22e-3 7.16 7.88e- 13 ## 10 species_Arbutus..Marina.....Hybrid.St… -3.67e-1 8.20e-3 -44.8 0 ## # ℹ 31 more rows trees_testing$pred_lm &lt;- predict(fitted_workflow_lm, trees_testing)$.pred rmse(trees_testing, diam, pred_lm) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 0.316 7.3.1 Different model, same recipe rand_forest_spec &lt;- rand_forest( mode = &#39;regression&#39;, mtry = 3, trees = 50, min_n = 10 ) %&gt;% set_engine(&#39;ranger&#39;) tree_workflow_rf &lt;- tree_workflow_lm %&gt;% update_model(rand_forest_spec) fitted_workflow_rf &lt;- tree_workflow_rf %&gt;% fit(trees_training) trees_testing$pred_rf &lt;- predict(fitted_workflow_rf, trees_testing)$.pred rmse(trees_testing, diam, pred_lm) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 0.316 rmse(trees_testing, diam, pred_rf) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 0.307 7.3.2 Same model, different preprocessing formula_predictions &lt;- tree_workflow_lm %&gt;% remove_recipe() %&gt;% add_formula(diam ~ is.na(date_planted) + longitude) %&gt;% fit(trees_training) %&gt;% predict(trees_testing) rmse_vec(trees_testing$diam, formula_predictions$.pred) ## [1] 0.3574375 "],["managing-many-workflows.html", "7.4 Managing many workflows", " 7.4 Managing many workflows rand_forest_spec &lt;- rand_forest( mode = &#39;regression&#39;, mtry = 2, trees = 25, min_n = 10 ) %&gt;% set_engine(&#39;ranger&#39;) tree_workflows &lt;- workflow_set( preproc = list( &quot;variables&quot; = workflow_variables(diam, c(longitude, latitude, site_order)), &quot;simple_formula&quot; = diam ~ is.na(date_planted) + longitude + latitude, &quot;trees_recipe&quot; = trees_recipe ), models = list( &quot;lm&quot; = linear_model_spec, &quot;rf&quot; = rand_forest_spec ) ) tree_workflows ## # A workflow set/tibble: 6 × 4 ## wflow_id info option result ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 variables_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 2 variables_rf &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 3 simple_formula_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 4 simple_formula_rf &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 5 trees_recipe_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 6 trees_recipe_rf &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; tree_predictions &lt;- tree_workflows %&gt;% rowwise() %&gt;% mutate(fitted_wf = list(fit(info$workflow[[1]], trees_training))) %&gt;% mutate(pred = list(predict(fitted_wf, trees_testing))) tree_predictions %&gt;% mutate(rmse = rmse_vec(trees_testing$diam, pred$.pred)) ## # A tibble: 6 × 7 ## # Rowwise: ## wflow_id info option result fitted_wf pred rmse ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;dbl&gt; ## 1 variables_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list&gt; &lt;workflow&gt; &lt;tibble&gt; 0.367 ## 2 variables_rf &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list&gt; &lt;workflow&gt; &lt;tibble&gt; 0.310 ## 3 simple_formula_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list&gt; &lt;workflow&gt; &lt;tibble&gt; 0.356 ## 4 simple_formula_rf &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list&gt; &lt;workflow&gt; &lt;tibble&gt; 0.337 ## 5 trees_recipe_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list&gt; &lt;workflow&gt; &lt;tibble&gt; 0.316 ## 6 trees_recipe_rf &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list&gt; &lt;workflow&gt; &lt;tibble&gt; 0.319 "],["notes.html", "7.5 Notes", " 7.5 Notes how the formula is used will depend on the model specification If a modeling package uses the formula not only for pre-processing or has a syntax not supported by model.matrix you can specify a formula in add_model Later the {workflows} package will contain tools to help with post processing, such as creating hard predictions from class probabilities. "],["meeting-videos-7.html", "7.6 Meeting Videos", " 7.6 Meeting Videos 7.6.1 Cohort 1 Meeting chat log 00:13:53 Tyler Grant Smith: i used skimr today on a dataset with 77 million rows and 200 columns...it took a while 00:14:14 Tan Ho: The official R soundtrack https://www.youtube.com/watch?v=-9BzWBufH1s 00:14:19 Tyler Grant Smith: that would have been smart...oh well 00:14:35 Ben Gramza: https://www.kaggle.com/yamaerenay/spotify-dataset-19212020-160k-tracks 00:16:21 Tony ElHabr: i&#39;m blind 00:16:31 Asmae Toumi: pAIN 00:16:42 Jordan Krogmann: the humanity 00:16:54 Scott Nestler: I just turned off my Vitamin D sunlamp. 00:17:01 Jordan Krogmann: coobalt 00:17:03 Jordan Krogmann: = love 00:17:14 Tony ElHabr: bad programmers use light mode so they can see their bugs 00:17:15 Tyler Grant Smith: thanks 00:17:16 Jim Gruman: monokai 00:17:31 Tan Ho: correct pane layout tho 00:17:34 Tan Ho: much appreciate 00:17:37 Asmae Toumi: Absolutely not 00:17:48 Jim Gruman: console, upper right... 00:21:20 Jon Harmon (jonthegeek): Is there actually no chat, or does zoom just not show it to me when I&#39;m late? 00:21:28 Tony ElHabr: it doesn&#39;t show 00:21:29 Tan Ho: you don&#39;t see it if you&#39;re late 00:21:40 Jordan Krogmann: yeah there were some comments up top 00:21:53 Jon Harmon (jonthegeek): Ok. That&#39;s funny, since I can see the full log after the meeting. 00:26:25 Tony ElHabr: steps like step_meanimpute will only do work on the training data, so you avoid data leakage 00:26:31 Jordan Krogmann: +1 00:29:57 Asmae Toumi: I do it with insurance costs a lot because I don’t want to throw out the information in claims with 0$ 00:30:58 Asmae Toumi: My internet is bad but I do use an offset 00:31:30 Jim Gruman: step_YeoJohnson and step_BoxCox would be better choices 00:32:11 Tyler Grant Smith: ^ 00:32:37 Tony ElHabr: are they always better tho? 00:35:38 Tyler Grant Smith: well yeo johnson is a generalization og log1p 00:36:22 Tony ElHabr: ah right. google verifies this is true 00:36:39 Pavitra Chakravarty: thanks Jon 00:36:41 Tyler Grant Smith: sure? 00:36:46 Tyler Grant Smith: no idea me neither 00:42:31 Jordan Krogmann: tidy is black magic 00:43:40 Tony ElHabr: how do we feel about super long function names like `pull_workflow_prepped_recipe()` 00:44:39 Jordan Krogmann: %&lt;&gt;% update_formula() would overwrite it 00:45:28 Conor Tompkins: Long specific functions are better than “bake” and “juice” IMO 00:45:58 Tony ElHabr: yeah i agree 00:46:15 Scott Nestler: Back to the log(0) issue. Transformations like log(x+c) where c is a positive constant &quot;start value&quot; can work--and can be indicated even when no value of x is zero--but sometimes they destroy linear relationships. 00:46:26 Scott Nestler: Here&#39;s the other method I recall seeing (in Hosmer, Lemeshow, &amp; Sturdivant&#39;s Logistic Regression book): A good solution is to create two variables. One of them equals log(x) when x is nonzero and otherwise is anything; it&#39;s convenient to let it default to zero. The other, let&#39;s call it zx, is an indicator of whether x is zero: it equals 1 when x=0 and is 0 otherwise. 00:47:31 Scott Nestler: These terms contribute a sum βlog(x)+β0zx to the estimate. When x&gt;0, zx=0 so the second term drops out leaving just βlog(x). When x=0, &quot;log(x)&quot; has been set to zero while zx=1, leaving just the value β0. Thus, β0 estimates the effect when x=0 and otherwise β is the coefficient of log(x). 00:47:57 Scott Nestler: Found a reference to it here: https://stats.stackexchange.com/questions/4831/regression-transforming-variables 00:54:16 Asmae Toumi: Resampling is my chapter *cracks knuckles* 00:54:28 Asmae Toumi: nooooope 00:54:39 Scott Nestler: rf_fit_rs &lt;- rf_wf %&gt;% fit_resamples(folds) 00:55:26 Conor Tompkins: Right Scott, that will contain the results of the fit 00:55:49 Conor Tompkins: If you keep the .pred, that is 00:56:20 Jordan Krogmann: Asmae, time to voluntell someone! 00:56:32 Asmae Toumi: Let me play the music of my people 00:56:37 Asmae Toumi: I nominateeeeeeeeeeeeeeee 00:56:40 Tan Ho: JOE 00:56:41 Asmae Toumi: JOE 00:56:45 Asmae Toumi: WE DID IT JOE 00:56:52 Jordan Krogmann: lol 00:57:10 Asmae Toumi: https://www.youtube.com/watch?v=dP6_pYYWAT8 00:57:11 Asmae Toumi: This is the song 00:57:50 Asmae Toumi: asorry 00:57:51 Asmae Toumi: https://www.youtube.com/watch?v=-9BzWBufH1s 00:57:52 Jon Harmon (jonthegeek): https://www.youtube.com/watch?v=-9BzWBufH1s 00:57:53 Asmae Toumi: THIS IS IT 00:58:52 Jordan Krogmann: Thanks! 00:59:04 Asmae Toumi: Goodnight gang 7.6.2 Cohort 2 Meeting chat log 00:14:55 Janita Botha: THIS IS SOOOOO COOL!!!! 00:33:32 shamsuddeen: This is good Kevin. Tidy Modelling should have something like this incorporated in a workflow. 00:33:33 shamsuddeen: Tidy target 00:48:17 Roberto Villegas-Diaz: https://snakemake.readthedocs.io/en/stable/ 00:52:40 shamsuddeen: Thanks Kevin 7.6.3 Cohort 3 Meeting chat log 00:06:03 Daniel Chen (he/him): hello. i&#39;m here. just finishing up stuff before meeting 00:06:44 Ildiko Czeller: welcome Federica. is this your first tidymodels bookclub? 00:15:21 Federica Gazzelloni: @Ildiko hello yes, just jumped in. Thanks! 00:16:05 Toryn Schafer (she/her): Welcome! 00:16:40 Daniel Chen (he/him): Q: the bake() gives you the data after all those step_() functions from recipies? 00:17:39 Toryn Schafer (she/her): Yes, I think that’s right, Daniel. From the help page the value of bake() is: “A tibble, matrix, or sparse matrix that may have different columns than the original columns in new_data.” 00:22:45 Daniel Chen (he/him): what is the response/y variable for this dataset? 00:28:36 Daniel Chen (he/him): oh I see it&#39;s `diam` 00:41:56 Federica Gazzelloni: @Toryn thanks very interesting 00:45:24 Federica Gazzelloni: Hello @Daniel 00:45:37 Daniel Chen (he/him): so working with workflow_sets still needs you to drop into rowwise? there&#39;s no function to fit those models for you? 00:45:49 Daniel Chen (he/him): hello @Federica! 00:47:27 Toryn Schafer (she/her): In the book they used map instead of rowwise, but yeah doesn’t seem to be one function: map(info, ~ fit(.x$workflow[[1]], ames_train)) 00:54:39 Federica Gazzelloni: need to go, very good job! see you next time hopefully 01:07:43 Ildiko Czeller: https://www.tidymodels.org/learn/develop/ 01:08:24 Ildiko Czeller: https://hardhat.tidymodels.org/ 01:09:07 Ildiko Czeller: https://tidymodels.github.io/model-implementation-principles/ 7.6.4 Cohort 4 "],["feature-engineering-with-recipes.html", "Chapter 8 Feature engineering with recipes", " Chapter 8 Feature engineering with recipes Learning objectives: Define feature engineering. List reasons that feature engineering might be beneficial. Use the {recipes} package to create a simple feature engineering recipe. Use selectors from the {recipes} package to apply transformations to specific types of columns. List some advantages of using a recipe for feature engineering. Describe what happens when a recipe is prepared with recipes::prep(). Use recipes::bake() to process a dataset. Recognize how to use recipes::step_unknown(), recipes::step_novel(), recipes::step_other() to prepare factor variables. Explain how recipes::step_dummy() encodes qualitative data in a numeric format. Recognize techniques for dealing with large numbers of categories, such as feature hashing or encoding using the {embed} package (as described in this talk by Alan Feder at rstudio::global(2021)). Recognize methods for encoding ordered factors. Use recipes::step_interact() to add interaction terms to a recipe. Understand why some steps might only be applicable to training data. Recognize the functions from {recipes} and {themis} that are only applied to training data by default. Recognize that {recipes} includes functions for creating spline terms, such as step_ns(). Recognize that {recipes} includes functions for feature extraction, such as step_pca(). Use themis::step_downsample() to downsample data. Recognize other row-sampling steps from the {recipes} package. Use recipes::step_mutate() and recipes::step_mutate_at() for general {dplyr}-like transformations. Recall that the {textrecipes} package exists for text-specific feature-engineering steps. Understand that the functions of the {recipes} package use training data for all preprocessing and feature engineering steps to prevent leakage. Use {recipes} to prepare data for traditional modeling functions. Use tidy() to examine a recipe and its steps. Refer to columns with roles other than \"predictor\" or \"outcome\". "],["in-summary.html", "8.1 In summary", " 8.1 In summary Preprocessing data How to use recipe() function What are the step_&lt;&gt;() functions 8.1.1 Intoduction Preprocessing data is a very important step of modeling, “to combine different feature engineering tasks”. In this chapter we will see how to preprocess our data using {tidymodles} meta package. "],["a-simple-recipe-for-the-ames-housing-data.html", "8.2 A SIMPLE RECIPE FOR THE AMES HOUSING DATA", " 8.2 A SIMPLE RECIPE FOR THE AMES HOUSING DATA A Recommended preprocessing Figure 8.1: Models preprocessing steps library(tidyverse) library(tidymodels) tidymodels_prefer() data(ames) fit &lt;- lm(Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) + Year_Built + Bldg_Type, data = ames) head(broom::tidy(fit),3) ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2320382. 101509. -22.9 1.84e-106 ## 2 NeighborhoodCollege_Creek 6361. 3602. 1.77 7.75e- 2 ## 3 NeighborhoodOld_Town 1485. 3673. 0.404 6.86e- 1 set.seed(1234) split&lt;-initial_split(ames, strata = Sale_Price) ames_train&lt;-training(split) ames_test&lt;-testing(split) simple_ames &lt;- recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type, data = ames_train) %&gt;% step_log(Gr_Liv_Area, base = 10) %&gt;% step_dummy(all_nominal_predictors()) simple_ames "],["using-recipes.html", "8.3 USING RECIPES", " 8.3 USING RECIPES Preprocessing is part of a modeling workflow lm_model &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) lm_wflow &lt;- workflow() %&gt;% add_model(lm_model) lm_wflow &lt;- lm_wflow %&gt;% add_formula(Sale_Price ~ Longitude + Latitude) # lm_wflow %&gt;% # add_recipe(simple_ames) lm_wflow &lt;- lm_wflow %&gt;% workflows::remove_formula() %&gt;% add_recipe(simple_ames) lm_fit &lt;- fit(lm_wflow, ames_train) predict(lm_fit, ames_test %&gt;% slice(1:3)) ## # A tibble: 3 × 1 ## .pred ## &lt;dbl&gt; ## 1 154932. ## 2 192414. ## 3 262260. lm_fit %&gt;% extract_recipe(estimated = TRUE) lm_fit %&gt;% # This returns the parsnip object: extract_fit_parsnip() %&gt;% # Now tidy the linear model object: tidy() %&gt;% slice(1:5) ## # A tibble: 5 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2431230. 118494. -20.5 1.25e- 85 ## 2 Gr_Liv_Area 263557. 7231. 36.4 3.48e-227 ## 3 Year_Built 900. 59.5 15.1 2.94e- 49 ## 4 Neighborhood_College_Creek 2098. 4235. 0.495 6.20e- 1 ## 5 Neighborhood_Old_Town 4194. 4277. 0.981 3.27e- 1 "],["how-data-are-used-by-the-recipe.html", "8.4 HOW DATA ARE USED BY THE RECIPE", " 8.4 HOW DATA ARE USED BY THE RECIPE 8.4.1 EXAMPLES OF RECIPE STEPS: ENCODING QUALITATIVE DATA IN A NUMERIC FORMAT step_unknown() change missing values to a dedicated factor level step_novel() a new factor level may be encountered in future data step_other() to analyze the frequencies of the factor levels the bottom 1% of the neighborhoods will be lumped into a new level called “other” step_other(Neighborhood, threshold = 0.01) step_dummy() for converting a factor predictor to a numeric format (one_hot argument to include the reference variable) simple_ames &lt;- recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type, data = ames_train) %&gt;% step_log(Gr_Liv_Area, base = 10) %&gt;% step_other(Neighborhood, threshold = 0.01) %&gt;% step_dummy(all_nominal_predictors()) INTERACTION TERMS step_interact(~ interaction terms) one predictor has an effect on the outcome that is contingent on one or more other predictors Numerically, an interaction term between predictors is encoded as their product. ggplot(ames_train, aes(x = Gr_Liv_Area, y = Sale_Price)) + geom_point(alpha = .2) + facet_wrap(~ Bldg_Type) + geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = &quot;lightblue&quot;) + scale_x_log10() + scale_y_log10() + labs(x = &quot;Gross Living Area&quot;, y = &quot;Sale Price (USD)&quot;) Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) + Bldg_Type + log10(Gr_Liv_Area):Bldg_Type or Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) * Bldg_Type simple_ames &lt;- recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type, data = ames_train) %&gt;% step_log(Gr_Liv_Area, base = 10) %&gt;% step_other(Neighborhood, threshold = 0.01) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% # Gr_Liv_Area is on the log scale from a previous step step_interact( ~ Gr_Liv_Area:starts_with(&quot;Bldg_Type_&quot;) ) Additional interactions can be specified in this formula by separating them by + SPLINE FUNCTIONS step_ns() for non-linear relationships Splines replace the existing numeric predictor with a set of columns that allow a model to emulate a flexible, non-linear relationship library(patchwork) library(splines) plot_smoother &lt;- function(deg_free) { ggplot(ames_train, aes(x = Latitude, y = 10^Sale_Price)) + geom_point(alpha = .2) + scale_y_log10() + geom_smooth( method = lm, formula = y ~ ns(x, df = deg_free), color = &quot;lightblue&quot;, se = FALSE ) + labs(title = paste(deg_free, &quot;Spline Terms&quot;), y = &quot;Sale Price (USD)&quot;) } ( plot_smoother(2) + plot_smoother(5) ) / ( plot_smoother(20) + plot_smoother(100) ) recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude, data = ames_train) %&gt;% step_log(Gr_Liv_Area, base = 10) %&gt;% step_other(Neighborhood, threshold = 0.01) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_interact( ~ Gr_Liv_Area:starts_with(&quot;Bldg_Type_&quot;) ) %&gt;% step_ns(Latitude, deg_free = 20) FEATURE EXTRACTION step_normalize() center and scale each column step_pca() principal component analysis, it extracts as much of the original information in the predictor set as possible using a smaller number of features,reducing the correlation between predictors. step_pca(matches(“(SF$)|(Gr_Liv)”)) step_ica() independent component analysis step_umap() uniform manifold approximation and projection ROW SAMPLING STEPS Downsampling ({themis} package) step_downsample(outcome_column_name) Upsampling Hybrid methods step_filter(), step_sample(), step_slice(), and step_arrange() … GENERAL TRANSFORMATIONS step_mutate() NATURAL LANGUAGE PROCESSING can apply natural language processing methods to the data "],["skipping-steps-for-new-data.html", "8.5 SKIPPING STEPS FOR NEW DATA", " 8.5 SKIPPING STEPS FOR NEW DATA subsampling process should not be applied to the data being predicted. skip that, when set to TRUE, will be ignored by the predict() step_&lt;&gt;(...,skip=TRUE) "],["tidy-a-recipe.html", "8.6 TIDY A RECIPE", " 8.6 TIDY A RECIPE tidy() method for recipes ames_rec &lt;- recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude + Longitude, data = ames_train) %&gt;% step_log(Gr_Liv_Area, base = 10) %&gt;% step_other(Neighborhood, threshold = 0.01) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_interact( ~ Gr_Liv_Area:starts_with(&quot;Bldg_Type_&quot;) ) %&gt;% step_ns(Latitude, Longitude, deg_free = 20) tidy(ames_rec) ## # A tibble: 5 × 6 ## number operation type trained skip id ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 1 step log FALSE FALSE log_qfevz ## 2 2 step other FALSE FALSE other_92ULm ## 3 3 step dummy FALSE FALSE dummy_4lh10 ## 4 4 step interact FALSE FALSE interact_DFf9K ## 5 5 step ns FALSE FALSE ns_EvmMV we add the id = \"my_id\" in the step_other() ames_rec &lt;- recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude + Longitude, data = ames_train) %&gt;% step_log(Gr_Liv_Area, base = 10) %&gt;% step_other(Neighborhood, threshold = 0.01, id = &quot;my_id&quot;) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_interact( ~ Gr_Liv_Area:starts_with(&quot;Bldg_Type_&quot;) ) %&gt;% step_ns(Latitude, Longitude, deg_free = 20) lm_wflow &lt;- workflow() %&gt;% add_model(lm_model) %&gt;% add_recipe(ames_rec) lm_fit &lt;- fit(lm_wflow, ames_train) estimated_recipe &lt;- lm_fit %&gt;% extract_recipe(estimated = TRUE) tidy(estimated_recipe, id = &quot;my_id&quot;) ## # A tibble: 21 × 3 ## terms retained id ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Neighborhood North_Ames my_id ## 2 Neighborhood College_Creek my_id ## 3 Neighborhood Old_Town my_id ## 4 Neighborhood Edwards my_id ## 5 Neighborhood Somerset my_id ## 6 Neighborhood Northridge_Heights my_id ## 7 Neighborhood Gilbert my_id ## 8 Neighborhood Sawyer my_id ## 9 Neighborhood Northwest_Ames my_id ## 10 Neighborhood Sawyer_West my_id ## # ℹ 11 more rows tidy(estimated_recipe, number = 2) ## # A tibble: 21 × 3 ## terms retained id ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Neighborhood North_Ames my_id ## 2 Neighborhood College_Creek my_id ## 3 Neighborhood Old_Town my_id ## 4 Neighborhood Edwards my_id ## 5 Neighborhood Somerset my_id ## 6 Neighborhood Northridge_Heights my_id ## 7 Neighborhood Gilbert my_id ## 8 Neighborhood Sawyer my_id ## 9 Neighborhood Northwest_Ames my_id ## 10 Neighborhood Sawyer_West my_id ## # ℹ 11 more rows "],["column-roles.html", "8.7 COLUMN ROLES", " 8.7 COLUMN ROLES add_role(), remove_role(), and update_role() ames_rec %&gt;% update_role(address, new_role = &quot;street address&quot;) "],["resources.html", "8.8 Resources", " 8.8 Resources All possible recipes Develop recipes "],["meeting-videos-8.html", "8.9 Meeting Videos", " 8.9 Meeting Videos 8.9.1 Cohort 1 Meeting chat log 00:06:28 Tyler Grant Smith: getting kind of scruffy jon 00:07:22 Jim Gruman: {purr} 00:07:37 Jim Gruman: {purrr} 00:18:31 Tony ElHabr: is this thing on? 00:18:39 Jonathan Trattner: The chat? 00:18:43 Jon Harmon (jonthegeek): I know, it&#39;s so quiet over here! 00:19:05 Tony ElHabr: quiet chat is making me nervous 00:19:27 Jonathan Trattner: I’ll make some noise 00:19:38 Jonathan Trattner: 🔈 00:20:06 Tyler Grant Smith: it would be good (in the bookdown) to have a comparison of stratified vs non-stratified sampling for this example. with a comparison of the distributions 00:20:07 Tony ElHabr: ugh I read chapter 7 00:21:38 Asmae Toumi: Wait what does all_nominal do, missed it 00:21:51 Jon Harmon (jonthegeek): Selects all columns that... what she&#39;s saying :D 00:21:54 Asmae Toumi: Oh ok nvmd 00:22:30 yonis: We basically we to make to create a design matrix for the regression 00:22:40 Joe Sydlowski: For clarity to Jon&#39;s answer it won&#39;t include numeric vars, right? 00:22:52 Tony ElHabr: right 00:23:06 Jon Harmon (jonthegeek): all_numeric() is its counterpart 00:23:59 Conor Tompkins: step_dummy() drops the reference level, I think 00:24:46 yonis: That is tricky. the reference level isn’t lined up with how base r is defined so you need to be careful with that 00:25:47 yonis: I ran a logistic regression and got into all kinds of trouble with how recipe was defining the ref level 00:27:11 arjun paudel: does it not set the reference level based on order of factor levels? that was my understanding 00:27:29 arjun paudel: if you want a specific level as you reference, you reorder your factor 00:27:49 Conor Tompkins: That is my understanding as well Arjun 00:29:17 Conor Tompkins: I use step_relevel to set the reference level 00:30:15 yonis: https://recipes.tidymodels.org/reference/step_relevel.html 00:30:20 Tyler Grant Smith: why are the counts almost monotonic, but not monotonic? 00:31:42 Conor Tompkins: This is a great table to show this 00:35:44 Scott Nestler: I don&#39;t follow the mention of one-hot encoding in the book. Why would you use that instead of binary encoding like was just shown here. 00:36:37 Asmae Toumi: I say Tilda, is that right? 00:38:01 Tony ElHabr: one-hot: like removing the intercept term in your regression with a univariate categorical variable. so you get coefficients for each term 00:38:16 Conor Tompkins: The winner of the big data bowl determines the pronunciation, I think 00:38:27 Asmae Toumi: Lmaooooooo 00:38:32 Tan Ho: asmae v tony, fightttt 00:38:53 Joe Sydlowski: The benefit of one hot encoding is that you don&#39;t need to know (or explain) what the reference variable is when interpreting the coefficients. Not ever model can use one hot encoding though 00:39:04 Scott Nestler: @ Tony: But wouldn&#39;t that create the linear dependency problem as is discussed in the text? 00:39:35 Scott Nestler: Thanks, Joe. Got it. That makes sense. 00:40:26 Tony ElHabr: i think you&#39;re right about that Scott. or maybe my analogy was just bad 00:41:06 Conor Tompkins: Are there best practices for determining the appropriate reference level? I typically use the most common level 00:42:39 Scott Nestler: ICA is my favorite type of feature extraction to use. Makes use of higher-level moments than PCA, resulting in components that are truly statistically independent, not just uncorrelated. 00:43:04 arjun paudel: @Conor, what level you want as reference is entirely based on context of the problem, I don&#39;t think one standard way of determining the reference level would make sense 00:43:37 Asmae Toumi: conor I pick mine in a way that makes interpretation easier for ppl who digest the findings 00:44:05 Scott Nestler: Agree with Arjun and Asmae; it depends on the variable and ease of interpretation. 00:44:16 Tony ElHabr: any kind of thought put into reference level is probably better than alphabetical imo 00:44:55 Tony ElHabr: Scott, do you have a good reference on ICA? 00:45:57 Jordan Krogmann: Has anyone had to create the &quot;bake&quot; function in sql? Let me tell you it&#39;s less than fun for new records hitting your model... 00:46:12 Scott Nestler: Book by Hyvarinen, Karhunen, Oja is one standard. Book by Stone is more approachable. I have a presentation on it that I developed too. Happy to share. 00:46:25 Daniel Chen: how is the recipie object implemented? is it a dataframe with an attribute table that defines whether or not a variable is a predictor or response? 00:47:22 Jon Harmon (jonthegeek): There&#39;s a tibble in there, but it has a lot going on. I... can&#39;t remember details, it&#39;s been a bit since I dug into it. 00:48:47 Daniel Chen: s3 objects are just lists with an attr defined, right? 00:50:16 Jon Harmon (jonthegeek): They&#39;re not necessarily lists. 00:50:33 Tony ElHabr: attributes are the magic to S3 00:50:48 Tony ElHabr: but right, not necessarily lists 00:50:49 Tan Ho: deep, dark magic 00:50:52 Jon Harmon (jonthegeek): If I remember right, recipes are lots o&#39; attributes. 00:50:56 Tyler Grant Smith: sounds like someone wants to write dbrecipes 00:51:04 Tony ElHabr: sounds like ETL 00:51:15 Asmae Toumi: dbrecipes omg 00:51:46 Conor Tompkins: Luv 2 engineer data 00:51:53 Tan Ho: &quot;do all the work for you&quot; eh 00:52:35 arjun paudel: hahaa 00:52:56 arjun paudel: i meant you don&#39;t have to prep or bake it yourself 00:53:13 Jordan Krogmann: lol, I guess I was looking for a reason to lose sleep @tyler 00:53:28 Tony ElHabr: step_drop_table in dbrecipes could be disastrous 00:54:46 Conor Tompkins: step_rm_rf 00:54:49 Tony ElHabr: tidymodels before workflows was an experience 00:55:14 Tan Ho: it&#39;s superseded by bake NULL 00:55:49 Tony ElHabr: every time i typed juice() i felt disgusting 00:56:00 Asmae Toumi: lmfao 00:56:29 Conor Tompkins: Workflow feels like the %&gt;% for tidymodels 00:58:17 Asmae Toumi: hahahahahahah 00:58:36 Tyler Grant Smith: no congrats for tan 00:58:39 Tony ElHabr: thanks y&#39;all 00:59:03 Jordan Krogmann: Thanks pavitra! 00:59:06 Jordan Krogmann: great job 00:59:10 Daniel Chen: my prelims are next week xDDD 00:59:14 Asmae Toumi: I made parsnip puree this weekend it was sooooooooooo good, so much better than mashed potatoes 00:59:24 Tony ElHabr: tony, tan, tyler, t-rex 00:59:26 Tony ElHabr: all the same 00:59:47 Asmae Toumi: Jordan should go since he’s gonna be busy soon writing dbrecipes 01:00:48 Asmae Toumi: YESSSSSSSSSSSSSSSSS 01:00:54 Asmae Toumi: My impact!!!!! 01:01:15 Jordan Krogmann: lol thanks a lot Asmae! 01:01:45 Tony ElHabr: thanks so much Pavitra! 01:01:50 Tony ElHabr: great presentation 01:01:54 Jonathan Trattner: Great job Pavitra! 01:02:00 Conor Tompkins: Thanks Pavitra! 01:02:00 Andrew G. Farina: Thank you Pavitra, that was a busy chapter! 01:02:09 Jonathan Trattner: I’ll deff be asking about prep and bake again (: 01:03:11 Scott Nestler: I&#39;m surprised that &quot;mise en place&quot; didn&#39;t make an appearance in this chapter. 01:03:12 Jonathan Trattner: I’ll play with it a little bit but thanks! 01:04:48 Daniel Chen: it&#39;ll be less weird when we get to workflow 01:05:09 Jonathan Trattner: Thank you! 01:06:40 Jordan Krogmann: Thanks gonna drop guys, it&#39;s been great! 8.9.2 Cohort 2 Meeting chat log 00:18:37 Kevin Kent: I found out the other day that you can group tabs in chrome 00:18:51 shamsuddeen: yes 00:18:57 Stephen Holsenbeck: yes! such a great new feature, I love it 00:18:59 Luke Shaw: Yes Kevin! I&#39;m a big fan 00:19:10 shamsuddeen: Now, feature comes in today in Chrome 00:19:12 Kevin Kent: https://blog.google/products/chrome/manage-tabs-with-google-chrome/ 00:19:37 Amélie Gourdon-Kanhukamwe (she/they): Ah, me too just last week, which means I feel even less bad running dozens at a time! 00:20:21 shamsuddeen: All tab, are automatically group in dropdown. Only available in Chrome Beta. - :) 00:20:30 shamsuddeen: Released this feature today 00:20:53 Layla Bouzoubaa: 👀 00:30:20 Layla Bouzoubaa: https://recipes.tidymodels.org/reference/step_YeoJohnson.html 00:32:00 Amélie Gourdon-Kanhukamwe (she/they): Discretization is reducing numerical variables into ordinal ones, with equal width or equal frequency of bins. My understanding is equal width is preferred. 00:33:38 Amélie Gourdon-Kanhukamwe (she/they): And that would be because some techniques don&#39;t cope would be fully continuous variables, for example Naïve Bayes (as per my understanding after learning machine learning with Weka). 00:34:16 Luke Shaw: interesting, thanks :) 00:34:18 Stephen - Computer - No Mic: Interesting, thank you Amelie 00:39:13 Luke Shaw: I love janitor::clean_names() for fixing that kind of thing 00:39:57 Luke Shaw: hmmmm there must be a way I agree! 00:40:45 Stephen - Computer - No Mic: mutate(data, col = gsub(&quot;\\\\+&quot;, col, &quot;p&quot;)) 00:41:11 Stephen - Computer - No Mic: or mutate(data, col = stringr::str_replace(col, &quot;\\\\+&quot;, &quot;p&quot;)) 00:42:01 Amélie Gourdon-Kanhukamwe (she/they): Actually, I retract the part about Naïve Bayes *needing* discretization, sorry. Can&#39;t name with certainty a classifier that needs discretizing specifically for now, but it is believed to help performance (it didn&#39;t much on my only project playing with ML). 00:42:04 Kevin Kent: One gotcha I’ve come across with making dummy variables or one-hot encoding is that the test set has levels of a dummified variable that the train set didn’t have. Argument for having or other methods I think 00:42:20 Kevin Kent: *sometimes has 00:43:13 Kevin Kent: **hashing or other methods 00:43:32 Amélie Gourdon-Kanhukamwe (she/they): But see eg: https://link.springer.com/content/pdf/10.1007/s10994-008-5083-5.pdf 00:52:38 Amélie Gourdon-Kanhukamwe (she/they): What if you do this Layla? simple_ames$var_info 00:52:57 Stephen - Computer - No Mic: Hey shamsuddeen, I didn&#39;t get any issue using the pipe: recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type, data = ames_train) %&gt;% step_log(Gr_Liv_Area, base = 10) %&gt;% step_dummy(all_nominal()) %&gt;% prep(training = ames_train) 00:53:00 Amélie Gourdon-Kanhukamwe (she/they): Only if you want to see the list of var? 00:53:22 Stephen - Computer - No Mic: The prep function takes the recipe as it&#39;s first argument 00:53:38 Stephen - Computer - No Mic: So piping it in to the first argument (as is default with a piple) works fine 00:55:07 shamsuddeen: Ok, thank you for looking at this. Below is an example that does not works. 00:55:12 shamsuddeen: simple_ames &lt;- ames_train %&gt;% recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type) %&gt;% step_log(Gr_Liv_Area, base = 10) %&gt;% step_dummy(all_nominal()) 00:55:19 shamsuddeen: The above works 00:55:35 Stephen - Computer - No Mic: Yes 00:55:41 shamsuddeen: The one below does not works 00:55:42 shamsuddeen: simple_ames &lt;- ames_train %&gt;% prep(simple_ames) 00:56:19 Stephen - Computer - No Mic: Yeah, simple_ames has not been created yet 00:56:27 Stephen - Computer - No Mic: You&#39;re assigned it in the step 00:56:32 Stephen - Computer - No Mic: So it can&#39;t be used in the step 00:56:40 Stephen - Computer - No Mic: You&#39;ve* 00:57:19 Stephen - Computer - No Mic: The variable simple_ames must exist in the environment by assignment before it can be used in code 00:57:48 shamsuddeen: But I call them in series 00:57:53 shamsuddeen: Like below 00:57:58 shamsuddeen: simple_ames &lt;- ames_train %&gt;% recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type) %&gt;% step_log(Gr_Liv_Area, base = 10) %&gt;% step_dummy(all_nominal()) 00:58:03 shamsuddeen: simple_ames &lt;- ames_train %&gt;% prep(simple_ames) 00:58:10 Stephen - Computer - No Mic: Ok 00:58:20 Stephen - Computer - No Mic: So you&#39;re passing in simple_ames as the recipe for prep 00:58:21 shamsuddeen: The Ames has been created in the first place 00:58:26 Stephen - Computer - No Mic: So to make that work: 00:58:53 Stephen - Computer - No Mic: simple_ames &lt;- ames_train %&gt;% {prep(simple_ames, training = .)} 00:59:08 Stephen - Computer - No Mic: The ames_train data has to be the second argument 00:59:22 Stephen - Computer - No Mic: So you have to use the brackets and the . to pipe it into that second spot 00:59:29 shamsuddeen: Yay…it works 00:59:39 shamsuddeen: Thanks 01:00:10 shamsuddeen: But, why in recipe we use not this format? 01:00:33 Stephen - Computer - No Mic: It&#39;s just the order of arguments, one moment I&#39;ll see if I can find an article on it 01:01:49 Luke Shaw: Harking back to a previous Q by Layla on name_repair error for the values in a column, I think the function janitor::make_clean_names(col) should do it - cleans the values of the column inputted 01:02:10 shamsuddeen: Yea, janitor can do that 01:02:27 Stephen - Computer - No Mic: shamsuddeen: https://magrittr.tidyverse.org/#the-argument-placeholder 01:02:33 shamsuddeen: Today session is full of questions -:) 01:02:49 Stephen - Computer - No Mic: And this: https://thatdatatho.com/tutorial-about-magrittrs-pipe-operator-and-placeholders/ 01:03:22 Kevin Kent: This fits with my mental model but not what we saw in august’s session https://stackoverflow.com/questions/62189885/what-is-the-difference-among-prep-bake-juice-in-the-r-package-recipes 01:08:14 Stephen - Computer - No Mic: This is really helpful 👆 01:08:41 shamsuddeen: Yes, it makes much much sense 01:09:45 Kevin Kent: People often forgot about the role of domain expertise in feature engineering. I’ve found that it helps prediction a lot and you come up with features that you or an algorithm would have never considered. 01:10:03 Layla Bouzoubaa: Thanks, Luke!! 01:10:12 Kevin Kent: Especially when you have a large number of possible data sets and features 01:11:04 Stephen - Computer - No Mic: Definitely 01:13:09 Luke Shaw: Would step_other cope better with the gotcha Kevin mentioned before? Of a value in test not seen in train 01:13:24 Janita Botha: Yes we can keep gping! 01:14:05 Kevin Kent: I think as long as the new level isn’t in the top n in the test set @luke 01:15:15 Kevin Kent: The model I was using just outputted it as a warning but I think the consequence is that it kind of ignored that new level, which isn’t ideal 01:15:39 Luke Shaw: Ah yeah that makes sense, thanks :) In that scenario I guess it a fairly big problem that something common in test data was never seen in train 01:16:14 Kevin Kent: yeah. and particularly challenging with categorical data…but I think embeddings might be the best way to address that for categorical data 01:19:25 Luke Shaw: Thanks August! :) 01:19:38 shamsuddeen: Thanks August 01:20:02 Kevin Kent: Thanks! Great discussion and presentatio 01:20:05 Kevin Kent: *presentation 01:22:54 Janita Botha: Lol 01:22:59 Janita Botha: We call iy autumn 01:23:00 Layla Bouzoubaa: Thanks everyone! 8.9.3 Cohort 3 Meeting chat log 00:30:17 Toryn Schafer (she/her): It says on the juice help page that it is superseded by bake 00:30:47 Ildiko Czeller: https://recipes.tidymodels.org/articles/Ordering.html 00:51:31 Ildiko Czeller: step_pca(matches(&quot;(SF$)|(Gr_Liv)&quot;), num_comp = 2) 00:52:01 jiwan: https://juliasilge.com/blog/best-hip-hop/ Julia Silge has a nice blog post on PCA 01:01:52 jiwan: ?step_normalize( 8.9.4 Cohort 4 Meeting chat log 00:38:40 Ryan Metcalf: After loading tidy models package, I ran `recipes::step` and reviewed the drop downs. This doesn’t directly answer your question….but may give some breadcrumbs for further reading. Maybe? 00:39:08 Ryan Metcalf: @Laura, what was the other package you mentioned? GAM? 00:39:20 Laura Rose: thanks. I wonder if it&#39;s not integrated with tidymodels...the s() function that is 00:39:28 Laura Rose: yeah there&#39;s mgcv and gam 00:39:33 Laura Rose: I think 00:39:44 Laura Rose: maybe it&#39;s mcgv 00:48:19 Isabella Velásquez: Thank you! "],["judging-model-effectiveness.html", "Chapter 9 Judging model effectiveness", " Chapter 9 Judging model effectiveness Learning objectives: How will we evaluate the performance of our workflow? How the model will be used? (predictive strength is primary) How close our predictions come to the observed data? How closely this model fits the actual data? "],["performance-metrics-and-inference.html", "9.1 Performance Metrics and Inference", " 9.1 Performance Metrics and Inference In this chapter, we will be talking about the qualities of a model, while applying several functions from the yardstick package. library(tidyverse) library(tidymodels) tidymodels_prefer() library(DiagrammeR) library(viridis) This package focuses on methods of resampling that are critical to modeling activities, such as performance measures and performance metrics. yardstick: Tidy Characterizations of Model Performance Identification of the quality of a model: The main takeaway of this chapter is Judging the model effectiveness, or identification of the effectiveness of the modeling procedures. Constraints may arise when the model uses different units for measuring the differences between observed and predicted values. In particular, transformations can be applied to standardize observed values so that they can be used in the model interchangeably. Somehow if a transformation is already in place within some variables in the observed data, it will be important to identify the type of transformation applied in order to proceed with the model specification correctly. It is even for this reason that the use of model metrics is very important. The metrics are able to summarize the results of a model. There are different types of metrics that can be used to summarize the results of a model fit, depending on the type of response variable whether is numeric or categorical, and so if a regression or classification modeling procedure is performed. We can use: the Root Mean Squared Error (RMSE), a performance metric used in regression modeling. the Accuracy, to estimate the model error the ROC and AUC, the receiver observation curve and the area under the curve, respectively, if we perform a classification modeling. This curve is calculated combining the Specificity and Sensitivity of the model. "],["a-little-recap-of-previous-chapters.html", "9.2 A little Recap of previous chapters", " 9.2 A little Recap of previous chapters A useful model would include: parameter estimation model selection and tuning performance assessment 9.2.1 Case Study 1 The construction of a model implies: check for correlation; to see if predictors influence each other, and in what way. For example, the predictors correlation can influence the estimation of the outcome, this can be identified when the estimated values change sign of have very different values, if a modification in association takes place. Here we use the crickets data from modeldata package: data(crickets, package = &quot;modeldata&quot;) crickets %&gt;% head ## # A tibble: 6 × 3 ## species temp rate ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 O. exclamationis 20.8 67.9 ## 2 O. exclamationis 20.8 65.1 ## 3 O. exclamationis 24 77.3 ## 4 O. exclamationis 24 78.7 ## 5 O. exclamationis 24 79.4 ## 6 O. exclamationis 24 80.4 In this case we check for the correlation between the response and the predictor: cor(crickets$rate,crickets$temp) ## [1] 0.9591189 While if we look at the correlation between predictors, which in this case one of the predictor is categorical, we need to make a transformation of the categorical predictor in order to be able to calculate the correlation. crickets%&gt;%count(species) ## # A tibble: 2 × 2 ## species n ## &lt;fct&gt; &lt;int&gt; ## 1 O. exclamationis 14 ## 2 O. niveus 17 crickets1 &lt;- crickets%&gt;% mutate(species=ifelse(species==&quot;O. exclamationis&quot;,1,0)) crickets1 %&gt;% head ## # A tibble: 6 × 3 ## species temp rate ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 20.8 67.9 ## 2 1 20.8 65.1 ## 3 1 24 77.3 ## 4 1 24 78.7 ## 5 1 24 79.4 ## 6 1 24 80.4 cor(crickets1$species,crickets1$temp) ## [1] 0.4807908 construction of the model; sample data are used to make a simple linear model with the function lm(), and a comparison of the two models is done with anova() function. A little EDA, exploratory data analysis is done to identfy the relationship between response and predictors. crickets %&gt;% ggplot(aes(temp,rate,color=species))+ geom_point()+ geom_smooth(se=F)+ theme_linedraw() interaction_fit &lt;- lm(rate ~ (temp + species)^2, data = crickets) main_effect_fit &lt;- lm(rate ~ temp + species, data = crickets) # Compare the two: standard two-way analysis of variance anova(main_effect_fit, interaction_fit) ## Analysis of Variance Table ## ## Model 1: rate ~ temp + species ## Model 2: rate ~ (temp + species)^2 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 28 89.350 ## 2 27 85.074 1 4.2758 1.357 0.2542 estimation of the model; obtained using the predict() function; it conducts specific calculations after the fitted model is created, in this case we test it against new data. new_values &lt;- data.frame(species = &quot;O. exclamationis&quot;, temp = 15:20) pred &lt;- predict(main_effect_fit, new_values) prediction_values &lt;- bind_cols(new_values,rate=pred) bind_rows(crickets,prediction_values)%&gt;% ggplot(aes(temp,rate,color=species))+ geom_point()+ geom_smooth(se=F)+ theme_linedraw() 9.2.2 Tidymodels: modeling as a step by step mode The strategy of Tidymodels is to decide for a model to use on a step-by-step mode, this is the main difference than just using lm() or glm() functions and then predict(). When valuing a model, the fundamental is to have a clear view of the data and the type of manipulation to obtain the answer to our questions The first step is to evaluate the structure of the data, it needs to be balanced for deciding a smart strategy for allocating data. Smart Strategy for allocating data: allocate specific subsets of data for different tasks allocate the largest possible amount to the model parameter estimation only The second is to split the data into two main parts as train and test sets, sometimes a third set is valued such as the validation set. Splitting data: training set (the substrate to develop the model and estimate the parameters) validation set (a small set of data to measure performance as the network was trained) test set (the final (unbiased) arbiter to determine the efficacy of the model) data(ames, package = &quot;modeldata&quot;) set.seed(123) ames_split &lt;- initial_split(ames, prop = 0.80) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) The step-by-step model decomposes the modeling procedure into a certain number of customized steps. Preprocessing model functions: recipe() prep() bake() Recipe objects for feature engineering and data preprocessing prior to modeling. Transformations and encoding of the data with recipes::recipe() help choosing the option which is the most associated with the outcome, through data preprocessing techniques applying it for different models. recipe() defines the formula and allows for the preprocessing steps of a model with the help of the step_** functions Figure 9.1 Graph of the modeling steps Figure 9.1: Graph of the modeling steps The matrix data transformation is obtained from a data frame through the modeling procedure of preprocessing data to obtain a new designed matrix. Modeling steps 9.2.3 Workflow: to combine models and recipes Once the preprocessing steps are done, data are allocated to test and training sets, and the model engine is set, the next step involves wrapping everything up into a workflow to have all the steps together. 9.2.4 Case Study 2 NYC flights data modeling Let’s see this in practice with an example taken from: https://www.tidymodels.org/start/recipes/ Data are from {nycflights13} package: library(nycflights13) # tidy data and manipulation set.seed(123) flight_data &lt;- flights %&gt;% # data modification mutate(arr_delay = ifelse(arr_delay &gt;= 30, &quot;late&quot;, &quot;on_time&quot;), arr_delay = factor(arr_delay), date = as.Date(time_hour)) %&gt;% inner_join(weather, by = c(&quot;origin&quot;, &quot;time_hour&quot;)) %&gt;% select(dep_time, flight, origin, dest, air_time, distance, carrier, date, arr_delay, time_hour) %&gt;% na.omit() %&gt;% mutate_if(is.character, as.factor) flight_data %&gt;% head ## # A tibble: 6 × 10 ## dep_time flight origin dest air_time distance carrier date arr_delay ## &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;date&gt; &lt;fct&gt; ## 1 517 1545 EWR IAH 227 1400 UA 2013-01-01 on_time ## 2 533 1714 LGA IAH 227 1416 UA 2013-01-01 on_time ## 3 542 1141 JFK MIA 160 1089 AA 2013-01-01 late ## 4 544 725 JFK BQN 183 1576 B6 2013-01-01 on_time ## 5 554 461 LGA ATL 116 762 DL 2013-01-01 on_time ## 6 554 1696 EWR ORD 150 719 UA 2013-01-01 on_time ## # ℹ 1 more variable: time_hour &lt;dttm&gt; flight_data%&gt;% ggplot(aes(arr_delay))+ geom_bar() Spending our data: ## split set.seed(555) data_split &lt;- initial_split(flight_data, prop = 3/4) train_data &lt;- training(data_split) test_data &lt;- testing(data_split) Apply the recipe: flights_rec &lt;- recipe(arr_delay ~ ., data = train_data) %&gt;% update_role(flight, time_hour, new_role = &quot;ID&quot;) %&gt;% step_date(date, features = c(&quot;dow&quot;, &quot;month&quot;)) %&gt;% step_holiday(date, holidays = timeDate::listHolidays(&quot;US&quot;)) %&gt;% step_rm(date) %&gt;% step_dummy(all_nominal(), -all_outcomes()) %&gt;% # remove columns from the data when the training # set data have a single value step_zv(all_predictors()) flights_rec %&gt;% prep() %&gt;% bake(new_data = NULL) %&gt;% head ## # A tibble: 6 × 161 ## dep_time flight air_time distance time_hour arr_delay ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;fct&gt; ## 1 656 2116 39 184 2013-02-20 07:00:00 on_time ## 2 2030 1465 339 2586 2013-07-25 19:00:00 late ## 3 1712 3448 92 589 2013-08-06 17:00:00 on_time ## 4 853 4087 96 645 2013-09-22 08:00:00 on_time ## 5 1709 652 39 200 2013-09-28 17:00:00 on_time ## 6 2257 608 46 273 2013-01-22 22:00:00 on_time ## # ℹ 155 more variables: date_USChristmasDay &lt;int&gt;, date_USColumbusDay &lt;int&gt;, ## # date_USCPulaskisBirthday &lt;int&gt;, date_USDecorationMemorialDay &lt;int&gt;, ## # date_USElectionDay &lt;int&gt;, date_USGoodFriday &lt;int&gt;, ## # date_USInaugurationDay &lt;int&gt;, date_USIndependenceDay &lt;int&gt;, ## # date_USJuneteenthNationalIndependenceDay &lt;int&gt;, date_USLaborDay &lt;int&gt;, ## # date_USLincolnsBirthday &lt;int&gt;, date_USMemorialDay &lt;int&gt;, ## # date_USMLKingsBirthday &lt;int&gt;, date_USNewYearsDay &lt;int&gt;, … Fit a model with a recipe and apply the workflow: process the recipe using the training set apply the recipe to the training set apply the recipe to the test set lr_mod &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) Workflow: to simplify the process a parsnip object pairs a model and recipe together flights_wflow &lt;- workflow() %&gt;% add_model(lr_mod) %&gt;% add_recipe(flights_rec) flights_wflow ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: logistic_reg() ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## 5 Recipe Steps ## ## • step_date() ## • step_holiday() ## • step_rm() ## • step_dummy() ## • step_zv() ## ## ── Model ─────────────────────────────────────────────────────────────────────── ## Logistic Regression Model Specification (classification) ## ## Computational engine: glm Fit: The function to prepare the recipe and train the model from the resulting predictors # this takes a bit flights_fit &lt;- flights_wflow %&gt;% # prepare the recipe and train the model fit(data = train_data) Extract the model or recipe objects from the workflow: flights_fit %&gt;% # pull_workflow_fit() %&gt;% tidy() %&gt;% head ## # A tibble: 6 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 5.26 2.72 1.93 5.36e- 2 ## 2 dep_time -0.00167 0.0000141 -118. 0 ## 3 air_time -0.0438 0.000561 -78.1 0 ## 4 distance 0.00615 0.00150 4.10 4.12e- 5 ## 5 date_USChristmasDay 1.14 0.171 6.70 2.12e-11 ## 6 date_USColumbusDay 0.626 0.169 3.71 2.10e- 4 Use a trained workflow to predict following these steps: build the model (lr_mod), create a preprocessing recipe (flights_rec), bundle the model and recipe (flights_wflow), and train the workflow using a single call to fit() Figure 9.2 Graph of the workflow steps Figure 9.2: Graph of the workflow steps Then use the trained workflow to predict using the test data or any other new data: flights_pred &lt;- predict(flights_fit, test_data, type = &quot;prob&quot;) %&gt;% bind_cols(test_data %&gt;% select(arr_delay, time_hour, flight)) flights_pred %&gt;%head ## # A tibble: 6 × 5 ## .pred_late .pred_on_time arr_delay time_hour flight ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dttm&gt; &lt;int&gt; ## 1 0.0183 0.982 on_time 2013-01-01 06:00:00 461 ## 2 0.0426 0.957 on_time 2013-01-01 06:00:00 5708 ## 3 0.0413 0.959 on_time 2013-01-01 06:00:00 71 ## 4 0.0253 0.975 on_time 2013-01-01 06:00:00 194 ## 5 0.0306 0.969 on_time 2013-01-01 06:00:00 1743 ## 6 0.0236 0.976 on_time 2013-01-01 06:00:00 1077 Figure 9.3 Graph of the workflow to predict Figure 9.3: Graph of the workflow to predict How will we evaluate the performance of our workflow? To finally answer our question we need to check the area under the ROC curve. What is a ROC curve? It is a curve that identifies the area of credibility of our model. To calculate the curve we use two functions: roc_curve() and roc_auc(), the curve and the area under the curve, respectively. The ROC curve uses the class probability estimates to give us a sense of performance across the entire set of potential probability cutoffs. Once the predicted class of probabilities are obtained we can generate a ROC curve. Let’s see it applied in our NYC flights case study. We need late and on_time variable predictors to create the curve and apply the autoplot() method as shown below: flights_pred %&gt;% roc_curve(truth = arr_delay, .pred_late)%&gt;% ggplot(aes(x=1-specificity,y=sensitivity))+ geom_abline(linetype=&quot;dashed&quot;)+ geom_line()+ coord_equal() flights_pred %&gt;% roc_curve(truth = arr_delay, .pred_late) %&gt;% autoplot() flights_pred %&gt;% roc_auc(truth = arr_delay, .pred_late) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.761 In particular, below are shown the steps for different cases, and the visualization used to evaluate the credibility of a model. "],["functions-used-to-measure-predictive-strengths-of-a-model.html", "9.3 Functions used to measure predictive strengths of a model", " 9.3 Functions used to measure predictive strengths of a model The assessment of the models is via empirical validation and grouped by the nature of the outcome data, and this can be done through: Regression: regression metrics (purely numeric) Classification: binary classes multilevel metrics (three or more class levels) 9.3.1 Case Study 3 For this example data is from The Trust for Public Land for ranking the public parks in the US. The dataset ranges within a period between 2012 and 2020, in 102 US cities, parks are ranked by characteristics of services. In particular we will be looking at selected amenities in the parks, which are things that conduce to comfort, convenience, or enjoyment. https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-06-22/readme.md Data load library(tidytuesdayR) tuesdata &lt;- tidytuesdayR::tt_load(2021, week = 26) ## ## Downloading file 1 of 1: `parks.csv` parks &lt;- tuesdata$parks parks%&gt;%head ## # A tibble: 6 × 28 ## year rank city med_park_size_data med_park_size_points park_pct_city_data ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2020 1 Minnea… 5.7 26 15% ## 2 2020 2 Washin… 1.4 5 24% ## 3 2020 3 St. Pa… 3.2 14 15% ## 4 2020 4 Arling… 2.4 10 11% ## 5 2020 5 Cincin… 4.4 20 14% ## 6 2020 6 Portla… 4.9 22 18% ## # ℹ 22 more variables: park_pct_city_points &lt;dbl&gt;, pct_near_park_data &lt;chr&gt;, ## # pct_near_park_points &lt;dbl&gt;, spend_per_resident_data &lt;chr&gt;, ## # spend_per_resident_points &lt;dbl&gt;, basketball_data &lt;dbl&gt;, ## # basketball_points &lt;dbl&gt;, dogpark_data &lt;dbl&gt;, dogpark_points &lt;dbl&gt;, ## # playground_data &lt;dbl&gt;, playground_points &lt;dbl&gt;, rec_sr_data &lt;dbl&gt;, ## # rec_sr_points &lt;dbl&gt;, restroom_data &lt;dbl&gt;, restroom_points &lt;dbl&gt;, ## # splashground_data &lt;dbl&gt;, splashground_points &lt;dbl&gt;, … parks%&gt;%names ## [1] &quot;year&quot; &quot;rank&quot; ## [3] &quot;city&quot; &quot;med_park_size_data&quot; ## [5] &quot;med_park_size_points&quot; &quot;park_pct_city_data&quot; ## [7] &quot;park_pct_city_points&quot; &quot;pct_near_park_data&quot; ## [9] &quot;pct_near_park_points&quot; &quot;spend_per_resident_data&quot; ## [11] &quot;spend_per_resident_points&quot; &quot;basketball_data&quot; ## [13] &quot;basketball_points&quot; &quot;dogpark_data&quot; ## [15] &quot;dogpark_points&quot; &quot;playground_data&quot; ## [17] &quot;playground_points&quot; &quot;rec_sr_data&quot; ## [19] &quot;rec_sr_points&quot; &quot;restroom_data&quot; ## [21] &quot;restroom_points&quot; &quot;splashground_data&quot; ## [23] &quot;splashground_points&quot; &quot;amenities_points&quot; ## [25] &quot;total_points&quot; &quot;total_pct&quot; ## [27] &quot;city_dup&quot; &quot;park_benches&quot; EDA: Exploratory data analysis ggplot(parks,aes(x=year,y = rank))+ geom_col(aes(fill=city)) + labs(x=&quot;Year&quot;, y = &quot;Rank values&quot;)+ guides(fill=&quot;none&quot;)+ labs(title=&quot;US City Ranks per Year&quot;)+ theme_minimal() Select three years 2018, 2019, and 2020 and 99 cities, with full information. parks_long &lt;- parks %&gt;% select(-amenities_points,-total_points, -contains(&quot;_data&quot;), -park_benches,-city_dup)%&gt;% drop_na() %&gt;% pivot_longer( cols = contains(&quot;_points&quot;), names_to = &quot;amenities&quot;, values_to = &quot;points&quot; ) %&gt;% mutate(amenities=gsub(&quot;_points&quot;,&quot;&quot;,amenities)) parks_long%&gt;% head ## # A tibble: 6 × 6 ## year rank city total_pct amenities points ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2020 1 Minneapolis 85.3 med_park_size 26 ## 2 2020 1 Minneapolis 85.3 park_pct_city 38 ## 3 2020 1 Minneapolis 85.3 pct_near_park 98 ## 4 2020 1 Minneapolis 85.3 spend_per_resident 100 ## 5 2020 1 Minneapolis 85.3 basketball 47 ## 6 2020 1 Minneapolis 85.3 dogpark 65 parks_long %&gt;% ggplot(aes(x = total_pct, y = rank, group=year,color=factor(year))) + geom_point(size = 0.5, alpha = 0.7) + geom_smooth(aes(color=factor(year)),linewidth=0.3,se=F) + scale_y_reverse()+ scale_color_viridis(discrete = TRUE) + labs(title = &quot;Amenities points&quot;,color=&quot;Year&quot;) + theme_minimal() Let’s pose some questions before to choose: What are we going to predict? What is our research question? Data Split set.seed(123) parks_split &lt;- initial_split(parks_long, strata=rank,prop = 0.80) parks_train &lt;- training(parks_split) parks_test &lt;- testing(parks_split) Preprocessing steps: recipe() %&gt;% step_*.* This step involves setting the model formula and eventually make some data preprocessing with the help of the step_*.* functions. In this case we don’t make any extra manipulations, in the first step of our model. parks_rec &lt;- recipe( rank ~ ., data = parks_train ) Set the Workflow Wrap everything into a workflow. # set model engine lm_model &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) # use a workflow lm_wflow &lt;- workflow() %&gt;% add_model(lm_model) %&gt;% add_recipe(parks_rec) Fit the workflow with the training set. lm_fit &lt;- fit(lm_wflow, parks_train) lm_fit %&gt;% tidy() %&gt;% head ## # A tibble: 6 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2162. 88.0 -24.6 3.71e-118 ## 2 year 1.14 0.0437 26.1 6.81e-131 ## 3 cityAnaheim 1.62 0.477 3.39 7.06e- 4 ## 4 cityAnchorage -1.08 0.438 -2.46 1.40e- 2 ## 5 cityArlington, Texas 3.21 0.504 6.38 2.22e- 10 ## 6 cityArlington, Virginia 9.39 0.621 15.1 2.53e- 49 Predict with new data from the testing set # predict(lm_fit, parks_test %&gt;% slice(1:3)) # test the model on new data pred &lt;- predict(lm_fit, new_data = parks_test %&gt;% filter(city %in% c(&quot;Seattle&quot;,&quot;Atlanta&quot;,&quot;Baltimore&quot;))) parks_test_res &lt;- predict(lm_fit, new_data = parks_test %&gt;% select(-rank)) %&gt;% bind_cols(parks_test %&gt;% select(rank)) parks_test_res%&gt;%head ## # A tibble: 6 × 2 ## .pred rank ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0862 1 ## 2 0.180 1 ## 3 0.187 1 ## 4 2.66 2 ## 5 3.76 3 ## 6 3.90 3 ggplot(parks_test_res, aes(x = rank, y = .pred)) + # Create a diagonal line: geom_abline(lty = 2) + geom_point(alpha = 0.5) + labs(y = &quot;Predicted Rank&quot;, x = &quot;Rank&quot;) + # Scale and size the x- and y-axis uniformly: coord_obs_pred() lm_fit%&gt;% augment(new_data = parks_test %&gt;% filter(city %in% c(&quot;Seattle&quot;,&quot;Atlanta&quot;,&quot;Baltimore&quot;)))%&gt;% group_by(city)%&gt;% reframe(rank=mean(rank),.pred=mean(.pred)) ## # A tibble: 3 × 3 ## city rank .pred ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Atlanta 41.2 41.9 ## 2 Baltimore 60 60.0 ## 3 Seattle 11 10.7 ggplot() + geom_point(data=parks_long, aes(x = total_pct, y = (rank)), color=&quot;grey0&quot;, size = 0.5, alpha = 0.7)+ scale_y_reverse()+ geom_smooth(data= lm_fit%&gt;% augment(new_data = parks_test), aes(total_pct,.pred), color=&quot;darkred&quot;, size = 0.5, alpha = 0.7) Apply the root mean squared error rmse() The first measure used for the model is the root mean squared error: RMSE # rmse(data, truth = outcome, estimate = .pred) rmse(parks_test_res, truth = rank, estimate = .pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 1.57 Then make a comparison adding more metrics at once: Multiple metrics at once # data_metrics &lt;- metric_set(rmse, rsq, mae) # data_metrics(data_test_res, truth = outcome, estimate = .pred) parks_metrics &lt;- metric_set(rmse, rsq, mae) parks_metrics(parks_test_res, truth = rank, estimate = .pred) ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 1.57 ## 2 rsq standard 0.997 ## 3 mae standard 1.21 Here we use some examples from the book with a sample predictions and multiple resampling: Binary classes Confusion matrix: Confusion matrix gives a holistic view of the performance of your model What is a Confusion Matrix? It is a matrix that contains values such as: True Positive (TP) True Negative (TN) False Positive – Type 1 Error (FP) False Negative – Type 2 Error (FN) Confusion matrix Example 1: two_class_example data(&quot;two_class_example&quot;) conf_mat(two_class_example, truth = truth, estimate = predicted) ## Truth ## Prediction Class1 Class2 ## Class1 227 50 ## Class2 31 192 The confusion matrix contains other metrics that can be extracted under specific conditions. Precision is how certain you are of your true positives Recall is how certain you are that you are not missing any positives. The measure used to estimate the effectiveness is the overall accuracy. It uses the hard class predictions to measure performance, which tells us whether our model is actually estimating a probability of cutoff to establish if the model predicted well or not with accuracy. Accuracy: accuracy(two_class_example, truth = truth, estimate = predicted) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.838 Matthews correlation coefficient: mcc(two_class_example, truth, predicted) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 mcc binary 0.677 F1 metric: F1-score is a harmonic mean of Precision and Recall. The F1-score captures both the trends in a single value: when we try to increase the precision of our model, the recall (aka, sensitivity) goes down, and vice-versa. f_meas(two_class_example, truth, predicted) #,event_level = &quot;second&quot;) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 f_meas binary 0.849 All of the above have the event_level argument (first/second level) To visualize the model metrics behavior, the receiver operating characteristic (ROC) curve computes the sensitivity and specificity over a continuum of different event thresholds roc_curve() (curve) roc_auc() (area) two_class_curve &lt;- roc_curve(two_class_example, truth, Class1) two_class_curve %&gt;% head ## # A tibble: 6 × 3 ## .threshold specificity sensitivity ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -Inf 0 1 ## 2 1.79e-7 0 1 ## 3 4.50e-6 0.00413 1 ## 4 5.81e-6 0.00826 1 ## 5 5.92e-6 0.0124 1 ## 6 1.22e-5 0.0165 1 roc_auc(two_class_example, truth, Class1) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.939 autoplot(two_class_curve) Multi-class Finally we see data with three or more classes Accuracy: data(hpc_cv) hpc_cv%&gt;%head ## obs pred VF F M L Resample ## 1 VF VF 0.9136340 0.07786694 0.008479147 1.991225e-05 Fold01 ## 2 VF VF 0.9380672 0.05710623 0.004816447 1.011557e-05 Fold01 ## 3 VF VF 0.9473710 0.04946767 0.003156287 4.999849e-06 Fold01 ## 4 VF VF 0.9289077 0.06528949 0.005787179 1.564496e-05 Fold01 ## 5 VF VF 0.9418764 0.05430830 0.003808013 7.294581e-06 Fold01 ## 6 VF VF 0.9510978 0.04618223 0.002716177 3.841455e-06 Fold01 hpc_cv%&gt;% accuracy(obs, pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy multiclass 0.709 Matthews correlation coefficient: mcc(hpc_cv, obs, pred) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 mcc multiclass 0.515 And then the sensitivity calculation for different estimators: macro_sens &lt;-sensitivity(hpc_cv, obs, pred, estimator = &quot;macro&quot;); weigh_sens &lt;- sensitivity(hpc_cv, obs, pred, estimator = &quot;macro_weighted&quot;); micro_sens &lt;- sensitivity(hpc_cv, obs, pred, estimator = &quot;micro&quot;); sens &lt;- rbind(macro_sens,weigh_sens,micro_sens) sens ## # A tibble: 3 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 sensitivity macro 0.560 ## 2 sensitivity macro_weighted 0.709 ## 3 sensitivity micro 0.709 And the ROC curve: roc_auc(hpc_cv, obs, VF, F, M, L) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc hand_till 0.829 The ROC area: roc_auc(hpc_cv, obs, VF, F, M, L, estimator = &quot;macro_weighted&quot;) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc macro_weighted 0.868 The ROC visualization: https://www.tidymodels.org/start/resampling/ hpc_cv %&gt;% group_by(Resample) %&gt;% roc_curve(obs, VF, F, M, L) %&gt;% autoplot() 9.3.2 Conclusion Judging the effectiveness of a variety of different models and to choose between them, we need to consider how well these models behave through the use of some performance statistics: the area under the Receiver Operating Characteristic (ROC) curve, and overall classification accuracy. In conclusion when judging on a model effectiveness is important to follow few clear steps: check of the data used for the modeling if contains any of the hidden information, such as modification of the units second step is to calculate the ROC curve and the Area underneath the curve, plot it to see how it behaves on the model third is to apply some selected metrics such as RMSE or RSQ, MAE etc.. to evaluate the estimation values fourth make the confusion matrix as well as all the related metrics (sensitivity, specificity, accuracy, F1 …) finally apply again the ROC curve visualization on resampling to see the best fit 9.3.2.1 Resources yardstick: https://yardstick.tidymodels.org/ recipes: https://www.tidymodels.org/start/recipes/ ROC curve: https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/?utm_source=linkedin&amp;utm_medium=social&amp;utm_campaign=old-blog&amp;utm_content=B&amp;custom=LDV150 Decoding the confusion matrix: https://towardsdatascience.com/decoding-the-confusion-matrix-bb4801decbb "],["measures-of-model-fit---case-study-cohort-1.html", "9.4 Measures of Model Fit - Case Study (Cohort 1)", " 9.4 Measures of Model Fit - Case Study (Cohort 1) Empirical Validation: a quantitative approach for estimating effectiveness Focused on how close our predictions come to the observed data Optimization of statistical characteristics of the model does not imply that the model fits the data well Choice of which metrics to examine can be critical "],["disclaimers.html", "9.5 Disclaimers", " 9.5 Disclaimers These examples are to demonstrate metric evaluation not good data science! Explore the full set of metrics available through {yardstick} Talk through use cases for different metrics "],["regression-metrics.html", "9.6 Regression Metrics", " 9.6 Regression Metrics Load in the Data library(tidymodels) library(glmnet) library(ranger) set.seed(1123) data(ames) ames &lt;- ames %&gt;% mutate( under_budget = as.factor(if_else(Sale_Price&lt;=160000,1,0)), Sale_Price = log10(Sale_Price)) #Cross-fold validation ames_folds &lt;- vfold_cv(ames, v = 5) #Create Recipe ames_recipe &lt;- recipe(formula = Sale_Price ~ Gr_Liv_Area + Full_Bath + Half_Bath + Lot_Area + Neighborhood + Overall_Cond, data = ames) %&gt;% step_dummy(all_nominal()) #Set the model and hyperparameters ames_spec &lt;- linear_reg(penalty = tune(), mixture = tune()) %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;glmnet&quot;) #Create workflow ames_workflow &lt;- workflow() %&gt;% add_recipe(ames_recipe) %&gt;% add_model(ames_spec) #Create metric set of all regression metrics ames_tune &lt;- tune_grid( ames_workflow, metrics = metric_set(rmse, rsq, rsq_trad, mae, mpe, mape, smape, mase, ccc, rpiq, rpd, huber_loss, huber_loss_pseudo, iic), resamples = ames_folds, grid = grid_latin_hypercube(penalty(), mixture(), size = 8) ) #Pick the best model for each metric and pull out the predictions best_models &lt;- tibble( metric_name = c(&#39;rmse&#39;, &#39;rsq&#39;, &#39;rsq_trad&#39;, &#39;mae&#39;, &#39;mpe&#39;, &#39;mape&#39;, &#39;smape&#39;, &#39;mase&#39;, &#39;ccc&#39;,&#39;rpiq&#39;, &#39;rpd&#39;, &#39;huber_loss&#39;, &#39;huber_loss_pseudo&#39;, &#39;iic&#39;)) %&gt;% mutate(metric_best = map(metric_name, ~select_best(ames_tune, .x)), wf_best = map(metric_best, ~finalize_workflow(ames_workflow, .x)), fit_best = map(wf_best, ~fit(.x, data = ames)), df_pred = map(fit_best, ~ames %&gt;% bind_cols(predict(.x, new_data = ames)) %&gt;% select(Sale_Price, .pred))) %&gt;% select(-c(wf_best, fit_best)) %&gt;% unnest(cols = c(metric_name, metric_best, df_pred)) #Plot! best_models %&gt;% mutate(metric_desc = factor( metric_name, levels = c(&#39;rmse&#39;, &#39;rsq&#39;, &#39;rsq_trad&#39;, &#39;mae&#39;, &#39;mpe&#39;, &#39;mape&#39;, &#39;smape&#39;, &#39;mase&#39;, &#39;ccc&#39;,&#39;rpiq&#39;, &#39;rpd&#39;, &#39;huber_loss&#39;, &#39;huber_loss_pseudo&#39;, &#39;iic&#39;), labels = c(&#39;rmse\\nwhen you cannot afford\\n to have a big error&#39;, &#39;rsq\\nwhen you want a measure\\n of consistency/correlation\\n and not accuracy&#39;, &#39;rsq_trad\\n r-sq not constrained\\n between 0 and 1&#39;, &#39;mae\\nwhen large errors are not\\n exponentially\\n worse than small errors&#39;, &#39;mpe\\nwhen you want an easy way\\n to calculate accuracy&#39;, &#39;mape\\nwhen you want to use mpe\\n with a better\\n representation of error&#39;, &#39;smape\\nwhen you want to use\\n mape expressed as a %&#39;, &#39;mase\\nwhen you need a scale\\n independent metric\\n for time-series data&#39;, &#39;ccc\\nwhen you want to measure\\n the distance from \\nperferct linearity&#39;, &#39;rpiq\\nwhen you need a different\\n measue of consistency/correlation\\n and not accuracy&#39;, &#39;rpd\\nwhen you need a different\\n measue of consistency/correlation\\n and not accuracy&#39;, &#39;huber_loss\\nwhen you need a loss\\n function less sensitive to outliers&#39;, &#39;huber_loss_pseudo\\nwhen you need\\n a smoothed version of huber_loss&#39;, &#39;iic\\nwhen you need an\\n alternative to the traditional\\n correlation coefficient&#39;))) %&gt;% ggplot(aes(x = Sale_Price, y = .pred)) + geom_abline(lty = 2) + geom_point(alpha = 0.5) + labs(y = &quot;Predicted Sale Price (log10)&quot;, x = &quot;Sale Price (log10)&quot;) + coord_obs_pred() + facet_wrap(~metric_desc, ncol = 2) + theme_minimal() + theme(panel.spacing = unit(2, &quot;lines&quot;), strip.text.x = element_text(size = 8)) best_models %&gt;% select(metric_name, penalty, mixture) %&gt;% distinct() ## # A tibble: 14 × 3 ## metric_name penalty mixture ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 rmse 1.74e- 6 0.887 ## 2 rsq 6.71e-10 0.112 ## 3 rsq_trad 1.74e- 6 0.887 ## 4 mae 1.74e- 6 0.887 ## 5 mpe 1.66e- 1 0.163 ## 6 mape 1.74e- 6 0.887 ## 7 smape 1.74e- 6 0.887 ## 8 mase 1.74e- 6 0.887 ## 9 ccc 6.71e-10 0.112 ## 10 rpiq 6.71e-10 0.112 ## 11 rpd 6.71e-10 0.112 ## 12 huber_loss 1.74e- 6 0.887 ## 13 huber_loss_pseudo 1.74e- 6 0.887 ## 14 iic 3.81e- 4 0.552 "],["binary-classification-metrics.html", "9.7 Binary Classification Metrics", " 9.7 Binary Classification Metrics Note: This code might take several minutes (or longer) to run. #Cross-fold validation ames_folds_binary &lt;- vfold_cv(ames, v = 5) #Create Recipe ames_recipe_binary &lt;- recipe(formula = under_budget ~ Gr_Liv_Area + Full_Bath + Half_Bath + Lot_Area + Neighborhood + Overall_Cond, data = ames) #Set the model and hyperparameters ames_spec_binary &lt;- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;ranger&quot;) #Create workflow ames_workflow_binary &lt;- workflow() %&gt;% add_recipe(ames_recipe_binary) %&gt;% add_model(ames_spec_binary) #Create metric set of all binary metrics ames_tune_binary &lt;- tune_grid( ames_workflow_binary, metrics = metric_set(sens,spec,recall,precision,mcc,j_index,f_meas,accuracy, kap,ppv,npv,bal_accuracy,detection_prevalence), resamples = ames_folds_binary, grid = grid_regular( mtry(range = c(2, 6)), min_n(range = c(2, 20)), trees(range = c(10,100)), levels = 10 ) ) #Pick the best model for each metric and pull out the predictions best_models_binary &lt;- tibble( metric_name = c(&#39;recall&#39;,&#39;sens&#39;,&#39;spec&#39;, &#39;precision&#39;,&#39;mcc&#39;,&#39;j_index&#39;,&#39;f_meas&#39;,&#39;accuracy&#39;, &#39;kap&#39;,&#39;ppv&#39;,&#39;npv&#39;,&#39;bal_accuracy&#39;,&#39;detection_prevalence&#39;)) %&gt;% mutate(metric_best = map(metric_name, ~select_best(ames_tune_binary, .x)), wf_best = map(metric_best, ~finalize_workflow(ames_workflow_binary, .x)), fit_best = map(wf_best, ~fit(.x, data = ames)), df_pred = map(fit_best, ~ames %&gt;% bind_cols(predict(.x, new_data = ames)) %&gt;% select(under_budget, .pred_class))) %&gt;% select(-c(wf_best, fit_best)) %&gt;% unnest(cols = c(metric_name, metric_best, df_pred)) # Plot! best_models_binary %&gt;% mutate(metric_desc = factor( metric_name, levels = c(&#39;recall&#39;,&#39;sens&#39;,&#39;spec&#39;, &#39;precision&#39;,&#39;mcc&#39;,&#39;j_index&#39;,&#39;f_meas&#39;,&#39;accuracy&#39;, &#39;kap&#39;,&#39;ppv&#39;,&#39;npv&#39;,&#39;bal_accuracy&#39;,&#39;detection_prevalence&#39;), labels = c(&#39;recall\\nhow many observations out \\nof all positive observations \\nhave we classified as positive&#39;, &#39;sens\\nhow many observations out \\nof all positive observations \\nhave we classified as positive&#39;, &#39;spec\\nhow many observations out \\nof all negative observations \\nhave we classified as negative&#39;, &#39;precision\\nhow many observations \\npredicted as positive are \\nin fact positive&#39;, &#39;mcc\\ncorrelation between \\npredicted classes and ground truth&#39;, &#39;j_index\\nbalance between \\nsensitivity and specificity&#39;, &#39;f_meas\\nbalance between \\nprecision and recall&#39;, &#39;accuracy\\nhow many observations,\\n both positive and negative,\\n were correctly classified&#39;, &#39;kap\\nhow much better is your model\\n over the random classifier\\n that predicts based on class frequencies&#39;, &#39;ppv\\nhow many observations\\n predicted as positive\\n are in fact positive&#39;, &#39;npv\\nhow many predictions\\n out of all negative\\n predictions were correct&#39;, &#39;bal_accuracy\\nbalance between\\n sensitivity and specificity&#39;, &#39;detection_prevalence\\nhow many positive\\n predictions were correct of\\n all the predictions&#39;))) %&gt;% group_by(metric_desc, under_budget, .pred_class) %&gt;% summarise(bin_count = n()) %&gt;% ungroup() %&gt;% ggplot(aes(x = under_budget, y = .pred_class, fill = bin_count, label = bin_count)) + scale_fill_binned() + geom_tile() + geom_label() + coord_fixed() + facet_wrap(~metric_desc, ncol = 2) + theme_minimal() + theme(panel.spacing = unit(2, &quot;lines&quot;), strip.text.x = element_text(size = 8)) "],["references-1.html", "9.8 References", " 9.8 References Regression Classification Metrics Binary Classification Metrics Tidymodels Function Reference Custom Metrics "],["meeting-videos-9.html", "9.9 Meeting Videos", " 9.9 Meeting Videos 9.9.1 Cohort 1 Meeting chat log 00:14:27 Jonathan Trattner: Is there supposed to be sound? 00:16:42 Jonathan Trattner: I love how R’s parent S is up next 00:24:03 Asmae Toumi: Amazing 00:24:49 Scott Nestler: This whole chapter reminds me of a classic 2010 paper from my professor and friend Galit Shmueili, &quot;To Explain or to Predict?&quot; https://projecteuclid.org/journals/statistical-science/volume-25/issue-3/To-Explain-or-to-Predict/10.1214/10-STS330.full 00:25:48 Scott Nestler: Correction … her last name is Shmueli (had an extra &#39;i&#39; in there). 00:26:08 Andy Farina: This chapter reminded me of a quote I have heard numerous times from my advisor over the past few years…”All models are wrong, some are useful” 00:26:24 Jon Harmon (jonthegeek): Hehe, yup! 00:28:11 Ben Gramza: I seem to hear that George Box quote 10000000 times a year 00:28:42 Conor Tompkins: map() go brrrrr 00:30:25 Conor Tompkins: Would be cool to create a raster of the density of the points, and find the differences between models 00:30:45 Jon Harmon (jonthegeek): I&#39;m a little skeptical, we&#39;ll have to dig into that code! 00:31:30 Scott Nestler: Something doesn&#39;t seem right. Some of these metrics should *not* come up with the same results. 00:35:49 Conor Tompkins: Can you expect higher RMSE in general for higher priced homes? Ie $3 million. Is it better to use a percentage error term if that is the case? 00:54:24 Jonathan Trattner: Gotta head out now. Thanks Joe! Great job! 00:58:20 Asmae Toumi: Cool thanks 00:59:54 Tyler Grant Smith: like rock is correct pronunciation 01:02:10 Scott Nestler: A macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally), whereas a micro-average will aggregate the contributions of all classes to compute the average metric. 01:02:21 Scott Nestler: The weighted macro computes them independently, but weights them by number of observations, rather than equally. Usually better than regular macro when there are class imbalances. 01:06:47 Andy Farina: Excellent Joe, thank you! 9.9.2 Cohort 2 Meeting chat log 00:10:24 Amélie Gourdon-Kanhukamwe (she/they): https://www.meetup.com/tech-ethics-bristol/ 00:11:52 Janita Botha: it&#39;s a little small but ok 00:12:34 Janita Botha: that is perfect! 00:27:39 Janita Botha: sidewarren - but I really like the idea of a &quot;stupid&quot; model. What would be the equivalent in a classification model? 00:28:45 rahul bahadur: I think that would be simply assigning all predictions to one value out of the 2 00:55:51 Janita Botha: bye folks! 9.9.3 Cohort 3 Meeting chat log 00:13:11 Ildiko Czeller: my connection is unstable, i am here but without video for now 00:57:20 jiwan: https://community.rstudio.com/t/predict-a-class-using-a-threshold-different-than-the-0-5-default-with-tidymodels/56273 00:58:29 Toryn Schafer (she/her): https://www.rdocumentation.org/packages/probably/versions/0.0.6/topics/make_class_pred 00:59:06 Toryn Schafer (she/her): https://cran.r-project.org/web/packages/probably/vignettes/where-to-use.html 00:59:42 Ildiko Czeller: https://probably.tidymodels.org/reference/append_class_pred.html 01:01:41 Toryn Schafer (she/her): https://adv-r.hadley.nz/fp.html 9.9.4 Cohort 4 Meeting chat log 00:08:59 Isabella Velásquez: https://twitter.com/WeAreRLadies "],["review-of-chapters-4-9.html", "Review of chapters 4-9", " Review of chapters 4-9 For this week, Cohort 1 met with the authors, Max Kuhn and Julia Silge! "],["meeting-videos-10.html", "9.10 Meeting Videos", " 9.10 Meeting Videos 9.10.1 Cohort 1 Meeting chat log 00:04:25 Janita Botha: I am gatcrashing, if that&#39;s oke? 00:04:56 Jim Gruman: Welcome Janita 00:05:45 Jon Harmon (jonthegeek): Everyone is welcome! 00:05:54 Tan Ho: R song when 00:07:26 Conor Tompkins: We can’t risk not recording this one 00:14:21 Jon Harmon (jonthegeek): When you do a 5-fold CV on say a dataset of 100 elements (80 for testing and 20 for the estimation for each fold), do you create 5 sets of 20 elements and then just randomize the way you create the resampling CV datasets? 00:17:05 Conor Tompkins: Out of sample can mean different things for continuous vs. categorical variables 00:17:53 Julia Silge: https://applicable.tidymodels.org/ 00:19:56 Jordan Krogmann: stats::filter 00:19:59 Jordan Krogmann: for time series 00:20:53 Emil Hansen: check_range() might also be nice to check for range of new observations https://recipes.tidymodels.org/reference/check_range.html 00:21:42 Tony ElHabr: I thought we were about to get a max v Julia fight 00:21:47 Tony ElHabr: get your popcorn ready 00:26:34 Tyler Grant Smith: you might use transformed forms of the original set of predictors 00:27:00 arjun paudel: you could use portion of original predictors and pca components 00:30:07 Daniel Chen (he/him): that sounds like the python 2 -&gt; 3 transision xD 00:30:25 Jordan Krogmann: tidypredict?? 00:32:06 Conor Tompkins: shinyapps.io works for basic stuff 00:33:06 Jordan Krogmann: @julia/max how do you handle the pipeline aspect of feature transformations in a sql database 00:33:25 Tony ElHabr: maybe use GitHub actions? 00:33:35 Asmae Toumi: I will blog the hell out of my process when the time comes (soon!) 00:33:39 Jordan Krogmann: it is an extreme pain to do some transformations as a sql f unction for new data 00:33:41 Conor Tompkins: Max what is your dog’s name? 00:35:26 Asmae Toumi: shaaaaaade 00:35:45 Jonathan Leslie: We use AWS to host shiny apps inside ECS (elastic container service?). Happy to discuss with anyone off-line if you’re curious. 00:35:52 Asmae Toumi: Bye Julia!!!! Thank you!!!! 00:35:56 Conor Tompkins: Thanks! 00:35:58 Daniel Chen (he/him): bye Julia! 00:35:59 rahul bahadur: Thanks Julia 00:35:59 Jonathan Trattner: Thank you!! 00:36:08 Jordan Krogmann: lator tator 00:36:13 pavitra: thanks Julia! 00:36:19 Tyler Grant Smith: 42! 00:36:21 Daniel Chen (he/him): 42! 00:36:25 Tan Ho: It&#39;s not 42 00:36:25 Daniel Chen (he/him): :D 00:36:29 Tony ElHabr: 6669 00:36:31 Daniel Chen (he/him): 4242 00:36:41 Tan Ho: 834 I think? 00:36:45 Emil Hansen: I would not trust 2020! 00:37:05 Max Kuhn: https://twitter.com/TechRonic9876/status/1247743194038546432 00:37:40 Tan Ho: lolol I was in the area code 00:38:02 Jordan Krogmann: birth year 00:38:05 Jordan Krogmann: everytime 00:38:40 Tyler Grant Smith: but what do you seed your random seed generator with... 00:39:00 Jordan Krogmann: birth year number... inception 00:39:07 Conor Tompkins: @max what is the simplest feature engineering step that has given you the most performance gain? 00:39:43 Tan Ho: ^ bonus points for something aside from &quot;log transform everything&quot; 00:40:31 Jordan Krogmann: https://datachef.co/ 00:41:20 pavitra: normalize 00:41:47 Tyler Grant Smith: preprocess 00:41:49 Emil Hansen: My mental model is: recipe() -&gt; define prep() -&gt; estimate bake() -&gt; apply 00:42:58 Joe Sydlowski: Are the MARS models the only models with feature selection in tidymodels today? Do you have any concerns with using the feature selection in MARS when resampling? 00:43:15 pavitra: once I discovered workflow I was delighted to not mess with bake and prep 00:43:48 Jon Harmon (jonthegeek): Can you talk about the design process of metric sets as functionals, e.g. ames_metrics &lt;- metric_set(rmse, rsq, mae), then calling ames_metrics(...)? The idea of using functionals is interesting, but what are the actual practical advantages? 00:47:05 Tony ElHabr: it is quite elegant 00:49:26 Jon Harmon (jonthegeek): https://github.com/tidymodels/workflows/issues 00:53:19 Tyler Grant Smith: I like both modeltime and fable... 00:54:13 Jon Harmon (jonthegeek): Three questions really. Firstly what package system do you think works better for time series in a tidymodels workflow for time series, Modeltime, fable, or other? Second question, this is possibly just my experience, but I often feel time series is poorly taught/documented by comparison (excluding FPP) to other techniques. I was wondering how you might think this could be improved upon, and how tidy models system may contribute to better understanding and comparison? A third question (cheeky sorry) a lot of time-series focuses on model fit, however, sometime we want to investigate potential uplifts based on the presence or absence of features across multiple future forecasts. In which case causal inference can be a useful option, is this something we would be able to do in tidy models? 00:57:13 arjun paudel: is there ever a reason to normalize dummy variables? 00:57:37 Conor Tompkins: Kenneth can you mute 00:58:20 Tan Ho: there was a mars question 00:58:30 Tan Ho: &gt; Are the MARS models the only models with feature selection in tidymodels today? Do you have any concerns with using the feature selection in MARS when resampling? - Joe 00:59:06 Jordan Krogmann: I think workflows is the future 00:59:07 Tyler Grant Smith: is there a way to do mean (target) encoding in recipes? 01:00:35 Conor Tompkins: Is that like putting tune() inside a step_? 01:00:59 Jordan Krogmann: anything on deployment @#max 01:02:33 arjun paudel: I saw &quot;workflowsets&quot; mentioned in tidyAPM repo...is it making to CRAN anytime soon? 01:02:37 Tyler Grant Smith: @conor no, it wouldn&#39;t really be tuned. It is learned from the data 01:03:21 Tyler Grant Smith: :thumbsup: 01:04:22 Tyler Grant Smith: does workflowsets process in parallel? 01:05:36 Jordan Krogmann: thanks max! 01:05:37 Jim Gruman: Thank You!!! 01:05:39 Conor Tompkins: Thanks max! 01:05:40 Joe Sydlowski: Thanks Max! 01:05:41 Jonathan Trattner: Thank you so much!! 01:05:43 rahul bahadur: Thanks Max 01:05:47 Tan Ho: Thank you :D 01:05:50 Asmae Toumi: Thanks max!!!!!!!!!!!!!! 01:05:51 pavitra: this was amazing! thanks max and Julia! 01:05:55 Jonathan Leslie: Thanks, Max 01:05:56 arjun paudel: Thank you!! 01:05:56 Andy Farina: Thank you! "],["resampling-for-evaluating-performance.html", "Chapter 10 Resampling for evaluating performance", " Chapter 10 Resampling for evaluating performance Learning objectives: Recognize why naive performance estimates can often fail. Explain the difference between low bias models and high bias models. Use resampling to divide a training set into an analysis set and an assessment set. Use cross-validation to resample a training set. Compare repeated cross-validation, leave-one-out cross-validation, and Monte Carlo cross-validation. Divide a “not testing” set into a single training set and a single validation set. Use bootstrap resampling to divide a training set into an analysis set and an assessment set. Use rolling forecast origin resampling to divide a training set into an analysis set and an assessment set. Use resampling to estimate model performance. Use tune::fit_resamples() to fit multiple models for a resampled dataset. Use tune::collect_metrics() to measure model performance. Use tune::collect_predictions() to analyze model predictions. Use parallel processing to speed up resample fitting. Save model objects created during resampling. "],["why.html", "10.1 Why?", " 10.1 Why? As mentioned in chapter 5, we do not touch the test set until a candidate model(s) has been chosen. We need to test the performance of the candidate model(s). This can be done with resampling. "],["resubstitution-approach.html", "10.2 Resubstitution approach", " 10.2 Resubstitution approach In chapter 8, we fit a linear model to the training set. This is candidate model #1. library(tidymodels) data(ames) ames &lt;- mutate(ames, Sale_Price = log10(Sale_Price)) set.seed(123) ames_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) ames_rec &lt;- recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude + Longitude, data = ames_train) %&gt;% step_log(Gr_Liv_Area, base = 10) %&gt;% step_other(Neighborhood, threshold = 0.01) %&gt;% step_dummy(all_nominal()) %&gt;% step_interact( ~ Gr_Liv_Area:starts_with(&quot;Bldg_Type_&quot;) ) %&gt;% step_ns(Latitude, Longitude, deg_free = 20) lm_model &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) lm_wflow &lt;- workflow() %&gt;% add_model(lm_model) %&gt;% add_recipe(ames_rec) lm_fit &lt;- fit(lm_wflow, ames_train) We can fit a random forest model the training data, and this random forest model will be candidate #2. rf_model &lt;- rand_forest(trees = 1000) %&gt;% set_engine(&quot;ranger&quot;) %&gt;% set_mode(&quot;regression&quot;) rf_wflow &lt;- workflow() %&gt;% add_formula( Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude + Longitude) %&gt;% add_model(rf_model) rf_fit &lt;- rf_wflow %&gt;% fit(data = ames_train) We can compare the performance of these two candidate models by estimate_perf &lt;- function(model, dat) { # Capture the names of the objects used cl &lt;- match.call() obj_name &lt;- as.character(cl$model) data_name &lt;- as.character(cl$dat) data_name &lt;- gsub(&quot;ames_&quot;, &quot;&quot;, data_name) # Estimate these metrics: reg_metrics &lt;- metric_set(rmse, rsq) model %&gt;% predict(dat) %&gt;% bind_cols(dat %&gt;% select(Sale_Price)) %&gt;% reg_metrics(Sale_Price, .pred) %&gt;% select(-.estimator) %&gt;% mutate(object = obj_name, data = data_name) } Resubstitution errors for the linear model: estimate_perf(lm_fit, ames_train) ## # A tibble: 2 × 4 ## .metric .estimate object data ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 rmse 0.0751 lm_fit train ## 2 rsq 0.819 lm_fit train Resubstitution errors for the random forest model: estimate_perf(rf_fit, ames_train) ## # A tibble: 2 × 4 ## .metric .estimate object data ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 rmse 0.0367 rf_fit train ## 2 rsq 0.960 rf_fit train We can see that the random forest model performs significantly better (2+ times as better). We can choose the random forest model as our only candidate model. We can test its performance on the test set: estimate_perf(rf_fit, ames_test) ## # A tibble: 2 × 4 ## .metric .estimate object data ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 rmse 0.0694 rf_fit test ## 2 rsq 0.853 rf_fit test The random forest model, which previously performed well on the training set, is significantly worse on the test set. Just out of curiosity, we can see how the linear model performs on the test set: It’s interesting that the linear model performs similarly (bad) for the training and test sets, while the random forest did not. This is because linear models are considered high bias models and random forest models are low bias: In this context, bias is the difference between the true data pattern and the types of patterns that the model can emulate. Many black-box machine learning models have low bias. Other models (such as linear/logistic regression, discriminant analysis, and others) are not as adaptable and are considered high-bias models. — Max Kuhn and Julia Silge Re-predicting the training set is bad for performance evaluation. We need to use resampling methods. "],["resampling-methods.html", "10.3 Resampling methods", " 10.3 Resampling methods This diagram from the Hands-on machine learning in R from Boehmke &amp; Greenwell illustrates how resampling fits into the general modeling workflow: We can see that in resampling: - happens after the data split into training and test sets - some data is used for analyzing (analysis set), and some for evaluation (evaluation or assessment set, we’ll use assessment set from now on) - this is an iterative as it can be repeated many times - it only applies to the training data Effectively, - the model is fit with the analysis set - the model is evaluated with the evaluation set The overall, final performance evaluation for the model is the average of the performance metrics of the n evaluation sets. The way in which these analysis and evaluation sets are created define what the resampling techniques are specifically. We’ll go through the common ones: cross-validation repeated cross-validation leave-one-out cross-validation monte-carlo cross-validation 10.3.1 Cross-validation Cross-validation is one of the most popular resampling techniques. There are several flavors 10.3.1.1 V-fold cross-validation V-fold cross-validation involves the random splitting of the training data into approximately equal-sized “folds”. The diagram below illustrates v-fold cross validation with v set to 3 folds. We can see that the training data points have been randomly assigned to roughly equal-sized folds (in this case, exactly equal) and that the assessment set for each fold is 2/3 of the training data. Max and Julia note that while 3-fold CV is good to use for illustrative purposed, it is not good in practice - in practice, rather 5- or 10- fold CV is preferred. set.seed(55) ames_folds &lt;- vfold_cv(ames_train, v = 10) ames_folds ## # 10-fold cross-validation ## # A tibble: 10 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [2107/235]&gt; Fold01 ## 2 &lt;split [2107/235]&gt; Fold02 ## 3 &lt;split [2108/234]&gt; Fold03 ## 4 &lt;split [2108/234]&gt; Fold04 ## 5 &lt;split [2108/234]&gt; Fold05 ## 6 &lt;split [2108/234]&gt; Fold06 ## 7 &lt;split [2108/234]&gt; Fold07 ## 8 &lt;split [2108/234]&gt; Fold08 ## 9 &lt;split [2108/234]&gt; Fold09 ## 10 &lt;split [2108/234]&gt; Fold10 The output contains info on how the training data was split: ~2000 are in the analysis set, and ~220 are in the assessment set. You can recuperate these sets by calling analysis() or assessment(). In chapter 5, we introduced the idea of stratified sampling, which is almost always useful but particularly in cases with class imbalance. You can perform V-fold CV with stratified samplying by using the strata argument in the vfold_cv() call. 10.3.1.2 Repeated cross validation V-fold CV introduced above may produce noisy estimates. A technique that averages over more than V statistics may be more appropriate to reduce the noise. This technique is repeated cross-validation, where we create R repeats of V-fold CV. Instead of averaging over V statistics, we are now averaging over V x R statistics: vfold_cv(ames_train, v = 10, repeats = 5) ## # 10-fold cross-validation repeated 5 times ## # A tibble: 50 × 3 ## splits id id2 ## &lt;list&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &lt;split [2107/235]&gt; Repeat1 Fold01 ## 2 &lt;split [2107/235]&gt; Repeat1 Fold02 ## 3 &lt;split [2108/234]&gt; Repeat1 Fold03 ## 4 &lt;split [2108/234]&gt; Repeat1 Fold04 ## 5 &lt;split [2108/234]&gt; Repeat1 Fold05 ## 6 &lt;split [2108/234]&gt; Repeat1 Fold06 ## 7 &lt;split [2108/234]&gt; Repeat1 Fold07 ## 8 &lt;split [2108/234]&gt; Repeat1 Fold08 ## 9 &lt;split [2108/234]&gt; Repeat1 Fold09 ## 10 &lt;split [2108/234]&gt; Repeat1 Fold10 ## # ℹ 40 more rows 10.3.1.3 Leave-one-out cross validation M &amp; J say it sucks, so I’m skipping this lol. 10.3.1.4 Monte Carlo cross validation (MCCV) It’s like V-fold CV in the sense that training data is allocated to the assessment set with some fixed proportion. The difference is the resampling objects generated by MCCV are not mutually exclusive as the same data points can appear in the assessment set multiple times. mc_cv(ames_train, prop = 9/10, times = 20) ## # Monte Carlo cross-validation (0.9/0.1) with 20 resamples ## # A tibble: 20 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [2107/235]&gt; Resample01 ## 2 &lt;split [2107/235]&gt; Resample02 ## 3 &lt;split [2107/235]&gt; Resample03 ## 4 &lt;split [2107/235]&gt; Resample04 ## 5 &lt;split [2107/235]&gt; Resample05 ## 6 &lt;split [2107/235]&gt; Resample06 ## 7 &lt;split [2107/235]&gt; Resample07 ## 8 &lt;split [2107/235]&gt; Resample08 ## 9 &lt;split [2107/235]&gt; Resample09 ## 10 &lt;split [2107/235]&gt; Resample10 ## 11 &lt;split [2107/235]&gt; Resample11 ## 12 &lt;split [2107/235]&gt; Resample12 ## 13 &lt;split [2107/235]&gt; Resample13 ## 14 &lt;split [2107/235]&gt; Resample14 ## 15 &lt;split [2107/235]&gt; Resample15 ## 16 &lt;split [2107/235]&gt; Resample16 ## 17 &lt;split [2107/235]&gt; Resample17 ## 18 &lt;split [2107/235]&gt; Resample18 ## 19 &lt;split [2107/235]&gt; Resample19 ## 20 &lt;split [2107/235]&gt; Resample20 10.3.2 Validation sets Another way you can assess the performance of your candidate model(s) - before moving forward to the test set - is to use a validation set. This might be an attractive option if you have big data. As the diagram below shows, the validation set is independent of the training data. You can create your validation set by calling on validation_split() and setting the proportion desired. There is also a strata argument to conduct stratified sampling. set.seed(12) val_set &lt;- validation_split(ames_train, prop = 3/4) val_set ## # Validation Set Split (0.75/0.25) ## # A tibble: 1 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [1756/586]&gt; validation 10.3.3 Boostrapping Bootstrapping can be used to estimate model performance. It’s good to be aware that while it does produce lower variance compared to other resampling methods, it has “significant pessimistic bias”. It also works differently than other resampling methods. In the diagram below, we can see that the analysis set is always equal to the size of the whole training set, and we can also see that the same points can be selected multiple times. The assessment sets contain the data points that were not previously included in the analysis sets. Furthermore, these assessment sets are not of the same size, as we’re about to see when we call bootstraps(). Operationally, performing bootstrap resampling involves specifying the number of bootstrap samples via the times argument. There is also a strata argument for conducting stratified sampling. bootstraps(ames_train, times = 5) ## # Bootstrap sampling ## # A tibble: 5 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [2342/830]&gt; Bootstrap1 ## 2 &lt;split [2342/867]&gt; Bootstrap2 ## 3 &lt;split [2342/856]&gt; Bootstrap3 ## 4 &lt;split [2342/849]&gt; Bootstrap4 ## 5 &lt;split [2342/888]&gt; Bootstrap5 10.3.4 Rolling forecasting origin resampling Resampling with time series data needs a special setup as random sampling can ignore important trends such as seasonality. Rolling forecast resampling involves specifying the size of the analysis and assessment sets, and each iteration after the first one skips by a set number as the diagram illustrates below (with a skip of 1 as an example): This time series resampling is done with rolling_origin. You can specify the number of samples to be used for analysis with initial, the number of samples used for each assessment resample with assess, and cumulative set to true if you want the analysis resample to grow beyong the size specified with initial. Additional arguments include the skip and lag. rolling_origin(data, initial = 5, assess = 1, cumulative = TRUE, skip = 0, lag = 0) "],["estimating-performance.html", "10.4 Estimating performance", " 10.4 Estimating performance To recap, the resampling methods above estimate overall model performance using the predictions from the assessment sets. The {tune} package (included in tidymodels package) contains a function called fit_resamples (which is akin to fit()) that computes a set of performance metrics across resamples (or just one, as is the case with a validation set). The call requires either a parsnip model specification or a workflows::workflow, and rset object (as created with rsample::vfold_cv for example). You can also specify the performance metrics you want with the metrics argument or stick with the defaults. The control argument can be used to view/retain/save outputs if further tuning is desired. Your call might look like: rf_res &lt;- rf_wflow %&gt;% fit_resamples(resamples = ames_folds, control = keep_pred) The output (not viewed here because it’s thicc) can be manipulated in a number of ways to view just what you need. You can for example run tune::collect_metrics(rf_res) to see just the performance metrics. For more on how the outputs can be used for diagnostics and further model evaluation refer to section 10.3 in the book. "],["parallel-processing.html", "10.5 Parallel processing", " 10.5 Parallel processing Parallel processing can speed up the computations done by the tune package (introduced in the last section via fit_resamples and collect_metrics). Speed can be improved when processing uses fewer than the number of physical cores. To enable this, parallel backend R packages need to be configured. These packages and operating system specific instructions are outlined in section 10.4. "],["saving-the-resampled-objects.html", "10.6 Saving the resampled objects", " 10.6 Saving the resampled objects By default, the models created during resampling are not saved since most users are interested in the performance metrics that they generate. But, sometimes, you might realize that a specific model is the better suited one and so you need fit again to the whole training set. There is a method of saving the models created during resampling. You can save them whole or just certain components. In the book, M &amp; J walk us through a linear regression model example where they save the model results. "],["meeting-videos-11.html", "10.7 Meeting Videos", " 10.7 Meeting Videos 10.7.1 Cohort 1 Meeting chat log 00:10:27 pavitra: hahaha..look at Jon resampling his wine 00:10:38 Jonathan Trattner: HAH 00:10:44 pavitra: 1-fold CV 00:11:16 pavitra: that&#39;s bad..jon&#39;s test and train sets are the same 00:11:36 pavitra: HAHAHA... 00:12:05 pavitra: Let me grab my IPA. Tan, get your Milk stout 00:12:30 Tan Ho: bahaha bottoms up! 00:13:23 Tan Ho: *awkward eyes meme* 00:13:30 pavitra: lmao 00:30:59 Tyler Grant Smith: i feel like this deserves experimental support to compare 50 fold cv vs 5fold x10 00:34:35 pavitra: gotcha… 00:42:34 pavitra: is this bootstrapping with replacement? 00:43:42 Jon Harmon (jonthegeek): 63.2% 36.8% 00:43:54 pavitra: 2 SDs 00:44:34 Conor Tompkins: @max why is it those numbers 00:45:24 Tan Ho: (or just doubleclick the function lol) 00:45:42 Jonathan Trattner: Both of those are GAME changers 00:45:43 Tyler Grant Smith: ive lost the ink on my f2 key i use it so much 00:45:46 Jonathan Trattner: wowowow 00:45:49 Tan Ho: er, wait, ctrl-click? 00:45:51 pavitra: yeah - I just discovered &quot;go to function definition&quot; in the code menu myself 00:46:31 Jonathan Trattner: Productivity boost 🤯 00:46:42 Tan Ho: it&#39;s a trap! - admiral ackbar 00:46:52 Tan Ho: (you can end up digging too deep) 00:47:16 pavitra: tyler is an expert at time series 00:54:03 Tyler Grant Smith: this seems to be contrary to how hyndman defines it 00:58:01 pavitra: isn&#39;t there a way to dynamically tune your hyperparameters such that it can do it for the defined range and you can see the performance using the control? 00:59:14 Jim Gruman: time series resampling at the tidymodels web site covers `sliding_window()` `sliding_index()` and `sliding_period()` functions. Maybe the book 10.2.4 rolling forecasting could be a little more clear about all three methods 00:59:31 Tyler Grant Smith: reading the documentation for rolling_origin i think that graphic represents cumulative=FALSE not TRUE. This agrees with the books code 01:06:24 Jon Harmon (jonthegeek): lm_res$.extracts[[1]][[1]] 01:07:34 Tyler Grant Smith: may not be tidy but thats code im used to! 01:09:44 Jonathan Trattner: Go, Jon, go! 01:10:44 Jim Gruman: thank you Jon!!!!! 01:10:52 Tyler Grant Smith: step_interact( ~ Gr_Liv_Area:starts_with(&quot;Bldg_Type_&quot;)) neat i didnt know you could do that 01:12:29 Tyler Grant Smith: my models are all organic non-gmo 01:17:36 Joe Sydlowski: For Conor&#39;s question: http://uc-r.github.io/mars 01:17:47 Jonathan Trattner: Thanks, Jon! Have a great night, everyone! 01:18:14 Joe Sydlowski: There&#39;s a plot with model selection for the autoplot in MARs that shows the metrics 01:18:19 pavitra: thanks Jon 01:18:19 Conor Tompkins: Thanks Jon! 10.7.2 Cohort 2 Meeting chat log 00:03:08 Roberto Villegas-Diaz: I’m using my 4G today, but it’s running really slow, so I won’t turn on my video! 00:03:29 Stephen Holsenbeck: ok 👍 00:22:57 August: FYI this is where the idea comes from: https://otexts.com/fpp3/tscv.html 00:29:12 August: https://robjhyndman.com/papers/cv-wp.pdf 00:35:03 rahul bahadur: http://appliedpredictivemodeling.com/blog/2014/11/27/vpuig01pqbklmi72b8lcl3ij5hj2qm 00:35:38 Amélie Gourdon-Kanhukamwe (she/they): Thanks 10.7.3 Cohort 3 Meeting chat log 00:21:50 Ildiko Czeller: https://stats.stackexchange.com/a/386608 about why boostrap resample is pessimistic 10.7.4 Cohort 4 Meeting chat log 00:48:26 Federica Gazzelloni: laura 00:50:36 Federica Gazzelloni: Hi @priyanka! 00:51:44 priyanka gagneja: Hi Federica, I was interested in topic particularly.. so I decided to jump in :) for one off calls ..hope that&#39;s ok 00:51:52 Federica Gazzelloni: thanks! 00:52:08 Federica Gazzelloni: sure of course! 00:52:38 Ryan Metcalf: Rolling Forecasting would be similar to Weather Prediction, correct? Using time series data. I’m imagining the radar prediction into the future. 00:52:50 Federica Gazzelloni: yep "],["comparing-models-with-resampling.html", "Chapter 11 Comparing models with resampling", " Chapter 11 Comparing models with resampling Learning objectives: Calculate performance statistics for multiple models. Recognize that within-resample correlation can impact model comparison. Define practical effect size. Compare models using differences in metrics. Use {tidyposterior} to compare models using Bayesian methods. "],["calculate-performance-statistics.html", "11.1 Calculate performance statistics", " 11.1 Calculate performance statistics my_cool_model_rsq &lt;- my_cool_model %&gt;% collect_metrics(summarize = FALSE) %&gt;% filter(.metric == &quot;rsq&quot;) %&gt;% select(id, my_cool_model = .estimate) ## Repeat that for more models, then: rsq_estimates &lt;- my_cool_model_rsq %&gt;% inner_join(my_other_model_rsq) %&gt;% inner_join(my_other_other_model_rsq) "],["calculate-performance-statistics-workflowsets.html", "11.2 Calculate performance statistics: {workflowsets}", " 11.2 Calculate performance statistics: {workflowsets} We’ll take a closer look at this, but workflowsets makes this stuff way cleaner! lm_models &lt;- workflowsets::workflow_set( preproc = list( basic = basic_recipe, interact = interaction_recipe, splines = spline_recipe ), models = list(lm = lm_model), cross = FALSE ) %&gt;% workflowsets::workflow_map( fn = &quot;fit_resamples&quot;, # Options to `workflow_map()`: seed = 1101, verbose = TRUE, # Options to `fit_resamples()`: resamples = ames_folds, control = keep_pred ) collect_metrics(lm_models) %&gt;% filter(.metric == &quot;rsq&quot;) "],["within-resample-correlation.html", "11.3 Within-resample correlation", " 11.3 Within-resample correlation Within-resample correlation: some folds are easier to predict than others Comparison of R^2 between models “If the resample-to-resample effect was not real, there would not be any parallel lines.” - Max Kuhn &amp; Julia Silge ie, the lines don’t cross that much, so there’s an effect. "],["practical-effect-size.html", "11.4 Practical effect size", " 11.4 Practical effect size It’s a good idea to think about how big of a difference matters to you. Maybe a change will be statistically significant, but is it worth the trouble of deploying a new model? "],["simple-comparison.html", "11.5 Simple Comparison", " 11.5 Simple Comparison Use difference to cancel out the resample-to-resample effect. compare_lm &lt;- rsq_estimates %&gt;% mutate(difference = `with splines` - `no splines`) lm(difference ~ 1, data = compare_lm) %&gt;% tidy(conf.int = TRUE) %&gt;% select(estimate, p.value, starts_with(&quot;conf&quot;)) "],["bayesian-methods.html", "11.6 Bayesian methods", " 11.6 Bayesian methods library(tidyposterior) library(rstanarm) rqs_diff &lt;- ames_folds %&gt;% bind_cols(rsq_estimates %&gt;% arrange(id) %&gt;% select(-id)) %&gt;% perf_mod( prior_intercept = student_t(df = 1), chains = 4, iter = 5000, seed = 2 ) %&gt;% contrast_models( list_1 = &quot;with splines&quot;, list_2 = &quot;no splines&quot;, seed = 36 ) summary(rqs_diff, size = 0.02) %&gt;% # 0.02 is our practical effect size. select(contrast, starts_with(&quot;pract&quot;)) #&gt; # A tibble: 1 x 4 #&gt; contrast pract_neg pract_equiv pract_pos #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 with splines vs no splines 0 0.989 0.0113 "],["meeting-videos-12.html", "11.7 Meeting Videos", " 11.7 Meeting Videos 11.7.1 Cohort 1 Meeting chat log 00:14:48 Tony ElHabr: seed = 1101 00:14:52 Tony ElHabr: what a hipster 00:15:11 pavitra: I see a subliminal binary message 00:17:41 Tony ElHabr: 1101 -&gt; D in hex 00:18:08 pavitra: D for dark magicks 00:39:59 Jonathan Leslie: I’m heading off. Thanks, Jon…really nice presentation! 00:45:54 Jim Gruman: thank you Jon!!! 00:47:45 Andy Farina: Thank you Jon, great presentation and addition of workflow sets 11.7.2 Cohort 2 Meeting chat log 00:08:57 Janita Botha: I have problems with physical knitting too... :) 00:10:18 Roberto Villegas-Diaz: XSEDE 00:13:15 rahul bahadur: Anyone works with Spark here? - SparkR/sparklyr? 00:22:38 Luke Shaw: no sorry, have used pyspark before so have some spark understanding though 01:04:05 Amélie Gourdon-Kanhukamwe (she/they): I have another call this week, gonna dash 01:04:18 Stephen Holsenbeck: ok, thanks for coming! 01:04:24 Janita Botha: bye! 01:04:28 Luke Shaw: Bye :) 01:13:19 Janita Botha: cool! :) 01:14:37 Janita Botha: I have to run! See you folks next week! 01:14:55 Stephen Holsenbeck: Bye Janita, have a good Monday! 11.7.3 Cohort 3 Meeting chat log 00:12:38 Daniel Chen: it&#39;s essentially doing the multiple recipes and collecting the model metrics for you across all your preprocessing steps/models 00:12:40 Daniel Chen: ? 00:14:29 Daniel Chen: fn The function to run. Acceptable values are: tune::tune_grid(), tune::tune_bayes(), tune::fit_resamples(), finetune::tune_race_anova(), finetune::tune_race_win_loss(), or finetune::tune_sim_anneal(). 00:15:00 Daniel Chen: seems like there&#39;s only a few functions that are availiable to be used 00:16:36 Daniel Chen: but they&#39;re using the string instead of quoted form because they&#39;re matching on string to see which functions are allowed: https://github.com/tidymodels/workflowsets/blob/main/R/workflow_map.R#L101 00:16:53 Ildiko Czeller: makes sense, thanks 00:16:55 Toryn Schafer (she/her): Thanks, Daniel! 00:32:13 Daniel Chen: i guess they&#39;re using tidyposterior, instead of tidymodels. so i guess that&#39;s what&#39;s adding to the confusion 00:35:06 Daniel Chen: cross A logical: should all combinations of the preprocessors and models be used to create the workflows? If FALSE, the length of preproc and models should be equal. 00:49:17 jiwan: tune_grid( object, preprocessor, resamples, ..., param_info = NULL, grid = 10, metrics = NULL, control = control_grid() ) 00:50:04 Daniel Chen: https://tune.tidymodels.org/reference/tune_grid.html 00:50:52 jiwan: A data frame of tuning combinations or a positive integer. The data frame should have columns for each parameter being tuned and rows for tuning parameter candidates. An integer denotes the number of candidate parameter sets to be created automatically 11.7.4 Cohort 4 "],["model-tuning-and-the-dangers-of-overfitting.html", "Chapter 12 Model tuning and the dangers of overfitting", " Chapter 12 Model tuning and the dangers of overfitting Learning objectives: Recognize examples of tuning parameters. Recognize hyperparameters for machine learning models. Recognize tuning parameters for preprocessing techniques. Recognize structural parameters for classical statistical models. Recognize examples of parameters that should not be tuned. Explain how different metrics can lead to different decisions about the choice of tuning parameter values. Explain how poor parameter estimates can lead to overfitting of training data. Recognize strategies for optimizing tuning parameters. Compare and contrast grid search and iterative search. Use tune::tune() and the {dials} package to optimize tuning parameters. "],["what-is-a-tuning-parameter.html", "12.1 What is a Tuning Parameter?", " 12.1 What is a Tuning Parameter? An unknown structural or other kind of value that has significant impact on the model but cannot be directly estimated from these data 12.1.1 Examples Machine Learning (hyperparameters) Boosting: number of boosting iterations ANN: number of hidden units and type of activation function Modern Gradient Descent: Learning rates, momentus, and iterations Random Forest: number of predictors, number of trees, number of data points Preprocessing (tuning parameters) PCA: number of extracted components Imputation (uses KNN): number of neighbors Statistical Models (structural parameters) Binary Regression (logistic regression): probit, logit link Longitudinal Models: correlation and covariance structure of the data "],["when-not-to-tune.html", "12.2 When not to tune", " 12.2 When not to tune Prior distribution (Bayesian analysis) Number of Trees (Random Forest and Bagging) Does not need tuning–instead focus on stability "],["decisions-decisions.html", "12.3 Decisions, Decisions…", " 12.3 Decisions, Decisions… data(ames, package = &quot;modeldata&quot;) ames &lt;- ames %&gt;% mutate(Sale_Price = log10(Sale_Price)) set.seed(63) ames_split &lt;- initial_split(ames, prop = 0.80, strata = Central_Air) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) set.seed(63) rs &lt;- vfold_cv(ames_train, repeats = 10) "],["what-metric-should-we-use.html", "12.4 What Metric Should We Use?", " 12.4 What Metric Should We Use? Which link function should we use…does it matter? logistic regression using a logit probit complementary log-log Table 12.1: Likelihood Statistics logLik link -380.0873 logit -381.7535 probit -389.0645 c-log-log If we just look at the log-likelihood statistic, the logistic link function appears to be statistically (significantly) better than the probit and complementary log-log link functions. However, if we use the area under the ROC curve, we see that there is no significant difference between the three link functions. When we plot the three link functions, we also see that they are not substantially different in predicting whether a house has central air. "],["can-we-make-our-model-too-good.html", "12.5 Can we make our model too good?", " 12.5 Can we make our model too good? Overfitting is always a concern as we start to tune hyperparameters. tip from the book: Using out of sample data is the solution for detecting when a model is overemphasizing the training set graphs depicting what overfitting looks like Image Credit (https://therbootcamp.github.io/ML_2019Oct/_sessions/Recap/Recap.html#8) "],["tuning-parameter-optimization-strategies.html", "12.6 Tuning Parameter Optimization Strategies", " 12.6 Tuning Parameter Optimization Strategies Grid Search (Space Filled Grid) Grid Search Graphic Random Search (Global Search) Random Search Graphic Iterative Search (Global Search) Iterative Search Graphic image credits: (https://en.wikipedia.org/wiki/Hyperparameter_optimization) "],["tuning-parameters-in-tidymodels-dials.html", "12.7 Tuning Parameters in tidymodels {dials}", " 12.7 Tuning Parameters in tidymodels {dials} Parsnip Model Specifications Main Arguments (rand_forest) engine-specific (ranger) Good starting points (tidymodels website): reference docs searchable table "],["lets-try-an-example.html", "12.8 Let’s try an example:", " 12.8 Let’s try an example: See if we can predict the home sale price in our ames dataset Start with a recipe ames_recipe &lt;- recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Remod_Add + Bldg_Type, data = ames_train) "],["build-our-random-forest-model.html", "12.9 Build our random forest model:", " 12.9 Build our random forest model: random forest model rf_spec &lt;- rand_forest() %&gt;% set_engine(&quot;ranger&quot;) %&gt;% set_mode(&quot;regression&quot;) Main Arguments args(): args(rand_forest) ## function (mode = &quot;unknown&quot;, engine = &quot;ranger&quot;, mtry = NULL, trees = NULL, ## min_n = NULL) ## NULL engine specific arguments: ?ranger::ranger() "],["add-tuning-parameters.html", "12.10 Add tuning parameters:", " 12.10 Add tuning parameters: We can add main arguments (mtry, min_n) and engine specific arguments (regularization.factor) rf_spec_tuned &lt;- rand_forest(mtry = tune(), trees = 2000, min_n = tune()) %&gt;% set_engine(&quot;ranger&quot;, regularization.factor = tune(&quot;reg&quot;)) %&gt;% set_mode(&quot;regression&quot;) tune() returns an expression. This tags the parameters for optimization within the tidymodels framework rf_spec_tuned %&gt;% extract_parameter_set_dials() ## Collection of 3 parameters for tuning ## ## identifier type object ## mtry mtry nparam[?] ## min_n min_n nparam[+] ## reg regularization.factor nparam[+] ## ## Model parameters needing finalization: ## # Randomly Selected Predictors (&#39;mtry&#39;) ## ## See `?dials::finalize` or `?dials::update.parameters` for more information. The notation nparam[+] indicates a complete numeric parameter, nparam[?] indicates a missing value that needs to be addressed. "],["updating-tuning-parameters.html", "12.11 Updating tuning parameters:", " 12.11 Updating tuning parameters: To see what we need to update/ finalize, we can call the function in the {dials} package mtry() ## # Randomly Selected Predictors (quantitative) ## Range: [1, ?] We can also use the {dials} package to see the tuning range min_n() ## Minimal Node Size (quantitative) ## Range: [2, 40] To update/finalize or adjust the hyperparameters we can use the update() function to update in-place: rf_spec_tuned %&gt;% extract_parameter_set_dials() %&gt;% update(mtry = mtry(c(1, 4))) ## Collection of 3 parameters for tuning ## ## identifier type object ## mtry mtry nparam[+] ## min_n min_n nparam[+] ## reg regularization.factor nparam[+] We see that mtry is now a complete numeric parameter "],["finalizing-tuning-parameters.html", "12.12 Finalizing tuning parameters:", " 12.12 Finalizing tuning parameters: The update function may not be useful if a recipe is attached to a workflow that adjusts the number of columns. Instead of update() we can use the finalize() function. updated_params &lt;- workflow() %&gt;% add_model(rf_spec_tuned) %&gt;% add_recipe(ames_recipe) %&gt;% extract_parameter_set_dials() %&gt;% finalize(ames_train) updated_params ## Collection of 3 parameters for tuning ## ## identifier type object ## mtry mtry nparam[+] ## min_n min_n nparam[+] ## reg regularization.factor nparam[+] With the finalize() function, mtry was completed based on the number of predictors in the training dataset updated_params %&gt;% extract_parameter_dials(&quot;mtry&quot;) ## # Randomly Selected Predictors (quantitative) ## Range: [1, 74] "],["what-is-next.html", "12.13 What is next?", " 12.13 What is next? The parameter object we just explored knows the range of the parameters. the {dials} package contains a number of grid_*() functions that takes the parameter object as input to produce different types of grids. Chapter 13 will explore this further. "],["meeting-videos-13.html", "12.14 Meeting Videos", " 12.14 Meeting Videos 12.14.1 Cohort 1 Meeting chat log 00:11:27 Jon Harmon (jonthegeek): usethis::use_package(&quot;patchwork&quot;) 00:23:12 Jon Harmon (jonthegeek): @jim Which chapter are you going to present: 13? 14? 15? 😊 00:27:52 Jim Gruman: :) sure, 13 sounds good 00:29:42 Jon Harmon (jonthegeek): Woot! 00:29:57 Jordan Krogmann: 1, 1000, 5000 is the correct answer 00:31:11 Jon Harmon (jonthegeek): Ok, Jim! You&#39;re PRed into the README, no take-backsies! 00:33:44 Tan Ho: fixed sorry 00:35:29 Tony ElHabr: i&#39;m sorry but are these python generated graphs? 00:35:33 Tony ElHabr: unforgiveable 00:36:06 Tan Ho: gg 00:36:12 Tan Ho: i&#39;m out 00:36:54 Jon Harmon (jonthegeek): Ok, after Jim everyone who&#39;s here will have gone at least once! Except wait Tan never actually went did he????? 00:37:45 Tony ElHabr: CALL HIM OUT 00:38:03 Jon Harmon (jonthegeek): TAN WHAT CHAPTER ARE YOU PRESENTING??? 14, right? 00:39:01 Tan Ho: *awkwardly looking around after going MIA from the book club since chapter 9* 00:39:43 Tony ElHabr: as long as tan gives us a killer review at the end of this all, then i&#39;ll give him a pass 00:40:03 Jon Harmon (jonthegeek): Fair! Tan *is* the master of the review week! 00:40:21 Jon Harmon (jonthegeek): I pinged Max on the Slack to see if we should do a review after 14, or after 15, or later, or what. 00:40:55 Tony ElHabr: how can we do a proper review if we never stop learning? 00:41:14 Tan Ho: I might be more apt to do 15, I think? 00:41:23 Jon Harmon (jonthegeek): Yeah, this one is tricky &#39;cuz I&#39;m not sure when it ends 🙃 00:41:53 Tony ElHabr: tan picks chapter 15 so he can have 3 weeks to prep :P 00:42:02 Tan Ho: two weeks to catch up and one week to prep! 00:42:21 Tan Ho: okay I lied twenty days to catch up and one day to prep, let&#39;s be honest 00:42:46 Tony ElHabr: 21 days to catch up cuz you&#39;re going to live code 00:42:51 Tan Ho: fair! 00:42:56 Tan Ho: that part don&#39;t scare me 00:54:23 Jordan Krogmann: Thanks andy! 00:54:23 Tan Ho: if 16 isn&#39;t written by the time I present we can probably just review lol 12.14.2 Cohort 2 Meeting chat log 00:13:12 Luke Shaw: sounds cool :) 00:13:26 Kevin Kent: For sure! 00:34:14 August: https://www.tidymodels.org/learn/develop/models/ 00:34:21 August: build your own engine] 00:34:40 August: that&#39;s what the link is sorry. 00:39:39 Amélie Gourdon-Kanhukamwe (she/they): Hear hear! 00:43:23 August: I&#39;ve added it to the slack group 00:43:36 August: come learn bayes!!! 00:43:47 Kevin Kent: Yeah I’m trying to convert :) 00:54:30 Kevin Kent: https://matrixprofile.org/ 00:55:53 August: found it! 00:55:55 August: https://www.tidymodels.org/learn/develop/recipes/ 00:56:48 Stephen Holsenbeck: Good to see you too, thank you! 00:56:49 Luke Shaw: thanks Janita - see you all next week :) 00:56:51 Amélie Gourdon-Kanhukamwe (she/they): Sorry all, been working all night and I am knackered, but thanks Janita! 12.14.3 Cohort 3 Meeting chat log 00:36:03 Federica : finalize() with {dial} take a parameter object and modify the unknown parts 00:44:40 Daniel Chen (TA), NYC: https://dials.tidymodels.org/ 00:49:04 jiwan: i think you&#39;re muted, couldn&#39;t hear you 00:52:24 Federica : I was wondering about the regularization_factor() 00:54:21 jiwan: These parameters are auxiliary to random forest models that use the &quot;ranger&quot; engine. They correspond to tuning parameters that would be specified using set_engine(&quot;ranger&quot;, ...). 00:59:27 jiwan: dials::mtry() # Randomly Selected Predictors (quantitative) Range: [1, ?] 01:00:29 jiwan: regularization.factor = tune(&quot;reg&quot;) 01:00:42 jiwan: regularization.factor = tune() 01:03:32 jiwan: &gt; dials::regularization_factor function (range = c(0, 1), trans = NULL) { new_quant_param(type = &quot;double&quot;, range = range, inclusive = c(TRUE, TRUE), trans = trans, default = 1, label = c(regularization_factor = &quot;Gain Penalization&quot;), finalize = NULL) } 12.14.4 Cohort 4 "],["grid-search.html", "Chapter 13 Grid search", " Chapter 13 Grid search Learning objectives: Use the {dials} package to create tuning grids. Compare and contrast regular and non-regular grids. Use dials::parameters() to examine tuning parameters. Use dials::grid_regular() to create a regular tuning grid. Use dials::grid_*random*() functions to create irregular tuning grids. Use tune::tune_grid() to conduct a grid search. Use the grid parameter to specify tuning grids. Finalize a tuned model. Use tune::select_*() functions to choose a tuned parameter set. Manually specify a parameter set. Improve grid search efficiency. Recognize how {parsnip} uses submodel optimization to make tuning more efficient. Specify {tune} parallel-processing rules using the parallel_over parameter. Use finetune::tune_race_anova() to make tuning more efficient via racing methods. The call to action: Tuning machine learning models can be time consuming and computationally expensive. Thoughtful choices in the experimental design of searches can make them easier to deal with. Last week: how to tag arguments using tune(). This week: how to optimize the parameters, a priori. Next week: iterative methods. "],["regular-and-non-regular-grids.html", "13.1 Regular and non-regular grids", " 13.1 Regular and non-regular grids Let’s consider an example model: an mlp neural network model. The parameters marked for tuning are: the number of hidden units, the number of fitting epochs in model training, and the amount of weight decay penalization. Using parsnip, the specification for a regression model fit using the nnet package for a multi layer perceptron is: mlp_spec &lt;- mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %&gt;% set_engine(&quot;nnet&quot;, trace = 0) %&gt;% set_mode(&quot;regression&quot;) The argument trace = 0 prevents extra logging of the training process. The parameters() function can extract the set of arguments with unknown values and set their dials objects. extract_parameter_dials() gives the current range of values. mlp_param &lt;- parameters(mlp_spec) mlp_param %&gt;% extract_parameter_dials(&quot;hidden_units&quot;) ## # Hidden Units (quantitative) ## Range: [1, 10] mlp_param %&gt;% extract_parameter_dials(&quot;penalty&quot;) ## Amount of Regularization (quantitative) ## Transformer: log-10 [1e-100, Inf] ## Range (transformed scale): [-10, 0] For penalty, the random numbers are uniform on the log (base 10) scale. The values in the grid are in their natural units. mlp_param %&gt;% extract_parameter_dials(&quot;epochs&quot;) ## # Epochs (quantitative) ## Range: [10, 1000] 13.1.1 Regular Grids The dials package contains a set of grid_*() functions that take the parameter object and produce different types of grids. grid_regular(mlp_param, levels = 2) ## # A tibble: 8 × 3 ## hidden_units penalty epochs ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 0.0000000001 10 ## 2 10 0.0000000001 10 ## 3 1 1 10 ## 4 10 1 10 ## 5 1 0.0000000001 1000 ## 6 10 0.0000000001 1000 ## 7 1 1 1000 ## 8 10 1 1000 The levels argument is the number of levels per parameter to create. It can also take a named vector of values: mlp_param %&gt;% grid_regular(levels = c(hidden_units = 3, penalty = 2, epochs = 2)) ## # A tibble: 12 × 3 ## hidden_units penalty epochs ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 0.0000000001 10 ## 2 5 0.0000000001 10 ## 3 10 0.0000000001 10 ## 4 1 1 10 ## 5 5 1 10 ## 6 10 1 10 ## 7 1 0.0000000001 1000 ## 8 5 0.0000000001 1000 ## 9 10 0.0000000001 1000 ## 10 1 1 1000 ## 11 5 1 1000 ## 12 10 1 1000 Regular grids can be computationally expensive to use, especially when there are a large number of tuning parameters. This is true for many models but not all. There are some models whose tuning time decreases with a regular grid. More on this in a moment. One advantage of a regular grid is that the relationships between the tuning parameters and the model metrics are easily understood. The full factorial nature of designs allows for examination of each parameter separately. 13.1.2 Irregular Grids Even with small grids, random values can still result in overlaps. Also, the random grid needs to cover the whole parameter space. Even for a sample of 15 candidate points, this plot shows some overlap between points for our example: library(ggforce) set.seed(200) mlp_param %&gt;% # The &#39;original = FALSE&#39; option keeps penalty in log10 units grid_random(size = 15, original = FALSE) %&gt;% ggplot(aes(x = .panel_x, y = .panel_y)) + geom_point() + geom_blank() + facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) + labs(title = &quot;Random design with 15 candidates&quot;) ggsave(filename = &quot;images/13_grid_random.png&quot;) A much better approach is to use designs called 13.1.2.1 Space Filling Designs !!! They generally find a configuration of points that cover the parameter space with the smallest chance of overlapping. The dials package has: Latin hypercube Maximum entropy As with grid_random(), the primary inputs are the number of parameter combinations and a parameter object. library(ggforce) set.seed(200) mlp_param %&gt;% grid_latin_hypercube(size = 15, original = FALSE) %&gt;% ggplot(aes(x = .panel_x, y = .panel_y)) + geom_point() + geom_blank() + facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) + labs(title = &quot;Latin Hypercube design with 15 candidates&quot;) ggsave(&quot;images/13_latin_hypercube.png&quot;) The default design used by tune: maximum entropy design. "],["evaluating-the-grid.html", "13.2 Evaluating the grid", " 13.2 Evaluating the grid “To choose the best tuning parameter combination, each candidate set is assessed using data on cross validation slices that were not used to train that model.” The user selects the most appropriate set. It might make sense to choose the empirically best parameter combination or bias the choice towards other aspects like simplicity. We will use the Chicago CTA data for modeling the number of people (in thousands) who enter the Clark and Lake L station, as ridership. The date column corresponds to the current date. The columns with station names (Austin through California) are 14 day lag variables. There are also columns related to weather and sports team schedules. data(Chicago) # from the modeldata package # also live data via RSocrata and Chicago portal glimpse(Chicago, width = 5) ## Rows: 5,698 ## Columns: 50 ## $ ridership &lt;dbl&gt; … ## $ Austin &lt;dbl&gt; … ## $ Quincy_Wells &lt;dbl&gt; … ## $ Belmont &lt;dbl&gt; … ## $ Archer_35th &lt;dbl&gt; … ## $ Oak_Park &lt;dbl&gt; … ## $ Western &lt;dbl&gt; … ## $ Clark_Lake &lt;dbl&gt; … ## $ Clinton &lt;dbl&gt; … ## $ Merchandise_Mart &lt;dbl&gt; … ## $ Irving_Park &lt;dbl&gt; … ## $ Washington_Wells &lt;dbl&gt; … ## $ Harlem &lt;dbl&gt; … ## $ Monroe &lt;dbl&gt; … ## $ Polk &lt;dbl&gt; … ## $ Ashland &lt;dbl&gt; … ## $ Kedzie &lt;dbl&gt; … ## $ Addison &lt;dbl&gt; … ## $ Jefferson_Park &lt;dbl&gt; … ## $ Montrose &lt;dbl&gt; … ## $ California &lt;dbl&gt; … ## $ temp_min &lt;dbl&gt; … ## $ temp &lt;dbl&gt; … ## $ temp_max &lt;dbl&gt; … ## $ temp_change &lt;dbl&gt; … ## $ dew &lt;dbl&gt; … ## $ humidity &lt;dbl&gt; … ## $ pressure &lt;dbl&gt; … ## $ pressure_change &lt;dbl&gt; … ## $ wind &lt;dbl&gt; … ## $ wind_max &lt;dbl&gt; … ## $ gust &lt;dbl&gt; … ## $ gust_max &lt;dbl&gt; … ## $ percip &lt;dbl&gt; … ## $ percip_max &lt;dbl&gt; … ## $ weather_rain &lt;dbl&gt; … ## $ weather_snow &lt;dbl&gt; … ## $ weather_cloud &lt;dbl&gt; … ## $ weather_storm &lt;dbl&gt; … ## $ Blackhawks_Away &lt;dbl&gt; … ## $ Blackhawks_Home &lt;dbl&gt; … ## $ Bulls_Away &lt;dbl&gt; … ## $ Bulls_Home &lt;dbl&gt; … ## $ Bears_Away &lt;dbl&gt; … ## $ Bears_Home &lt;dbl&gt; … ## $ WhiteSox_Away &lt;dbl&gt; … ## $ WhiteSox_Home &lt;dbl&gt; … ## $ Cubs_Away &lt;dbl&gt; … ## $ Cubs_Home &lt;dbl&gt; … ## $ date &lt;date&gt; … Ridership is the dependent variable. Sorted by oldest to newest date, it matches exactly the Clark_Lake lagged by 14 days. Chicago$ridership[25:27] ## [1] 15.685 15.376 2.445 Chicago$Clark_Lake[39:41] ## [1] 15.685 15.376 2.445 Ridership is in thousands per day and ranges from 600 to 26,058 summary(Chicago$ridership) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.601 6.173 15.902 13.619 18.931 26.058 Cross validation folds here are taken on a sliding window set.seed(33) split &lt;- rsample::initial_time_split(Chicago) Chicago_train &lt;- training(split) Chicago_test &lt;- testing(split) Chicago_folds &lt;- sliding_period( Chicago_train, index = date, period = &quot;year&quot;, lookback = 3, assess_stop = 1 ) Training and validation data range range(Chicago_train$date) ## [1] &quot;2001-01-22&quot; &quot;2012-10-03&quot; Testing data range range(Chicago_test$date) ## [1] &quot;2012-10-04&quot; &quot;2016-08-28&quot; ggplot(Chicago_folds %&gt;% tidy(), aes(x = Resample, y = Row, fill = Data)) + geom_tile() Because of the high degree of correlation between predictors, it makes sense to use PCA feature extraction. While the resulting PCA components are technically on the same scale, the lower-rank components tend to have a wider range than the higher-rank components. For this reason, we normalize again to coerce the predictors to have the same mean and variance. The resulting recipe: mlp_rec &lt;- recipe(ridership ~ ., data = Chicago_train) %&gt;% step_date(date, features = c(&quot;dow&quot;, &quot;month&quot;), ordinal = FALSE) %&gt;% step_rm(date) %&gt;% step_normalize(all_numeric(), -ridership) %&gt;% # remove the dependent step_pca(all_numeric(), -ridership, num_comp = tune()) %&gt;% step_normalize(all_numeric(), -ridership) # remove the dependent mlp_wflow &lt;- workflow() %&gt;% add_model(mlp_spec) %&gt;% add_recipe(mlp_rec) In step_pca(), using zero PCA components is a shortcut to skip the feature extraction. In this way, the original predictors can be directly compared to the results that include PCA components. Let’s create a parameter object to adjust a few of the default ranges. mlp_param &lt;- mlp_wflow %&gt;% parameters() %&gt;% update( epochs = epochs(c(50, 200)), num_comp = num_comp(c(0, 20)) ) rmse_mape_rsq_iic &lt;- metric_set(rmse, mape, rsq, iic) tune_grid() is the primary function for conducting grid search. It resembles fit_resamples() from prior chapters, but adds grid: An integer or data frame. When an integer is used, the function creates a space-filling design. If specific parameter combinations exist, the grid parameter is used to pass them to the function. param_info: An optional argument for defining the parameter ranges, when grid is an integer. set.seed(99) mlp_reg_tune &lt;- mlp_wflow %&gt;% tune_grid( Chicago_folds, grid = mlp_param %&gt;% grid_regular(levels = 3), metrics = rmse_mape_rsq_iic ) write_rds(mlp_reg_tune, file = &quot;data/13-Chicago-mlp_reg_tune.rds&quot;, compress = &quot;gz&quot;) There are high-level convenience functions to understand the results. First, the autoplot() method for regular grids shows the performance profiles across tuning parameters: autoplot(mlp_reg_tune) + theme(legend.position = &quot;top&quot;) ggsave(&quot;images/13_mlp_reg_tune_autoplot.png&quot;, width = 12) The best model, per the index of ideality of correlation (iic), on the validation folds More study might be warranted to dial in the resolution of the penalty and number of pca components. To evaluate the same range using (the tune grid default) maximum entropy design with 20 candidate values: set.seed(99) mlp_sfd_tune &lt;- mlp_wflow %&gt;% tune_grid( Chicago_folds, grid = 20, # Pass in the parameter object to use the appropriate range: param_info = mlp_param, metrics = rmse_mape_rsq_iic ) write_rds(mlp_sfd_tune, file = &quot;data/13-Chicago-mlp_max_entropy.rds&quot;, compress = &quot;gz&quot;) autoplot(mlp_sfd_tune) ggsave(&quot;images/13_mlp_max_entropy_plot.png&quot;) Care should be taken when examining this plot; since a regular grid is not used, the values of the other tuning parameters can affect each panel. show_best(mlp_sfd_tune, metric = &quot;iic&quot;) %&gt;% select(-.estimator) hidden_units penalty epochs num_comp .metric mean n std_err .config &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; 1 9 7.80e- 3 158 14 iic 0.790 8 0.0439 Preprocessor~ 2 4 7.01e- 9 173 18 iic 0.779 8 0.0375 Preprocessor~ 3 10 2.96e- 4 155 19 iic 0.777 8 0.0293 Preprocessor~ 4 8 2.96e- 6 69 19 iic 0.760 8 0.0355 Preprocessor~ 5 5 8.76e-10 199 9 iic 0.756 8 0.0377 Preprocessor~ It often makes sense to choose a slightly suboptimal parameter combination that is associated with a simpler model. For this model, simplicity corresponds to larger penalty values and/or fewer hidden units. "],["finalizing-the-model.html", "13.3 Finalizing the model", " 13.3 Finalizing the model Two methods: manually choose parameters, or select_best() select_best(mlp_sfd_tune, metric = &quot;iic&quot;) # or, a manual selection that corresponds to the regular method reg_param &lt;- tibble( num_comp = 0, epochs = 200, hidden_units = 5, penalty = 1 # log10 ) # A tibble: 1 x 5 hidden_units penalty epochs num_comp .config &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; 1 9 0.00780 158 14 Preprocessor06_Model1 final_sfd_wflow &lt;- mlp_wflow %&gt;% finalize_workflow(select_best(mlp_sfd_tune, metric = &quot;rmse&quot;)) Now the model can fit the entire training set: final_sfd_fit &lt;- final_sfd_wflow %&gt;% fit(Chicago_train) This object can now be used to make future predictions on new data. final_sfd_fit %&gt;% predict(new_data = Chicago_test) %&gt;% bind_cols(Chicago_test) %&gt;% rmse(truth = ridership, estimate = .pred) # A tibble: 1 x 3 .metric .estimator .estimate &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 rmse standard 3.01 final_sfd_fit %&gt;% predict(new_data = Chicago_test) %&gt;% bind_cols(Chicago_test) %&gt;% mutate(weekday = wday(date, label = TRUE)) %&gt;% ggplot(aes(ridership, .pred, color = weekday)) + geom_point() + geom_abline(color = &quot;red&quot;) + scale_color_brewer(type = &quot;qual&quot;) + coord_fixed() + expand_limits(x = 0, y = 0) ggsave(&quot;images/13_test_performance.png&quot;) I learned here that the recipe must exclude the dependent variable for the predict() to run on test data. "],["tools-for-efficient-grid-search.html", "13.4 Tools for efficient grid search", " 13.4 Tools for efficient grid search A few tricks: 13.4.1 Submodel optimization Types of models where, from a single model fit, multiple tuning parameters can be evaluated without refitting: Partial Least Squares (no. of components to retain) Boosting models (no. of boosting iterations, i.e. trees) glmnet makes (across the amount of regularization) MARS adds a set of nonlinear features (number of terms to retain) The tune package automatically applies this type of optimization whenever an applicable model is tuned. See also this vignette methods(&quot;multi_predict&quot;) ## [1] multi_predict._C5.0* multi_predict._earth* ## [3] multi_predict._elnet* multi_predict._glmnetfit* ## [5] multi_predict._lognet* multi_predict._multnet* ## [7] multi_predict._torch_mlp* multi_predict._train.kknn* ## [9] multi_predict._xgb.Booster* multi_predict.default* ## see &#39;?methods&#39; for accessing help and source code parsnip:::multi_predict._C5.0 %&gt;% formals() %&gt;% names() ## [1] &quot;object&quot; &quot;new_data&quot; &quot;type&quot; &quot;trees&quot; &quot;...&quot; For example, if a C5.0 model is fit to this cell classification data challenge, we can tune the trees. With all other parameters set at their default values, we can rapidly evaluate iterations from 1 to 100 : data(cells) cells &lt;- cells %&gt;% select(-case) cell_folds &lt;- vfold_cv(cells) roc_res &lt;- metric_set(roc_auc) c5_spec &lt;- boost_tree(trees = tune()) %&gt;% set_engine(&quot;C5.0&quot;) %&gt;% set_mode(&quot;classification&quot;) set.seed(2) c5_tune &lt;- c5_spec %&gt;% tune_grid( class ~ ., resamples = cell_folds, grid = data.frame(trees = 1:100), metrics = roc_res ) Even though we fit the model without the submodel prediction trick, this optimization is automatically applied by parsnip. autoplot(c5_tune) ggsave(&quot;images/13_c5_submodel.png&quot;) 13.4.2 Parallel processing backend packages right now are doFuture, doMC, doMPI, doParallel, doRedis,doRNG, doSNOW, and doAzureParallel In tune_*(), there are two approaches, often set in control_grid() or control_resamples() parallel_over = \"resamples or parallel_over = \"everything\" or parallel_over = NULL (the default) chooses “resamples” if there are more than one resample, otherwise chooses “everything” to attempt to maximize core utilization Note that switching between parallel_over strategies is not guaranteed to use the same random number generation schemes. However, re-tuning a model using the same parallel_over strategy is guaranteed to be reproducible between runs. To use them, register the parallel backend first. On a shared server, never never consume all of the cores. all_cores &lt;- parallel::detectCores(logical = FALSE) library(doParallel) cl &lt;- makePSOCKcluster(all_cores) doParallel::registerDoParallel(cl) Be careful to avoid use of variables from the global environment. For example: num_pcs &lt;- 3 recipe(mpg ~ ., data = mtcars) %&gt;% # Bad since num_pcs might not be found by a worker process step_pca(all_predictors(), num_comp = num_pcs) recipe(mpg ~ ., data = mtcars) %&gt;% # Good since the value is injected into the object step_pca(all_predictors(), num_comp = !!num_pcs) for the most part, the logging provided by tune_grid() will not be seen when running in parallel. 13.4.3 Benchmarking Parallel with boosted trees Three scenarios Preprocess the data prior to modeling using dplyr Conduct the same preprocessing via a recipe With a recipe, add a step that has a high computational cost using variable numbers of worker processes and using the two parallel_over options, on a computer with 10 physical cores For dplyr and the simple recipe There is little difference in the execution times between the panels. There is some benefit for using parallel_over = \"everything\" with many cores. However, as shown in the figure, the majority of the benefit of parallel processing occurs in the first five workers. With the expensive preprocessing step, there is a considerable difference in execution times. Using parallel_over = \"everything\" is problematic since, even using all cores, it never achieves the execution time that parallel_over = \"resamples\" attains with just five cores. This is because the costly preprocessing step is unnecessarily repeated in the computational scheme. Overall, note that the increased computational savings will vary from model-to-model and are also affected by the size of the grid, the number of resamples, etc. A very computationally efficient model may not benefit as much from parallel processing. 13.4.4 Racing Methods The finetune package contains functions for racing. One issue with grid search is that all models need to be fit across all resamples before any tuning parameters can be evaluated. It would be helpful if instead, at some point during tuning, an interim analysis could be conducted to eliminate any truly awful parameter candidates. In racing methods the tuning process evaluates all models on an initial subset of resamples. Based on their current performance metrics, some parameter sets are not considered in subsequent resamples. As an example, in the Chicago multilayer perceptron tuning process with a regular grid above, what would the results look like after only the first three folds? We can fit a model where the outcome is the resampled area under the ROC curve and the predictor is an indicator for the parameter combination. The model takes the resample-to-resample effect into account and produces point and interval estimates for each parameter setting. The results of the model are one-sided 95% confidence intervals that measure the loss of the ROC value relative to the currently best performing parameters. Any parameter set whose confidence interval includes zero would lack evidence that its performance is not statistically different from the best results. We retain 10 settings; these are resampled more. The remaining 10 submodels are no longer considered. Video Racing methods can be more efficient than basic grid search as long as the interim analysis is fast and some parameter settings have poor performance. It also is most helpful when the model does not have the ability to exploit submodel predictions. The tune_race_anova() function conducts an Analysis of Variance (ANOVA) model to test for statistical significance of the different model configurations. library(finetune) set.seed(99) mlp_sfd_race &lt;- mlp_wflow %&gt;% tune_race_anova( Chicago_folds, grid = 20, param_info = mlp_param, metrics = rmse_mape_rsq_iic, control = control_race(verbose_elim = TRUE) ) write_rds(mlp_sfd_race, &quot;data/13-Chicago-mlp_sfd_race.rds&quot;, compress = &quot;gz&quot;) autoplot(mlp_sfd_race) ggsave(&quot;images/13_mlp_sfd_race.png&quot;, width = 12) show_best(mlp_sfd_race, n = 6) hidden_units penalty epochs num_comp .metric .estimator mean n &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; 1 6 3.08e- 5 126 3 rmse standard 2.47 8 2 8 2.15e- 1 148 9 rmse standard 2.48 8 3 10 9.52e- 3 157 3 rmse standard 2.55 8 4 6 2.60e-10 84 12 rmse standard 2.56 8 5 5 1.48e- 2 94 4 rmse standard 2.57 8 6 4 7.08e- 1 98 14 rmse standard 2.60 8 # ... with 2 more variables: std_err &lt;dbl&gt;, .config &lt;chr&gt; Warning message: No value of `metric` was given; metric &#39;rmse&#39; will be used. "],["chapter-summary.html", "13.5 Chapter Summary", " 13.5 Chapter Summary regular and irregular grids, including space-filling designs build manually or using the family of grid_*() functions. tune_grid() can evaluate candidate sets of model parameters using resampling. autoplot() the tune object for the preferred performance metrics show_best() for a list of top models fast submodel optimization for some models / parameters on regular grids how to finalize a model, recipe, or workflow to update the parameter values for the final fit parallel processing backend capabilities consider racing methods to skip poor parameter combinations Grid search is computationally expensive, but thoughtful choices in the experimental design can make them tractable. "],["meeting-videos-14.html", "13.6 Meeting Videos", " 13.6 Meeting Videos 13.6.1 Cohort 1 Meeting chat log 00:17:09 Andy Farina: Not sure if everyone saw this, but tomorrow (Tuesday, 07 April) at noon (MDT), the salt lake city r users group is hosting a talk on Stacked ensemble Modeling using {stacks} https://www.meetup.com/slc-rug/events/275790402/?rv=cr1&amp;_xtd=gatlbWFpbF9jbGlja9oAJDVkMmFjNjc0LTJlOGUtNGJlNS1hYjk5LWM1ZDBjODU5YTEyYQ&amp;_af=event&amp;_af_eid=275790402 00:17:28 Jordan Krogmann: +1 noice 00:27:10 Conor Tompkins: Very cool 01:02:28 Asmae Toumi: I volunteeeeeeeeeerrrrrrrrrrrr 01:02:36 Asmae Toumi: Tony! 01:02:41 Asmae Toumi: Or else you’re not getting big data bowl money 01:02:53 Asmae Toumi: That’s too bad, im keeping your 5k 01:03:25 Asmae Toumi: I can try and harass someone in the slack 01:03:44 Asmae Toumi: Lmao 01:05:57 Asmae Toumi: Bye yall 13.6.2 Cohort 2 Meeting chat log 00:10:22 Janita Botha: did anyone join the other cohort&#39;s session last week? 00:10:43 kevin : Yeah I joined for most of it 00:10:56 kevin : Even got in a question at the end 😎 00:11:40 Janita Botha: That is awesome. Just had too much going on that day. 00:12:03 kevin : Always the recording to watch 01:04:34 Luke Shaw: Gotta go, cheers for the session :) 01:07:20 kevin : Thanks so much Stephen, I gotta drop as well 01:07:24 kevin : Great sessions 01:30:42 Amélie Gourdon-Kanhukamwe (she/they): Gonna drop too, thanks Stephen! 01:30:49 Stephen: Thanks Amelie! 01:31:13 Janita Botha: I&#39;ve really enjoyed this and listening in on your conversation! 01:31:26 Janita Botha: happy days! 01:31:26 Stephen: Thanks for coming Janita :) glad you enjoyed! 13.6.3 Cohort 3 Meeting chat log 00:33:23 jiwan: how do we go about visualizing/ making sense of hyperparameters when there&#39;s more than 3-4 00:52:32 jiwan: https://juliasilge.com/blog/shelter-animals/ 13.6.4 Cohort 4 Meeting chat log 00:15:43 Ryan Metcalf: Quick comment on Neural Networks and Perceptrons. Check out Professor Geoffrey Hinton, University of Toronto. I’m still looking for my book reference. 00:21:32 Ryan Metcalf: Perceptrons by By Marvin Minsky and Seymour A. Papert, https://mitpress.mit.edu/books/perceptrons 00:39:47 Isabella Velásquez: https://www.tidyverse.org/blog/2022/03/usemodels-0-2-0/ 00:59:46 Steve: Gotta go, sorry 00:59:48 Steve: Thanks! Meeting chat log 00:11:54 Brandon Hurr: https://stackoverflow.com/questions/58962748/opencv-with-multiple-webcams-how-to-tell-which-camera-is-which-in-code 00:23:54 Ryan Metcalf: https://www.rdocumentation.org/packages/rlang/versions/0.1.6/topics/quosure 00:33:42 Federica Gazzelloni: https://recipes.tidymodels.org/reference/step_spatialsign.html "],["iterative-search.html", "Chapter 14 Iterative search", " Chapter 14 Iterative search Learning objectives: Use tune::tune_bayes() to optimize model parameters using Bayesian optimization. Describe how a Gaussian process model can be applied to parameter optimization. Explain how acquisition functions can be expressed as a trade-off between exploration and exploitation. Describe expected improvement, the default acquisition function used by {tidymodels}. Use finetune::tune_sim_anneal() to optimize model parameters using Simulated annealing. Describe simulated annealing search. "],["svm-model-as-motivating-example.html", "14.1 SVM model as motivating example", " 14.1 SVM model as motivating example We’re interested in developing a classification model to classify sex for the palmers penguins dataset using a radial basis function support vector machine (svm). library(tidyverse) library(tidymodels) library(palmerpenguins) library(patchwork) library(finetune) penguins_df &lt;- penguins %&gt;% filter(!is.na(sex)) %&gt;% # discarding NA obs select(-year, -island) # not useful splits &lt;- initial_split(penguins_df, strata = sex) penguins_folds &lt;- vfold_cv(training(splits), v = 5, strata = sex) set.seed(420) roc_res &lt;- metric_set(roc_auc) # accuracy, a classification metric Let’s fit a radial basis function support vector machine to the palmers penguins and tune the SVM cost parameter (cost()) and the σ parameter in the kernel function (rbf_sigma): svm_rec &lt;- recipe(sex ~ ., data = penguins_df) svm_spec &lt;- svm_rbf(cost = tune(), rbf_sigma = tune()) %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;kernlab&quot;) Now, let’s set up our workflow() and feeding it our svm model svm_wflow &lt;- workflow() %&gt;% add_model(svm_spec) %&gt;% add_recipe(svm_rec) Let’s zoom in on the default parameter values for our two tuning parameters: cost() ## Cost (quantitative) ## Transformer: log-2 [1e-100, Inf] ## Range (transformed scale): [-10, 5] rbf_sigma() ## Radial Basis Function sigma (quantitative) ## Transformer: log-10 [1e-100, Inf] ## Range (transformed scale): [-10, 0] We can change them: svm_param &lt;- svm_wflow %&gt;% parameters() %&gt;% update(rbf_sigma = rbf_sigma(c(-7, -1))) Because the methods that we will go over later need some resampled performance statistics before proceeding, we can use tune_grid() function to resample these values: start_grid &lt;- svm_param %&gt;% update( cost = cost(c(-6, 1)), rbf_sigma = rbf_sigma(c(-6, -4))) %&gt;% grid_regular(levels = 2) set.seed(2) svm_initial &lt;- svm_wflow %&gt;% tune_grid(resamples = penguins_folds, grid = start_grid, metrics = roc_res) collect_metrics(svm_initial) ## # A tibble: 4 × 8 ## cost rbf_sigma .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.0156 0.000001 roc_auc binary 0.588 5 0.171 Preprocessor1_Model1 ## 2 2 0.000001 roc_auc binary 0.588 5 0.171 Preprocessor1_Model2 ## 3 0.0156 0.0001 roc_auc binary 0.588 5 0.172 Preprocessor1_Model3 ## 4 2 0.0001 roc_auc binary 0.836 5 0.0545 Preprocessor1_Model4 We can see that there’s one point in which the performance is better. There results can be fed into iterative tuning functions as initial values, which we’ll see shortly. "],["bayesian-optimization.html", "14.2 Bayesian Optimization", " 14.2 Bayesian Optimization Bayesian optimization is one of the techniques that can be used to iteratively search for new tuning parameter values. Bayesian optimization consists of the following steps: create a model predicting new tuning parameter values based on the previously generated resampling results resampling of the new tuning parameter values create another model that recommends additional tuning parameter values based on the resampling results of the previous step This process can occur: for a predetermined number of iterations, or until there is no improvement in the results Let’s go over the most commonly used technique for Bayesian optimization, called the Gaussian process model. 14.2.1 Gaussian process model, at a high level In plain terms, Gaussian processes (GP) models allow us to make predictions about our data by incorporating prior knowledge and fitting a function to the data. With a given set of training points, there are potentially an infinite number of functions. GP shine by giving each of these functions a probability. This generates a probability distribution, which can harnessed. 14.2.1.1 How is it used for tuning? As the name suggests, the Gaussian distribution is central to GP models. We’re interested in the multivariate case of this distribution, where each random variable is distributed normally and their joint distribution is also Gaussian. This collection of random variables in the context of our example is the collection of performance metrics for the tuning parameter candidate values. The 25 random variables making up the grid for our SVM model is assumed to be distributed as multivariate Gaussian. For the GP model: the inputs (i.e. predictors) are the tuning parameters, cost and rbf_sigma the outcome is the performance metric, ROC AUC the outputs are predicted mean and variance (of ROC AUC) for the new candidate tuning parameters note: the predicted variance is mostly driven by how far it is from existing data A candidate is selected Performance estimate are calculated for all existing results This process is iterative, and keeps repeating until number of iterations is exhausted or no improvement occurs. See Max and Julia’s notes for an in-depth appreciation of the mathematical implications of GP, along with this excellent interactive blog post by researchers at the University of Konstanz. The elegant properties of GP allow us to: compute new performance statistics because we obtain a full probability model which reflects the entire distribution of the outcome represent highly non-linear relationships between model performance and the tuning parameters While it’s a powerful technique that can yield good results, it can be complex to set up. The two main considerations are: how to set up the model how to pick the parameter values suggested by the model resources, as it can be computationally expensive Point 2 is further explored in the next section. 14.2.1.2 Acquisition functions As we saw previously, GP model generates predicted mean and variance for candidate combinations of parameter values. The next step is picking the parameter combination that could give us the best results. This picking process can be tricky, as there is a trade-off between the predicted mean and variance for new candidates. This trade-off is similar to another, the exploration-exploitation trade-off: Exploration: selection is towards “relatively unexplored” areas i.e. where there are fewer (or no) candidate models. This results in candidates having relatively higher variance, as they are “further” from existing data. Exploitation: selection is based on the best mean prediction. This results in candidates having relatively lower variance, as it focuses on existing data. The following is an example consisting of 1 tuning parameter (0,1) where the performance metric is r-squared. The points correspond to the observed candidate values for the tuning parameter. The shaded regions represent the mean +/- 1 standard error. From an exploitation standpoint, one might select a parameter value right next to the observed point - i.e. near left vertical line - as it has the best r-squared. From an exploration standpoint, one might consider the parameter value with the largest confidence interval - i.e. near right vertical line - since it would push our selection towards a region with no observed result. This is known as the confidence bound approach. Max and Julia note that the latter approach is not often used. This is where acquisition functions come in, as they can help us in this process of picking a suitable parameter value. The most commonly used one is expected improvement. Let’s illustrate how it works by bringing back the two candidate parameter values we were considering, 0.1 and 0.25: We can see that the distribution for 0.1 is much narrower (red line), and has the best r-squared (vertical line). So, 0.1 is our current best on average; however, we can see that for parameter value 0.25 there is higher variance and more overall probability area above the current best. What does this mean for our expected improvement acquisition function? We can see that the expected improvement is significantly higher for parameter value 0.25! 14.2.1.3 The tune_bayes() function The tune_bayes() function sets up Bayesian optimization iterative search. It’s similar to tune_grid() but with additional arguments. You can specify the maximum number of search iterations, the acquisition function to be used, and whether to stop the search if no improvement is detected. See Max and Julia for details and additional arguments. Let’s keep going with our SVM model. We can use the earlier SVM results as the initial substrate. Once again, we’re trying to maximize ROC AUC. ctrl &lt;- control_bayes(verbose = TRUE) # can also add more arguments, like no_improve set.seed(420) svm_bo &lt;- svm_wflow %&gt;% tune_bayes( resamples = penguins_folds, metrics = roc_res, initial = svm_initial, # tune_grid object produced earlier param_info = svm_param, # specified earlier too, with our new bounds for rbf_sigma iter = 25, # maximum number of search iterations control = ctrl ) Looks like the improvements occurred at iterations 8, 11, 13, 12 and 10. We can pull the best result(s) like so: show_best(svm_bo) # same call as with grid search ## # A tibble: 5 × 9 ## cost rbf_sigma .metric .estimator mean n std_err .config .iter ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 31.5 0.0986 roc_auc binary 0.965 5 0.00909 Iter21 21 ## 2 24.9 0.0979 roc_auc binary 0.965 5 0.00896 Iter25 25 ## 3 22.5 0.0984 roc_auc binary 0.965 5 0.00900 Iter23 23 ## 4 7.89 0.0548 roc_auc binary 0.965 5 0.0103 Iter20 20 ## 5 30.2 0.0881 roc_auc binary 0.964 5 0.00905 Iter10 10 p1 &lt;- autoplot(svm_bo, type = &quot;performance&quot;) p2 &lt;- autoplot(svm_bo, type = &quot;parameters&quot;) p1 + p2 "],["simulated-annealing.html", "14.3 Simulated annealing", " 14.3 Simulated annealing Simulated annealing is loosely related to annealing in metallurgy. When hot, the atoms in the material are more free to move around, and, through random motion, tend to settle into better positions. A slow cooling brings the material to an ordered, crystalline state. Page 128, Algorithms for Optimization, 2019. If you were to cool rapidly, the atoms would stay wherever they were while the metal was hot and your blade or whatever would be ugly and brittle. 14.3.1 How it works At a high level, simulated annealing is an iterative process. It involves: starts with a single candidate value takes a random but constrained walk (controlled random walk) in a parameter search space (local neighborhood) if the new candidate parameter value is better than the current candidate value - i.e. leads to better performance - then the current value is replaced with this new parameter value the algorithm can still accept worse candidate values sometimes; however it will do so to a lesser extent as: performance gets worse iterations increase Why would it do this? Max and Julia sum it up perfectly: “The acceptance probabilities of simulated annealing allows the search to proceed in the wrong direction, at least for the short term, with the potential to find a much better region of the parameter space in the long run.” We can illustrate this graphically: We can imagine the green color - the acceptance probability - is the temperature. At the beginning, it’s a real hot girl summer, we’re throwing it back everywhere, accepting poor solutions left and right. as the temperature cools, cuffing season starts, we are wayyyy more selective. This is how simulated annealing works - poor candidate parameter values have a higher chance of being accepted by the algorithm at the earlier iterations, and the algorithm hones in on the optimal candidate values as performance gets worse in the later iterations. On a more serious note, you might be wondering how this probability is worked out. See Max and Julia for formal details. On a high level, it’s influenced by: iteration number performance user-specified constant: coefficient that can be changed from the default value of 0.02 in finetune::control_sim_anneal() From earlier, we said simulation annealing searches for values within a search space, called the local neighborhood. This “neighborhood” is defined by a radius that fluctuates randomly over a range and around the initial point. Once a candidate is chosen in that neighborhood, it becomes the new “initial point” and a new candidate is selected randomly in the radius range, and so on. The following graph illustrates this process using the penalty parameter of a glmnet model. For models with non-numeric parameters, we can assign a probability for how often the parameter value changes. One last note: simulation annealing keeps going until there is no best result within a pre-specified number of iterations. Max and Julia note that you should set a restart threshold so that the process can restart after it goes through a bad stretch. 14.3.2 The tune_sim_anneal() function The tune_sim_anneal() function uses the generalized simulated annealing method of Bohachevsky, Johnson, and Stein (1986). There are more flavors in the literature, but this is the one that tidymodels uses. Important specifications include: no_improve: the number of iterations before it stops if it finds no improved results restart: number of iterations where allowing to be bad before starting anew radius: radius range on (0, 1) that defines the search space i.e. local neighborhood flip: for non-numeric parameters, this is the probability for how often the parameter value changes cooling_coef: dictates how quickly the acceptance probability decreases as the we go through iterations. Larger coefficient values means the probability of accepting a bad result will decrease. Implemention is very similar to grid search and Bayesian optimization. We can print out the best results, and have visual assessments of our search went across iterations. ctrl_sa &lt;- control_sim_anneal(verbose = TRUE, no_improve = 10L) set.seed(1234) svm_sa &lt;- svm_wflow %&gt;% tune_sim_anneal( resamples = penguins_folds, metrics = roc_res, initial = svm_initial, param_info = svm_param, iter = 50, control = ctrl_sa ) show_best(svm_sa) ## # A tibble: 5 × 9 ## cost rbf_sigma .metric .estimator mean n std_err .config .iter ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 19.5 0.0199 roc_auc binary 0.965 5 0.0101 Iter27 27 ## 2 14.2 0.0276 roc_auc binary 0.964 5 0.0105 Iter30 30 ## 3 5.91 0.0930 roc_auc binary 0.964 5 0.00958 Iter29 29 ## 4 25.0 0.0786 roc_auc binary 0.964 5 0.00874 Iter31 31 ## 5 4.14 0.0666 roc_auc binary 0.963 5 0.0110 Iter23 23 Voilààà! "],["references-2.html", "14.4 References", " 14.4 References Get started with tidymodels and TidyTuesday Palmer penguins: https://juliasilge.com/blog/palmer-penguins/ A Visual Exploration of Gaussian Processes: https://distill.pub/2019/visual-exploration-gaussian-processes/#Multivariate "],["meeting-videos-15.html", "14.5 Meeting Videos", " 14.5 Meeting Videos 14.5.1 Cohort 1 Meeting chat log 00:18:01 Conor Tompkins: The cat is behind you 14.5.2 Cohort 2 Meeting chat log 00:40:55 Janita Botha: can you zoom in please? 00:44:58 Janita Botha: all good 00:45:00 Janita Botha: :) 00:47:42 Janita Botha: thanks stephen 00:47:48 Amélie Gourdon-Kanhukamwe (she/they): Thanks! 00:48:24 Stephen: https://docs.google.com/spreadsheets/d/1vD4LG4_nhsxSAxXiBi42iKIvZXQtNxgB5C_PUkIZ0wo/edit#gid=0 00:53:14 Janita Botha: Yes! 00:54:07 Janita Botha: bye! 00:54:12 Amélie Gourdon-Kanhukamwe (she/they): Bye! 14.5.3 Cohort 3 Meeting chat log 00:11:21 Daniel Chen: is this like picking the parameters for a given distribution? like beta(0, 1) vs beta(0.1, 0.9)? 00:12:09 Daniel Chen: ok nvm this is the svm example right now. I guess that question is for the Bayesian point 00:12:49 Jiwan Heo: I think so, updating priors to make posterior distribution 00:31:36 Daniel Chen: the usemodels is mainly a way to help you out with somewhat reasonable defaults 00:35:02 Daniel Chen: like the &quot;caterpillar plot&quot; to help you see if things converged? 00:35:08 Jiwan Heo: yea! 00:38:13 Daniel Chen: i forgot what data is using? the post resamples? 00:38:23 Daniel Chen: you might need to manually pull that out from the data? 00:44:57 Daniel Chen: for the bayes stuff. is there a way to change the backend for the bayes calculation? e.g., using STAN or something? 00:47:33 Jiwan Heo: maybe in the model spec? 00:47:43 Jiwan Heo: bayes.model = linear_reg() %&gt;% set_engine(engine = &quot;stan&quot;, prior_intercept = prior.dist, prior = prior.dist) %&gt;% set_mode(mode = &quot;regression&quot;) 00:49:08 Daniel Chen: oooh. yeah. okay 00:54:52 Daniel Chen: yeah i guess this ends up being hard since bayes is anaother layer of things to understand the examples 00:55:07 Daniel Chen: i do wonder if the SVM example or something can/should be simplified to be used as a chapter 0 00:55:26 Daniel Chen: I&#39;ve only really used grid search in the past personally in the past 00:55:36 Daniel Chen: but i was before tidymodels and all hand coded 00:55:42 Daniel Chen: expand.grid was a friend :) 01:03:16 Jiwan Heo: have to jump off, thank you for the presentation! 01:09:27 Daniel Chen: i&#39;m good for next week 14.5.4 Cohort 4 Meeting chat log 00:30:17 Stephen Charlesworth: https://www.youtube.com/watch?v=FyyVbuLZav8 00:30:37 Stephen Charlesworth: https://www.youtube.com/embed/enNgiWuIHAo "],["screening-many-models.html", "Chapter 15 Screening Many Models", " Chapter 15 Screening Many Models Learning objectives: Use the {parsnip} Generate parsnip model specifications addin to create a set of model specifications. Create a workflow set with the {workflowsets} package. Describe the purposes of the workflow set columns. Create a workflow set with a recipe preprocessor. Create a workflow set with a {dplyr} selector preprocesor. Tune and evaluate workflow sets. Use workflowsets::workflow_map() to tune all models in a workflow set. Use convenience functions such as workflowsets::rank_results() to examine workflow set tuning results. Visualize workflow set tuning results. Use workflowsets::workflow_map with {finetune} to efficiently screen models using the racing approach. Compare the results of the racing approach to the results of the full workflow set screening. Finalize the best model from a workflow set. "],["obligatory-setup.html", "15.1 Obligatory Setup", " 15.1 Obligatory Setup Using the 2021 World Happiness Report. Why? Small Interesting How I felt reading this chapter with concrete from {modeldata} library(tidyverse) library(tidymodels) theme_set(theme_minimal(base_size = 16)) df &lt;- here::here(&#39;data&#39;, &#39;world-happiness-report-2021.csv&#39;) %&gt;% read_csv() %&gt;% janitor::clean_names() df %&gt;% skimr::skim() Table 15.1: Data summary Name Piped data Number of rows 149 Number of columns 20 _______________________ Column type frequency: character 2 numeric 18 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace country_name 0 1 4 25 0 149 0 regional_indicator 0 1 9 34 0 10 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist ladder_score 0 1 5.53 1.07 2.52 4.85 5.53 6.26 7.84 ▁▅▇▇▃ standard_error_of_ladder_score 0 1 0.06 0.02 0.03 0.04 0.05 0.07 0.17 ▇▆▁▁▁ upperwhisker 0 1 5.65 1.05 2.60 4.99 5.62 6.34 7.90 ▁▃▇▇▃ lowerwhisker 0 1 5.42 1.09 2.45 4.71 5.41 6.13 7.78 ▁▃▇▇▃ logged_gdp_per_capita 0 1 9.43 1.16 6.64 8.54 9.57 10.42 11.65 ▂▆▇▇▅ social_support 0 1 0.81 0.11 0.46 0.75 0.83 0.90 0.98 ▁▂▃▇▇ healthy_life_expectancy 0 1 64.99 6.76 48.48 59.80 66.60 69.60 76.95 ▂▃▃▇▅ freedom_to_make_life_choices 0 1 0.79 0.11 0.38 0.72 0.80 0.88 0.97 ▁▂▅▇▇ generosity 0 1 -0.02 0.15 -0.29 -0.13 -0.04 0.08 0.54 ▅▇▅▁▁ perceptions_of_corruption 0 1 0.73 0.18 0.08 0.67 0.78 0.84 0.94 ▁▁▁▅▇ ladder_score_in_dystopia 0 1 2.43 0.00 2.43 2.43 2.43 2.43 2.43 ▁▁▇▁▁ explained_by_log_gdp_per_capita 0 1 0.98 0.40 0.00 0.67 1.02 1.32 1.75 ▂▆▇▇▅ explained_by_social_support 0 1 0.79 0.26 0.00 0.65 0.83 1.00 1.17 ▁▂▅▇▇ explained_by_healthy_life_expectancy 0 1 0.52 0.21 0.00 0.36 0.57 0.66 0.90 ▂▃▃▇▅ explained_by_freedom_to_make_life_choices 0 1 0.50 0.14 0.00 0.41 0.51 0.60 0.72 ▁▂▅▇▇ explained_by_generosity 0 1 0.18 0.10 0.00 0.10 0.16 0.24 0.54 ▅▇▅▁▁ explained_by_perceptions_of_corruption 0 1 0.14 0.11 0.00 0.06 0.10 0.17 0.55 ▇▅▁▁▁ dystopia_residual 0 1 2.43 0.54 0.65 2.14 2.51 2.79 3.48 ▁▂▅▇▃ library(corrr) df_selected &lt;- df %&gt;% select( ladder_score, logged_gdp_per_capita, social_support, healthy_life_expectancy, freedom_to_make_life_choices, generosity, perceptions_of_corruption ) cors &lt;- df_selected %&gt;% select(where(is.numeric)) %&gt;% corrr::correlate() %&gt;% rename(col1 = term) %&gt;% pivot_longer( -col1, names_to = &#39;col2&#39;, values_to = &#39;cor&#39; ) %&gt;% arrange(desc(abs(cor))) cors %&gt;% filter(col1 == &#39;ladder_score&#39;) ## # A tibble: 7 × 3 ## col1 col2 cor ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 ladder_score logged_gdp_per_capita 0.790 ## 2 ladder_score healthy_life_expectancy 0.768 ## 3 ladder_score social_support 0.757 ## 4 ladder_score freedom_to_make_life_choices 0.608 ## 5 ladder_score perceptions_of_corruption -0.421 ## 6 ladder_score generosity -0.0178 ## 7 ladder_score ladder_score NA p_cors &lt;- cors %&gt;% filter(col1 &lt; col2) %&gt;% ggplot() + aes(x = col1, y = col2) + geom_tile(aes(fill = cor), alpha = 0.7) + geom_text(aes(label = scales::number(cor, accuracy = 0.1))) + guides(fill = &quot;none&quot;) + scale_fill_viridis_c(option = &#39;E&#39;, direction = 1, begin = 0.2) + labs(x = NULL, y = NULL) + theme( panel.grid.major = element_blank(), axis.text.x = element_blank() ) p_cors "],["creating-workflow_sets.html", "15.2 Creating workflow_sets", " 15.2 Creating workflow_sets seed &lt;- 2021 col_y &lt;- &#39;ladder_score&#39; col_y_sym &lt;- col_y %&gt;% sym() set.seed(seed) split &lt;- df_selected %&gt;% initial_split(strata = !!col_y_sym) df_trn &lt;- split %&gt;% training() df_tst &lt;- split %&gt;% testing() folds &lt;- df_trn %&gt;% vfold_cv(strata = !!col_y_sym, repeats = 5) folds ## # 10-fold cross-validation repeated 5 times using stratification ## # A tibble: 50 × 3 ## splits id id2 ## &lt;list&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &lt;split [97/12]&gt; Repeat1 Fold01 ## 2 &lt;split [97/12]&gt; Repeat1 Fold02 ## 3 &lt;split [97/12]&gt; Repeat1 Fold03 ## 4 &lt;split [97/12]&gt; Repeat1 Fold04 ## 5 &lt;split [97/12]&gt; Repeat1 Fold05 ## 6 &lt;split [97/12]&gt; Repeat1 Fold06 ## 7 &lt;split [97/12]&gt; Repeat1 Fold07 ## 8 &lt;split [100/9]&gt; Repeat1 Fold08 ## 9 &lt;split [101/8]&gt; Repeat1 Fold09 ## 10 &lt;split [101/8]&gt; Repeat1 Fold10 ## # ℹ 40 more rows # My weird way of creating formulas sometimes, which can be helpful if you&#39;re experimenting with different response variables. form &lt;- paste0(col_y, &#39;~ .&#39;) %&gt;% as.formula() rec_norm &lt;- df_trn %&gt;% recipe(form, data = .) %&gt;% step_normalize(all_predictors()) rec_poly &lt;- rec_norm %&gt;% step_poly(all_predictors()) %&gt;% step_interact(~ all_predictors():all_predictors()) rec_poly Code for recipes… library(rules) library(baguette) f_set &lt;- function(spec) { spec %&gt;% set_mode(&#39;regression&#39;) } spec_lr &lt;- linear_reg(penalty = tune(), mixture = tune()) %&gt;% set_engine(&#39;glmnet&#39;) spec_mars &lt;- mars(prod_degree = tune()) %&gt;% set_engine(&#39;earth&#39;) %&gt;% f_set() spec_svm_r &lt;- svm_rbf(cost = tune(), rbf_sigma = tune()) %&gt;% set_engine(&#39;kernlab&#39;) %&gt;% f_set() spec_svm_p &lt;- svm_poly(cost = tune(), degree = tune()) %&gt;% set_engine(&#39;kernlab&#39;) %&gt;% f_set() spec_knn &lt;- nearest_neighbor( neighbors = tune(), dist_power = tune(), weight_func = tune() ) %&gt;% set_engine(&#39;kknn&#39;) %&gt;% f_set() spec_cart &lt;- decision_tree(cost_complexity = tune(), min_n = tune()) %&gt;% set_engine(&#39;rpart&#39;) %&gt;% f_set() spec_cart_bag &lt;- bag_tree() %&gt;% set_engine(&#39;rpart&#39;, times = 50L) %&gt;% f_set() spec_rf &lt;- rand_forest(mtry = tune(), min_n = tune(), trees = 200L) %&gt;% set_engine(&#39;ranger&#39;) %&gt;% f_set() spec_xgb &lt;- boost_tree( tree_depth = tune(), learn_rate = tune(), loss_reduction = tune(), min_n = tune(), sample_size = tune(), trees = 200L ) %&gt;% set_engine(&#39;xgboost&#39;) %&gt;% f_set() spec_cube &lt;- cubist_rules(committees = tune(), neighbors = tune()) %&gt;% set_engine(&#39;Cubist&#39;) How I felt after creating 10 recipes We can create workflow_sets, combining the recipes that standardizes the predictors with the non-linear models that work best when predictors are all on the same scale. library(workflowsets) sets_norm &lt;- workflow_set( preproc = list(norm = rec_norm), models = list( svm_r = spec_svm_r, svm_p = spec_svm_p, knn = spec_knn ) ) sets_norm ## # A workflow set/tibble: 3 × 4 ## wflow_id info option result ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 norm_svm_r &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 2 norm_svm_p &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 3 norm_knn &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; Let’s apply the quadratic pre-processing to models where it is most applicable. sets_poly &lt;- workflow_set( preproc = list(poly = rec_poly), models = list(lr = spec_lr, knn = spec_knn) ) Finally, there are several recipes that don’t really need pre-processing. Nonetheless, we need to have a preproc step, so we can use workflowsets::workflow_variables() for a dummy pre-processing step. sets_simple &lt;- workflow_set( preproc = list(form), models = list( mars = spec_mars, cart = spec_cart, cart_bag = spec_cart_bag, rf = spec_rf, gb = spec_xgb, cube = spec_cube ) ) sets_simple ## # A workflow set/tibble: 6 × 4 ## wflow_id info option result ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 formula_mars &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 2 formula_cart &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 3 formula_cart_bag &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 4 formula_rf &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 5 formula_gb &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 6 formula_cube &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; We can bind all of our workflow_sets together. sets &lt;- bind_rows(sets_norm, sets_poly, sets_simple) %&gt;% mutate(across(wflow_id, ~str_remove(.x, &#39;^simple_&#39;))) sets ## # A workflow set/tibble: 11 × 4 ## wflow_id info option result ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 norm_svm_r &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 2 norm_svm_p &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 3 norm_knn &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 4 poly_lr &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 5 poly_knn &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 6 formula_mars &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 7 formula_cart &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 8 formula_cart_bag &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 9 formula_rf &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 10 formula_gb &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 11 formula_cube &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; And do the thing! (Observe the elegance.) ctrl_grid &lt;- control_grid( save_pred = TRUE, parallel_over = &#39;everything&#39;, save_workflow = TRUE ) res_grid &lt;- sets %&gt;% workflow_map( seed = seed, resamples = folds, grid = 3, control = ctrl_grid, verbose = TRUE ) How I felt waiting for this to finish running "],["ranking-models.html", "15.3 Ranking models", " 15.3 Ranking models Let’s look at our results # How many models are there? n_model &lt;- res_grid %&gt;% collect_metrics(summarize = FALSE) %&gt;% nrow() n_model ## [1] 3000 res_grid_filt &lt;- res_grid %&gt;% # &#39;cart_bag&#39; has &lt;rsmp[+]&gt; in the `results` column, so it won&#39;t work with `rank_results()` filter(wflow_id != &#39;cart_bag&#39;) # Note that xgboost sucks if you don&#39;t have good parameters res_ranks &lt;- res_grid_filt %&gt;% workflowsets::rank_results(&#39;rmse&#39;) %&gt;% # Why this no filter out rsquared already? filter(.metric == &#39;rmse&#39;) %&gt;% select(wflow_id, model, .config, rmse = mean, rank) %&gt;% group_by(wflow_id) %&gt;% slice_min(rank, with_ties = FALSE) %&gt;% ungroup() %&gt;% arrange(rank) res_ranks ## # A tibble: 11 × 5 ## wflow_id model .config rmse rank ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 norm_svm_p svm_poly Preprocessor1_Model2 0.513 1 ## 2 formula_rf rand_forest Preprocessor1_Model2 0.546 3 ## 3 formula_cart_bag bag_tree Preprocessor1_Model1 0.559 5 ## 4 formula_cube cubist_rules Preprocessor1_Model2 0.567 7 ## 5 formula_mars mars Preprocessor1_Model2 0.570 9 ## 6 formula_gb boost_tree Preprocessor1_Model2 0.580 11 ## 7 norm_knn nearest_neighbor Preprocessor1_Model2 0.587 12 ## 8 poly_knn nearest_neighbor Preprocessor1_Model3 0.674 17 ## 9 formula_cart decision_tree Preprocessor1_Model3 0.689 18 ## 10 poly_lr linear_reg Preprocessor1_Model2 0.768 23 ## 11 norm_svm_r svm_rbf Preprocessor1_Model3 1.03 24 Plot the ranks with standard errors. If we wanted to look at the sub-models for a given wflow_id, we could do that with autoplot(). autoplot( res_grid, id = &#39;norm_svm_p&#39;, metric = &#39;rmse&#39; ) How I feel every time I use autoplot() As shown in the book chapter, this could be a really good use case for finetune::control_race() and workflowsets::workflow_map('tune_race_anova', ...) "],["finalizing-the-model-1.html", "15.4 Finalizing the model", " 15.4 Finalizing the model Now we can finalize our choice of model. wflow_id_best &lt;- res_ranks %&gt;% slice_min(rank, with_ties = FALSE) %&gt;% pull(wflow_id) wf_best &lt;- res_grid %&gt;% extract_workflow_set_result(wflow_id_best) %&gt;% select_best(metric = &#39;rmse&#39;) fit_best &lt;- res_grid %&gt;% extract_workflow(wflow_id_best) %&gt;% finalize_workflow(wf_best) %&gt;% last_fit(split = split) metrics_best &lt;- fit_best %&gt;% collect_metrics() metrics_best ## # A tibble: 2 × 4 ## .metric .estimator .estimate .config ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 rmse standard 0.514 Preprocessor1_Model1 ## 2 rsq standard 0.816 Preprocessor1_Model1 Finally, the canonical observed vs. predicted scatter plot. p_preds &lt;- fit_best %&gt;% collect_predictions() %&gt;% ggplot() + aes(x = !!col_y_sym, y = .pred) + geom_abline(linetype = 2) + # Big cuz we don&#39;t have that many points. geom_point(size = 4) + tune::coord_obs_pred() + labs(x = &#39;observed&#39;, y = &#39;predicted&#39;) p_preds "],["meeting-videos-16.html", "15.5 Meeting Videos", " 15.5 Meeting Videos 15.5.1 Cohort 1 Meeting chat log 00:07:17 Jordan Krogmann: @jon the power of the doge shirt! 00:13:59 Jordan Krogmann: ggpairs is a great package for this 00:14:20 Conor Tompkins: I always want to tilt corr matrices a little bit 00:14:22 Asmae Toumi: eyes emoji 00:14:36 Jon Harmon: 👀 00:26:51 Jon Harmon: https://github.com/tidymodels/workflowsets/pull/48 00:34:23 Jim Gruman: gotta step off … thank you Tony!!! 15.5.2 Cohort 2 Meeting chat log 00:08:13 Luke Shaw: https://www.datakind.org/ 00:11:46 Luke Shaw: https://www.tidyverse.org/blog/2021/05/choose-tidymodels-adventure/ 00:12:54 Luke Shaw: https://www.youtube.com/watch?v=2OfTEakSFXQ 00:44:23 shamsuddeen: Is ok for me too 15.5.3 Cohort 3 Meeting chat log 00:16:29 Federica Gazzelloni: https://www.tidymodels.org/start/models/ 00:17:29 Federica Gazzelloni: https://r4ds.had.co.nz/many-models.html 00:31:46 Daniel Chen (he/him): since there was a talk about &quot;the whole thing&quot; I feel like this set of examples is a good candidate for it all wihtout having to we-write large swaths of things 00:32:54 Ildiko Czeller: exactly! I feel like some of the package articles do a better job showing the &quot;whole&quot; thing at one place than the book 00:34:28 Daniel Chen (he/him): the book is more for learning ml and concepts, but since tidymodels is new (to me) i&#39;m always fighting with how I used to do things or how things are done manually 00:35:17 Daniel Chen (he/him): I also haven&#39;t fit a predictive model since dissertation things. so I know for me personally my confusion and struggle is mainly not actually using it regularly 00:36:57 Ildiko Czeller: for me too I currently do not need predictive models for work so I need to find use cases outside work... which I haven&#39;t done yet but plan to 00:38:32 Ben: I thought it was only me, I can relate to your struggle, its actually a thing. 00:53:30 Ildiko Czeller: workflow_map has a default value fn = &quot;tune_grid&quot;, which I would prefer always write explicityly. it is a sensible default but it is just weird to read the code without it for me. like we map without specifying a function 00:55:24 Daniel Chen (he/him): +1 00:58:39 Daniel Chen (he/him): 40 minutes : | 00:59:00 Daniel Chen (he/him): the verbose = TRUE is probably there so you know it&#39;s doing something. and didn&#39;t just stall 00:59:09 Ildiko Czeller: I wonder which of the 10 models makes it so slow. maybe neural net? 00:59:52 Daniel Chen (he/him): 3min * 12models. that&#39;s not too bad 01:00:29 Ildiko Czeller: hmm there are several models taking several minutes there. interesting. it would probably worth it to set up computing in the cloud on a more powerful computer if it is important. 01:01:18 Ildiko Czeller: yeah it is not too bad to wait once, but if you realise in the end you made a mistake... 01:05:26 Daniel Chen (he/him): that was a good review of the most confusing chapters 01:11:28 Daniel Chen (he/him): i have conference stuff next 2 weeks so i&#39;m +1 for break 15.5.4 Cohort 4 Meeting chat log 00:23:21 Federica Gazzelloni: https://www.tidymodels.org/find/#search-parsnip-models 00:23:27 Federica Gazzelloni: https://baguette.tidymodels.org/ 00:23:57 Federica Gazzelloni: mars Multivariate adaptive regression spline 00:25:21 Ryan Metcalf: [SubModel Optimization](https://www.tmwr.org/grid-search.html#submodel-trick) 00:39:01 Steve C: https://cran.r-project.org/web/packages/Cubist/vignettes/cubist.html 00:39:05 Steve C: cubist 00:57:32 Brandon Hurr: https://stats.oarc.ucla.edu/other/mult-pkg/whatstat/ 01:10:12 Isabella Velásquez: Thank you!! "],["review-of-chapters-10-15.html", "Review of chapters 10-15 ", " Review of chapters 10-15 "],["meeting-videos-17.html", "15.6 Meeting Videos", " 15.6 Meeting Videos 15.6.1 Cohort 1 Meeting chat logs # 2021-05-11 Review 00:26:16 Jon Harmon (jonthegeek): https://www.tmwr.org/recipes.html#a-simple-recipe-for-the-ames-housing-data#GNaFeX:~:text=The%20function%20all_nominal_predictors 00:39:19 Jim Gruman: library(InformationValue) was what we used to use to set the cutoff for glm models 00:40:10 Tony ElHabr: nice 00:42:01 Asmae Toumi: Maybe the changes were the friends we made all along 00:42:03 tan_iphone: Lmao hiya! 00:46:14 tan_iphone: Cookbook! 00:50:02 Asmae Toumi: We could just find cool data sets and kick the shit together 00:50:51 Asmae Toumi: I can volunteer for the first one 00:50:58 Asmae Toumi: It would be on workflow sets and stacks 00:51:11 Asmae Toumi: yessss 00:51:35 Asmae Toumi: Yessssssssss 00:51:46 Asmae Toumi: Together?? 00:52:05 Asmae Toumi: Lmao straight up # 2021-05-18 Q&amp;A 00:09:55 pavitra: your hair looks cute, Julia 00:10:36 Julia Silge: Thank you so much!! My bangs are back 00:24:50 Bryan Shalloway: +1 on that function! 00:27:50 Tony ElHabr: sandwich! 00:27:55 Tony ElHabr: putting all the ingredients together 00:47:13 Kevin Kent: Sounds like a lot of mind mapping (to make sense of all of that) 00:47:30 Jon Harmon (jonthegeek): mind_map() 00:47:48 Kevin Kent: Oh snap, that’s good :) 00:51:15 Julia Silge: https://github.com/tidymodels/dials/issues 00:54:35 Julia Silge: https://github.com/tidymodels/dials/blob/master/.github/CONTRIBUTING.md 00:54:39 Jon Harmon (jonthegeek): https://www.tidymodels.org/contribute/ 00:56:28 Tony ElHabr: did my first pull request at a dev day 00:58:54 Apoorva Srinivasan: For me personally, julia your blogs have helped me learn about the functions in todymodels works the most 00:59:19 Kevin Kent: +1 to that and the screencasts 01:01:23 Julia Silge: Thank you so much! 01:02:33 Kevin Kent: One more question - how often do you expect users to write custom step or model functions for their tidymodels code? For instance in sklearn it seems like there is a strong role for custom transformers and esitmators. 01:04:03 Jon Harmon (jonthegeek): https://www.tidymodels.org/learn/develop/ 01:04:18 Max Kuhn: Gotta go. Thanks! 01:04:22 Kevin Kent: Cool thanks for the explanation 01:05:16 arjun paudel: Any plan on creating recipe for quantile normalizer similar to quantile_transform in sklearn 01:05:34 Jordan Krogmann: Thanks Julia and Max! 01:05:40 Kevin Kent: This was great, thanks so much 01:07:06 Apoorva Srinivasan: Thank you so much!! 01:07:17 Conor Tompkins: Thanks for the talk! 01:07:25 Jonathan Leslie: Thank you! 01:07:39 shahrdad: Thanks a lot Julia and Max 01:08:18 Asmae Toumi: Thanks queen, and thanks king (if you see the chat Max) 01:08:28 Asmae Toumi: Omg, what dataset? Does someone have a link 01:08:36 Asmae Toumi: I NEED THOSE BEANS 01:08:51 Andrew G. Farina: Thank you both, this was great! 01:08:58 Julia Silge: I will send you the beans! 01:09:21 Asmae Toumi: Hahahahahah thank you 01:09:27 Daniel Lupercio: Thank you Julia, have a good evening everyone! "],["dimensionality-reduction.html", "Chapter 16 Dimensionality reduction", " Chapter 16 Dimensionality reduction Learning objectives: Understand recipes Create, prep, and bake recipes outside of a workflow to test or debug the recipes. Understand dimensionality reduction techniques Compare and contrast four dimensionality reduction techniques (techniques used to create a small set of features that capture the main aspects of the original predictor set): Principal component analysis (PCA) Partial least squares (PLS) Independent component analysis (ICA) Uniform manifold approximation and projection (UMAP) Use dimensionality reduction techniques in conjunction with modeling techniques. "],["recipes-without-workflows.html", "16.1 {recipes} without {workflows}", " 16.1 {recipes} without {workflows} "],["why-do-dimensionality-reduction.html", "16.2 Why do dimensionality reduction?", " 16.2 Why do dimensionality reduction? Visualisation and exploratory data analysis: understand the structure of your data Avoid having too many predictors –&gt; improve model performance Linear regression: number of predictors should be less than the number of data points Multicollinearity: independent predictor variables are highly correlated "],["introducing-the-beans-dataset.html", "16.3 Introducing the beans dataset", " 16.3 Introducing the beans dataset Dry bean images (Koklu and Ozkan 2020) Predict bean types from images Features have already been calculated from images of bean samples: area, perimeter, eccentricity, roundness, etc How do these features relate to each other? library(tidymodels) tidymodels_prefer() library(beans) library(corrr) beans_corr &lt;- beans %&gt;% select(-class) %&gt;% # drop non-numeric cols correlate() %&gt;% # generate a correlation matrix in data frame format rearrange() %&gt;% # group highly correlated variables together shave() # shave off the upper triangle # plot the correlation matrix beans_corr %&gt;% rplot(print_cor=TRUE) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) We can see that many features are highly correlated. "],["prepare-the-beans-data-using-recipes.html", "16.4 Prepare the beans data using recipes", " 16.4 Prepare the beans data using recipes Get the ingredients (recipe()): specify the response variable and predictor variables Write the recipe (step_zzz()): define the pre-processing steps, such as imputation, creating dummy variables, scaling, and more Prepare the recipe (prep()): provide a dataset to base each step on (e.g. if one of the steps is to remove variables that only have one unique value, then you need to give it a dataset so it can decide which variables satisfy this criteria to ensure that it is doing the same thing to every dataset you apply it to) Bake the recipe (bake()): apply the pre-processing steps to your datasets Using the recipes package for easy pre-processing library(ggforce) library(bestNormalize) library(learntidymodels) library(embed) set.seed(1701) bean_split &lt;- initial_split(beans, strata = class, prop = 3/4) bean_train &lt;- training(bean_split) bean_test &lt;- testing(bean_split) set.seed(1702) bean_val &lt;- validation_split(bean_train, strata = class, prop = 4/5) bean_val$splits[[1]] ## &lt;Training/Validation/Total&gt; ## &lt;8163/2044/10207&gt; bean_rec &lt;- # Use the training data from the bean_val split object # 1. get the ingredients recipe(class ~ ., data = analysis(bean_val$splits[[1]])) %&gt;% # 2. write the recipe step_zv(all_numeric_predictors()) %&gt;% step_orderNorm(all_numeric_predictors()) %&gt;% step_normalize(all_numeric_predictors()) # 3. prepare the recipe bean_rec_trained &lt;- prep(bean_rec) show_variables &lt;- bean_rec %&gt;% prep(log_changes = TRUE) ## step_zv (zv_SqX2i): same number of columns ## ## step_orderNorm (orderNorm_x4c8K): same number of columns ## ## step_normalize (normalize_NF9ZV): same number of columns bean_validation &lt;- bean_val$splits %&gt;% pluck(1) %&gt;% assessment() # 4. bake the recipe bean_val_processed &lt;- bake(bean_rec_trained, new_data = bean_validation) plot_validation_results &lt;- function(recipe, dat = assessment(bean_val$splits[[1]])) { recipe %&gt;% # Estimate any additional steps prep() %&gt;% # Process the data (the validation set by default) bake(new_data = dat) %&gt;% # Create the scatterplot matrix ggplot(aes(x = .panel_x, y = .panel_y, col = class, fill = class)) + geom_point(alpha = 0.4, size = 0.5) + geom_autodensity(alpha = .3) + facet_matrix(vars(-class), layer.diag = 2) + scale_color_brewer(palette = &quot;Dark2&quot;) + scale_fill_brewer(palette = &quot;Dark2&quot;) } Some examples of recipe steps: step_zv() step_orderNorm() step_normalize() step_dummy() "],["principal-component-analysis-pca.html", "16.5 Principal Component Analysis (PCA)", " 16.5 Principal Component Analysis (PCA) Unsupervised method: acts on the data without any regard for the outcome Finds features that try to account for as much variation as possible in the original data bean_rec_trained %&gt;% step_pca(all_numeric_predictors(), num_comp = 4) %&gt;% plot_validation_results() + ggtitle(&quot;Principal Component Analysis&quot;) We can see the first two components separate the classes well. How do they do this? library(learntidymodels) bean_rec_trained %&gt;% step_pca(all_numeric_predictors(), num_comp = 4) %&gt;% prep() %&gt;% plot_top_loadings(component_number &lt;= 4, n = 5) + scale_fill_brewer(palette = &quot;Paired&quot;) + ggtitle(&quot;Principal Component Analysis&quot;) The predictors contributing to PC1 are all related to size, while PC2 relates to measures of elongation. "],["partial-least-squares-pls.html", "16.6 Partial Least Squares (PLS)", " 16.6 Partial Least Squares (PLS) Supervised: basically PCA, but makes use of the outcome variable Tries to maximise variation in predictors, while also maximising the relationship between these components and the outcome bean_rec_trained %&gt;% step_pls(all_numeric_predictors(), outcome = &quot;class&quot;, num_comp = 4) %&gt;% plot_validation_results() + ggtitle(&quot;Partial Least Squares&quot;) The first two components are very similar to the first two PCA components, but the remaining components are different. Let’s look at the top features for each component: bean_rec_trained %&gt;% step_pls(all_numeric_predictors(), outcome = &quot;class&quot;, num_comp = 4) %&gt;% prep() %&gt;% plot_top_loadings(component_number &lt;= 4, n = 5, type = &quot;pls&quot;) + scale_fill_brewer(palette = &quot;Paired&quot;) + ggtitle(&quot;Partial Least Squares&quot;) Solidity and roundness are the features behind PLS3. "],["independent-component-anysis-ica.html", "16.7 Independent Component Anysis (ICA)", " 16.7 Independent Component Anysis (ICA) Unsupervised Finds components that are statistically independent from each other, rather than uncorrelated Maximise the ‘non-Gaussianity’ of the ICA components - i.e. non-linear # Note: ICA requires the &quot;dimRed&quot; and &quot;fastICA&quot; packages. bean_rec_trained %&gt;% step_ica(all_numeric_predictors(), num_comp = 4) %&gt;% plot_validation_results() + ggtitle(&quot;Independent Component Analysis&quot;) There isn’t much separation between the classes in the first few components, so these independent components don’t separate the bean types. "],["uniform-manifold-approximation-and-projection-umap.html", "16.8 Uniform Manifold Approximation and Projection (UMAP)", " 16.8 Uniform Manifold Approximation and Projection (UMAP) Non-linear, like ICA Powerful: divides the group a lot Uses distance-based nearest neighbor to find local areas where data points are more likely related Creates smaller feature set Unsupervised and supervised versions Can be sensitive to tuning parameters library(embed) bean_rec_trained %&gt;% step_umap(all_numeric_predictors(), num_comp = 4) %&gt;% plot_validation_results() + ggtitle(&quot;UMAP (unsupervised)&quot;) bean_rec_trained %&gt;% step_umap(all_numeric_predictors(), outcome = &quot;class&quot;, num_comp = 4) %&gt;% plot_validation_results() + ggtitle(&quot;UMAP (supervised)&quot;) The supervised method looks to perform better. "],["modeling.html", "16.9 Modeling", " 16.9 Modeling Let’s explore some different models with different dimensionality reduction techniques: single layer neural network, bagged trees, flexible discriminant analysis (FDA), naive Bayes, and regularized discriminant analysis (RDA) (This is slow so I don’t actually run it here.) ctrl &lt;- control_grid(parallel_over = &quot;everything&quot;) bean_res &lt;- workflow_set( preproc = list(basic = class ~., pls = pls_rec, umap = umap_rec), models = list(bayes = bayes_spec, fda = fda_spec, rda = rda_spec, bag = bagging_pec, mlp = mlp_spec) ) %&gt;% workflow_map( verbose = TRUE, seed = 1703, resamples = bean_val, grid = 10, metrics = metric_set(roc_auc) ) rankings &lt;- rank_results(bean_res, select_best = TRUE) %&gt;% mutate(method = map_chr(wflow_id, ~ str_split(.x, &quot;_&quot;, simplify = TRUE)[1])) rankings %&gt;% ggplot(aes(x = rank, y = mean, pch = method, col = model)) + geom_point(cex = 3) + theme(legend.position = &quot;right&quot;) + labs(y = &quot;ROC AUC&quot;) + coord_cartesian(ylim = c(0, 1)) Most models give good performance. Regularized discriminant analysis with PLS seems the best. "],["meeting-videos-18.html", "16.10 Meeting Videos", " 16.10 Meeting Videos 16.10.1 Cohort 1 Meeting chat log 00:24:23 Daniel Chen (he/him): PCA maximizes the variance 00:41:01 Daniel Chen (he/him): I guess it depends on what you&#39;re using it for? like for a visualization or using PCA for feature engineering 00:42:01 Daniel Chen (he/him): how useful is tuning the number of PCs? I&#39;ve always looked at elbow plots or something for that stuff? 00:45:45 Daniel Chen (he/him): run pca after LASSO! :p 00:45:52 Daniel Chen (he/him): wait. that doesn&#39;t make sense 00:45:53 Daniel Chen (he/him): nvm 00:49:15 Daniel Chen (he/him): kind of surprised they didn&#39;t show other MDS (multi dimensional scaling) examples since PCA is a special case of MDS 00:50:14 Daniel Chen (he/him): if you want to give names to &quot;loadings&quot; you&#39;d use factor analysis 00:52:44 Jim Gruman: thank you Jon!! 00:53:13 Daniel Chen (he/him): bye everyone! 16.10.2 Cohort 3 Meeting chat log 00:39:34 Ildiko Czeller: do you know from where the function name plot_top_loadings comes? I have not heard the term loading in the context of PCA before. As far as I understand it plots top most contributing variables/features in each PCA component 00:40:00 Jiwan Heo: loading is the &quot;weights&quot; in PCA 00:40:46 Jiwan Heo: the coefficient of the linear combination of variables 00:40:56 Ildiko Czeller: ahh, makes sense, thanks! 00:56:04 Ildiko Czeller: I guess difference between PCA and PLS would be bigger if there were some rubbish features as well with high variance but without much predicting power 00:56:21 Jiwan Heo: PLS is supervised, PCA is unsupervised 00:56:25 Ildiko Czeller: the first 2 components seem to be basically mirror images of each other 00:57:38 Jiwan Heo: rubbish features would not get picked up, i&#39;d imagine. If it doesn&#39;t impact the outcome 00:58:30 Ildiko Czeller: yeah, I think that they would not be picked in PLS at all, but might be picked up by PCA because it is unsupervised (?) 00:59:15 Jiwan Heo: I think so. PCA just picks up any large variance, but in PLS, it has to also move the outcome in some way 01:04:16 Jiwan Heo: sorry I have to jump off! Thank you for the presentation :) 01:05:45 Ildiko Czeller: To build an intuition about UMAP i found this interactive website very useful: https://pair-code.github.io/understanding-umap/ 16.10.3 Cohort 4 Meeting chat log 01:10:50 Isabella Velásquez: Gotta go, but thank you! 01:12:12 Federica Gazzelloni: https://docs.google.com/spreadsheets/d/1-S1UbKWay_TeR5n9LkztZY2XXrMjZr3snl1srPvTvH4/edit#gid=0 "],["encoding-categorical-data.html", "Chapter 17 Encoding categorical data", " Chapter 17 Encoding categorical data Learning objectives: transformation to a numeric representation for categorical data options for encoding categorical predictors when is encoding data necessary? "],["effect-of-encoding.html", "17.1 Effect of Encoding", " 17.1 Effect of Encoding We use the {embed} and {textrecipes} packages for transformation of the categorical data to a numeric version. Tree based models and Naive Bayes models deal with categorical data making the encoding. Methods for encoding categorical variables into numerical can be done by applying polynomial transformations. In tidymodels there are some step_functions such as: step_unorder() step_ordinalscore() used for assigning to each order in the categorical vector a specific numerical value. Categorical variables can be ordered and unordered, when in presence of a high number of categories, fundamental is the categorization of the levels and this can be challenging, for the result of predictions. In particular, issues arise when infinite values, invalid values, NA, too many categorical levels, rare categorical levels, or new categorical levels, are the values we want to encode. 3 Source: Processor for Predictive Modeling↩︎ "],["encoding-methods.html", "17.2 Encoding methods:", " 17.2 Encoding methods: Effect or likelihood encodings —-&gt; No Pooling and Partial Pooling “you create an effect encoding for your categorical variable” This can be seen when the transformation happens between the levels of the categorical variable and another numerical variable in the set. An example would be: “These steps use a generalized linear model to estimate the effect of each level in a categorical predictor on the outcome.” lencode stands for linear encoding step_lencode_glm() —-&gt; mixed or hierarchical generalized linear model step_lencode_mixed() —-&gt; partial pooling step_lencode_bayes() —-&gt; Bayesian hierarchical model The {embed} package documentation provides some more detailed information about different types of step_ that can be used. pooling —&gt; we shrink the effect estimates toward the mean Feature hashing Create dummy variables, but only consider the value of the category to assign it to a predefined pool of dummy variables. It is for text data and high cardinality. rlang::hash() mutate(Hash = map_chr(..categorical.., hash)) Neighborhoods are called the “keys”, while the outputs are the “hashes”. The number of possible hashes can be customized as it is a hyperparameter. strtoi() —-&gt; Convert Strings to Integers mutate(Hash = strtoi(substr(Hash, 26, 32), base = 16L), Hash = Hash %% 16) Entity embeddings To transform a categorical variable with many levels to a set of lower-dimensional vectors. Embeddings is learned via a TensorFlow neural network. step_embed() —-&gt; TensorFlow neural network step_woe() —-&gt; weight of evidence transformation-Bayes factor 17.2.1 Cohort 4 Meeting chat log 00:44:41 Stephen.Charlesworth: https://www.amazon.com/Machine-Learning-Design-Patterns-Preparation/dp/1098115783 00:50:20 Federica Gazzelloni: https://dl.acm.org/doi/10.1145/507533.507538 00:52:42 Federica Gazzelloni: https://arxiv.org/abs/1611.09477 00:53:26 Stephen.Charlesworth: https://community.tibco.com/feed-items/comparison-different-encoding-methods-using-tibco-data-science "],["explaining-models-and-predictions.html", "Chapter 18 Explaining models and predictions", " Chapter 18 Explaining models and predictions Learning objectives: Recognize some R packages for model explanations. Use {DALEX} and {DALEXtra} to produce local model explanations for a model trained using {tidymodels}. Use {DALEX} and {DALEXtra} to produce global model explanations for a model trained using {tidymodels}. Use {DALEX} and {DALEXtra} to produce partial dependence profiles for a model trained using {tidymodels}. "],["chapter-18-setup.html", "18.1 Chapter 18 Setup", " 18.1 Chapter 18 Setup Load in the data and set up explainer library(tidymodels) library(skimr) library(DALEX) library(DALEXtra) library(iBreakDown) rush_model &lt;- readRDS(here::here(&quot;data&quot;, &quot;18-fit_rush_yards.RDS&quot;)) rush_df &lt;- readRDS(here::here(&quot;data&quot;, &quot;18-nfl_rush_df.RDS&quot;)) skim(rush_df) Table 18.1: Data summary Name rush_df Number of rows 95186 Number of columns 40 _______________________ Column type frequency: character 9 factor 15 numeric 16 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace posteam_type 0 1 4 4 0 2 0 game_wday 0 1 3 5 0 4 0 game_half 0 1 5 8 0 3 0 run_location 0 1 4 6 0 3 0 run_gap 0 1 3 6 0 3 0 run_gap_dir 0 1 8 12 0 7 0 surface 0 1 4 5 0 2 0 roof 0 1 4 8 0 4 0 position 0 1 2 2 0 4 0 Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts season 0 1 TRUE 7 202: 13935, 201: 13792, 201: 13726, 201: 13517 week 0 1 TRUE 21 15: 5831, 17: 5815, 14: 5774, 3: 5687 score 0 1 FALSE 2 0: 91955, 1: 3231 first_down 0 1 FALSE 2 0: 71845, 1: 23341 game_month 0 1 TRUE 4 12: 31623, 10: 22281, 11: 22142, 9: 19140 game_week 0 1 TRUE 18 53: 7245, 50: 5911, 38: 5753, 51: 5667 game_time 0 1 TRUE 10 13: 49795, 16: 24222, 20: 17960, 18: 793 qtr 0 1 TRUE 5 1: 24079, 4: 23911, 2: 23315, 3: 23270 down 0 1 TRUE 4 1: 52324, 2: 31524, 3: 9908, 4: 1430 goal_to_go 0 1 FALSE 2 0: 88458, 1: 6728 shotgun 0 1 FALSE 2 0: 58260, 1: 36926 no_huddle 0 1 FALSE 2 0: 87972, 1: 7214 qb_dropback 0 1 FALSE 2 0: 89705, 1: 5481 qb_scramble 0 1 FALSE 2 0: 89705, 1: 5481 two_point_attempt 0 1 FALSE 2 0: 95006, 1: 180 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist rushing_yards 0 1 4.43 6.39 -17.00 1.00 3.00 6.00 99.00 ▇▂▁▁▁ rushing_fantasy_points 0 1 0.63 1.33 -3.70 0.10 0.30 0.60 15.90 ▆▇▁▁▁ wind 0 1 6.06 5.62 0.00 0.00 6.00 9.00 71.00 ▇▁▁▁▁ temp 0 1 60.98 15.24 -6.00 52.00 67.00 68.00 97.00 ▁▁▃▇▂ rusher_age 0 1 25.96 3.17 20.95 23.57 25.29 27.76 43.46 ▇▆▂▁▁ yardline_100 0 1 51.03 25.47 1.00 31.00 55.00 73.00 99.00 ▅▅▆▇▃ quarter_seconds_remaining 0 1 461.02 263.99 0.00 228.00 458.00 692.00 900.00 ▇▇▇▇▇ half_seconds_remaining 0 1 908.72 526.68 0.00 445.00 900.00 1367.00 1800.00 ▇▇▇▇▇ game_seconds_remaining 0 1 1804.95 1053.60 0.00 896.00 1800.00 2713.00 3600.00 ▇▇▇▇▇ fixed_drive 0 1 11.38 6.90 1.00 5.00 11.00 17.00 38.00 ▇▆▆▁▁ drive_play_count 0 1 7.86 3.58 0.00 5.00 8.00 10.00 21.00 ▅▇▆▂▁ ydstogo 0 1 8.14 3.88 0.00 5.00 10.00 10.00 46.00 ▅▇▁▁▁ score_differential 0 1 0.47 10.70 -56.00 -6.00 0.00 7.00 52.00 ▁▁▇▂▁ ep 0 1 2.35 1.72 -2.92 1.00 2.14 3.62 6.59 ▁▅▇▅▂ vegas_wp 0 1 0.57 0.31 0.00 0.30 0.60 0.85 1.00 ▅▅▅▅▇ total_line 0 1 45.75 4.25 35.00 43.00 45.50 48.50 63.50 ▂▇▆▁▁ explainer_boost &lt;- explain_tidymodels( rush_model, data = rush_df, y = rush_df$rushing_yards, verbose = TRUE ) ## Preparation of a new explainer is initiated ## -&gt; model label : workflow ( default ) ## -&gt; data : 95186 rows 40 cols ## -&gt; data : tibble converted into a data.frame ## -&gt; target variable : 95186 values ## -&gt; predict function : yhat.workflow will be used ( default ) ## -&gt; predicted values : No value for predict function target column. ( default ) ## -&gt; model_info : package tidymodels , ver. 1.1.1 , task regression ( default ) ## -&gt; predicted values : the predict_function returns an error when executed ( WARNING ) ## -&gt; residual function : difference between y and yhat ( default ) ## -&gt; residuals : the residual_function returns an error when executed ( WARNING ) ## A new explainer has been created! "],["overview.html", "18.2 Overview", " 18.2 Overview "],["local-explanations.html", "18.3 Local Explanations", " 18.3 Local Explanations Provides information about a prediction for a single observation Which variables contribute to this result the most? “Break-down” explanations compute the contribution from each feature Results for many explanatory variables can be presented in a limited space Only the additive attributions, misleading for models with interactions Break-down plots with interactions More accurate if the model itself uses interactions Much more time-consuming Interactions is not based on any formal statistical-significance test SHapley Additive exPlanations (SHAP) are based on “Shapley values” “Cooperation is beneficial, because it may bring more benefit than individual actions” Decompose a model’s predictions into contributions that can be attributed additively to different explanatory variables If the model is not additive, then the Shapley values may be misleading #Break-down boost_breakdown &lt;- predict_parts(explainer = explainer_boost, new_observation = sample_n(rush_df,1)) png(file=&quot;images/18_boost_breakdown.png&quot;, width = 600) plot(boost_breakdown) dev.off() #Break-dwon Interactions boost_breakdown2 &lt;- predict_parts(explainer = explainer_boost, new_observation = sample_n(rush_df,1), type = &quot;break_down_interactions&quot;) png(file=&quot;images/18_boost_breakdown2.png&quot;, width = 600) plot(boost_breakdown2) dev.off() #SHAP boost_breakdown3 &lt;- predict_parts(explainer = explainer_boost, new_observation = sample_n(rush_df,1), type = &quot;shap&quot;) png(file=&quot;images/18_boost_breakdown3.png&quot;, width = 600) plot(boost_breakdown3) dev.off() "],["local-explanations-for-interactions.html", "18.4 Local Explanations for Interactions", " 18.4 Local Explanations for Interactions “Ceteris-paribus” profiles show how a model’s prediction would change if the value of a single exploratory variable changed Graphical representation is easy to understand and explain Not a valid assumption with highly correlated or interaction variables #Ceterus Paribus boost_paribus &lt;- predict_profile(explainer = explainer_boost, new_observation = sample_n(rush_df,1), variables = c(&quot;rusher_age&quot;, &quot;yardline_100&quot;)) png(file=&quot;images/18_boost_paribus.png&quot;) plot(boost_paribus, variables = c(&quot;rusher_age&quot;)) dev.off() png(file=&quot;images/18_boost_paribus2.png&quot;) plot(boost_paribus, variables = c(&quot;yardline_100&quot;)) dev.off() "],["global-explanations.html", "18.5 Global Explanations", " 18.5 Global Explanations Which features are most important in driving the predictions aggregated over the whole training set Measure how much does a model’s performance change if the effect of a selected explanatory variable(s) is(are) removed If variables are correlated, then models like random forest are expected to spread importance across many variables Dependent on the random nature of the permutations boost_vip &lt;- model_parts(explainer_boost, loss_function = loss_root_mean_square) png(file=&quot;images/18_boost_vip.png&quot;) plot(boost_vip, max_featuers = 10) dev.off() "],["global-explanations-from-local-explanations.html", "18.6 Global Explanations from Local Explanations", " 18.6 Global Explanations from Local Explanations Partial-dependence plots How does the expected value of model prediction behave as a function of a selected explanatory variable? PD profiles are averages of CP profiles Problematic for correlated explanatory variables boost_profile &lt;- model_profile(explainer_boost, N = 1000, variables = &quot;rusher_age&quot;, groups = &quot;position&quot;) png(file=&quot;images/18_boost_profile.png&quot;) plot(boost_profile) dev.off() "],["references-3.html", "18.7 References", " 18.7 References DALEX Github DALEXtra Github Exploratory Model Anaylsis "],["meeting-videos-19.html", "18.8 Meeting Videos", " 18.8 Meeting Videos 18.8.1 Cohort 1 Meeting chat log 00:03:12 tan_iphone: Hullo! 00:05:15 tan_iphone: I seem to have influenced the ballot ok tho! 00:06:20 tan_iphone: Boaty mc boat dog 00:13:56 Jon Harmon (jonthegeek): https://cran.r-project.org/package=nflfastR 00:17:17 Tony ElHabr: Acronyms are my fav 00:17:21 Tony ElHabr: *bacronyms 00:17:31 Jon Harmon (jonthegeek): I think it stands for &quot;I&#39;m a Doctor Who fan.&quot; 00:17:59 Jordan Krogmann: they have the death bots as the logo 00:18:10 Jordan Krogmann: or whatever... I am a casual doctor fan 00:18:28 Jon Harmon (jonthegeek): https://dalex.drwhy.ai/ 00:24:26 Jon Harmon (jonthegeek): Rowers in a boat is the explanation I&#39;ve heard a lot. You can&#39;t just be the fastest, you need to also be in sync with the other rowers. 00:24:52 Tony ElHabr: Is the default shapley? Or is it permutation? And what’s the difference? 00:31:19 Jon Harmon (jonthegeek): Dunno about anybody else but I had to look up what that meant: ceteris paribus = &quot;all other things being equal.&quot; 00:31:38 Tony ElHabr: This is the same as partial dependence plots? 00:32:45 Tony ElHabr: Oh, CP is the instance-level equivalent of PDP (used for data-set level) 00:33:46 Jon Harmon (jonthegeek): &quot;A profile showing how an individual observation’s prediction changes as a function of a given feature is called an ICE (individual conditional expectation) profile or a CP (ceteris paribus) profile.&quot; Yeah, they never definitively explain PDP vs these other terms. I hate to make them rebuild this monster chapter but I think I have thoughts about making some stuff in here clearer! 00:37:52 Tony ElHabr: i don’t think vip supports catboost 😢 00:39:04 Jon Harmon (jonthegeek): Wait, do you pronounce it &quot;vip&quot; or &quot;V I P&quot;? 00:39:20 Tony ElHabr: Yeah v i p 00:39:46 tan_iphone: very important predictor 00:40:04 tan_iphone: But wait is there a prequel? 00:40:56 Tony ElHabr: meme (tweet) for the day: https://twitter.com/benbbaldwin/status/1431300791516663816?s=20 00:42:08 Jon Harmon (jonthegeek): Dude, you can&#39;t send something that requires audio in the middle of a meeting 🙃 01:05:41 tan_iphone: Lmaoooo I have not nearly read enough of the book to do it 01:05:47 tan_iphone: Currently at the gym :) cheers folks 18.8.2 Cohort 3 18.8.3 Cohort 4 Meeting chat log 00:22:02 Federica Gazzelloni: this is interesting: “ game theory techniques like SHAP are able to provide insight into how much crime is “good,” and how much is “too much.” (https://geophy.com/insights/whats-the-shap-how-crime-influences-property-value/) 00:26:51 Federica Gazzelloni: example of using shap: 00:26:56 Federica Gazzelloni: https://juliasilge.com/blog/board-games/ "],["when-should-you-trust-predictions.html", "Chapter 19 When should you trust predictions?", " Chapter 19 When should you trust predictions? Learning objectives: Use the {probably} package to create an equivocal zone to improve model accuracy. Describe the trade-off between accuracy and reportability. Use the {applicable} package to quantify how applicable a model is to new data. "],["equivocal-results.html", "19.1 Equivocal Results", " 19.1 Equivocal Results What the heck is this? Loosely, equivacol results are a range of results indicating that the prediction should not be reported. Let’s take a soccer example. (Data courtesy of 538.) df &lt;- here::here(&#39;data&#39;, &#39;spi_matches_latest.csv&#39;) %&gt;% read_csv() %&gt;% drop_na(score1) %&gt;% mutate(w1 = ifelse(score1 &gt; score2, &#39;yes&#39;, &#39;no&#39;) %&gt;% factor()) %&gt;% select( w1, season, matches(&#39;(team|spi|prob|importance)[12]&#39;), probtie ) %&gt;% # we need this as feature, so it can&#39;t be NA drop_na(importance1) # much more data in 2021 season than in older seasons, so use older seasons as test set trn &lt;- df %&gt;% filter(season == 2021) %&gt;% select(-season) tst &lt;- df %&gt;% filter(season != 2021) %&gt;% select(-season) trn %&gt;% count(w1) ## # A tibble: 2 × 2 ## w1 n ## &lt;fct&gt; &lt;int&gt; ## 1 no 1376 ## 2 yes 1054 tst %&gt;% count(w1) ## # A tibble: 2 × 2 ## w1 n ## &lt;fct&gt; &lt;int&gt; ## 1 no 248 ## 2 yes 227 fit &lt;- logistic_reg() %&gt;% set_engine(&#39;glm&#39;) %&gt;% # 2 variables cuz dumb humans like 2-D plots fit(w1 ~ spi1 + importance1, data = trn) fit %&gt;% tidy() ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -0.742 0.107 -6.92 4.50e-12 ## 2 spi1 0.0100 0.00259 3.86 1.13e- 4 ## 3 importance1 0.00381 0.00250 1.52 1.27e- 1 predict_stuff &lt;- function(fit, set) { bind_cols( fit %&gt;% predict(set), fit %&gt;% predict(set, type = &#39;prob&#39;), fit %&gt;% predict(set, type = &#39;conf_int&#39;, std_error = TRUE), set ) } preds_tst &lt;- fit %&gt;% predict_stuff(tst) How’s the model accuracy? preds_tst %&gt;% accuracy(estimate = .pred_class, truth = w1) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.579 Seems reasonable… so maybe modeling soccer match outcomes with 2 variables ain’t so bad, eh? Observe the fitted class boundary. (Confidence intervals shown instead of prediction intervals because I didn’t want to use stan… Max please forgive me.) Use the {probably} package! lvls &lt;- levels(preds_tst$w1) preds_tst_eqz &lt;- preds_tst %&gt;% mutate(.pred_with_eqz = make_two_class_pred(.pred_yes, lvls, buffer = 0.025)) preds_tst_eqz %&gt;% count(.pred_with_eqz) ## # A tibble: 3 × 2 ## .pred_with_eqz n ## &lt;clss_prd&gt; &lt;int&gt; ## 1 [EQ] 50 ## 2 no 48 ## 3 yes 377 Look at how make_two_class_pred changes our confusion matrix. # All data preds_tst_eqz %&gt;% conf_mat(w1, .pred_class) %&gt;% autoplot(&#39;heatmap&#39;) # Reportable results only preds_tst_eqz %&gt;% conf_mat(w1, .pred_with_eqz) %&gt;% autoplot(&#39;heatmap&#39;) Does the equivocal zone help improve accuracy? How sensitive is accuracy and our reportable rate to the width of the buffer? eq_zone_results &lt;- function(buffer) { preds_tst_eqz &lt;- preds_tst %&gt;% mutate(.pred_with_eqz = make_two_class_pred(.pred_no, lvls, buffer = buffer)) acc &lt;- preds_tst_eqz %&gt;% accuracy(w1, .pred_with_eqz) rep_rate &lt;- reportable_rate(preds_tst_eqz$.pred_with_eqz) tibble(accuracy = acc$.estimate, reportable = rep_rate, buffer = buffer) } map_dfr(seq(0, 0.15, length.out = 40), eq_zone_results) %&gt;% pivot_longer(c(-buffer)) %&gt;% ggplot(aes(x = buffer, y = value, col = name)) + geom_step(size = 1.2, alpha = 0.8) + labs(y = NULL) How does the standard error look across the feature space Makes sense… we’re more uncertain for cases outside of the normal boundary of our data. "],["model-applicability.html", "19.2 Model Applicability", " 19.2 Model Applicability Let’s stress-test a model, seeing how it might work on some unusual observations. For this, we fit a new model with pre-game team 1 probability of winning (prob1) and pre-game probability of a draw (probtie). (We can better illustrate an extreme example with these features.) fit2 &lt;- logistic_reg() %&gt;% set_engine(&#39;glm&#39;) %&gt;% fit(w1 ~ prob1 + probtie, data = trn) fit2 %&gt;% tidy() ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -2.25 0.440 -5.12 3.10e- 7 ## 2 prob1 4.48 0.357 12.6 3.59e-36 ## 3 probtie 0.116 1.36 0.0852 9.32e- 1 How’s the accuracy looking? preds_tst2 &lt;- fit2 %&gt;% predict_stuff(tst) preds_tst2 %&gt;% accuracy(estimate = .pred_class, truth = w1) ## # A tibble: 1 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.604 preds_tst2 %&gt;% conf_mat(w1, .pred_class) %&gt;% autoplot(&#39;heatmap&#39;) Not bad… but is it deceiving in some extreme cases? Note that this model is pretty confident even for weird combinations like probtie = 0.5 and prob1 = 0.5 (implying that the other team has 0% chance of winning). Can we identify how applicable the model is for any new prediction (a.k.a the model’s applicability domain)? Let’s use PCA to do so. The PCA scores for the training set are shown in panel (b). Next, using these results, we measure the distance of each training set point to the center of the PCA data (panel (c)). We can then use this reference distribution (panel (d)) to estimate how far away a data point is from the mainstream of the training data. So, how can we use this PCA suff? Well, we can compute distances and percentiles based on those distances. The plot below overlays an average testing set sample (in blue) and a rather extreme sample (in red) with the PCA distances from the training set. Let’s use the {applicable} package! (We’ll include more features this time around.) pca_stat &lt;- apd_pca(~ ., data = trn %&gt;% select(where(is.numeric)), threshold = 0.99) pca_stat ## # Predictors: ## 7 ## # Principal Components: ## 5 components were needed ## to capture at least 99% of the ## total variation in the predictors. We can plot a CDF looking thing with our computed distances. autoplot(pca_stat, distance) Observe that a strange observation gets a very high distance and 100 distance_pctl. score( pca_stat, bind_rows( tibble( # set these to pretty average values spi1 = 40, spi2 = 40, importance1 = 100/3, importance2 = 100/3, # set these to weird values prob1 = 0.1, prob2 = 0.1, probtie = 0.8 ), tst ) ) %&gt;% select(starts_with(&quot;distance&quot;)) ## # A tibble: 476 × 2 ## distance distance_pctl ## &lt;dbl&gt; &lt;dbl&gt; ## 1 14.9 1 ## 2 1.93 39.6 ## 3 3.27 84.1 ## 4 5.31 96.6 ## 5 3.04 80.4 ## 6 0.569 0.772 ## 7 1.05 6.62 ## 8 1.57 22.6 ## 9 2.56 67.2 ## 10 5.76 98.1 ## # ℹ 466 more rows "],["meeting-videos-20.html", "19.3 Meeting Videos", " 19.3 Meeting Videos 19.3.1 Cohort 1 19.3.2 Cohort 3 Meeting chat log 00:12:14 Daniel Chen: here isn&#39;t an echo on my end. not relaly 00:26:36 Daniel Chen: so far we&#39;ve created a simulated set of values with random noise right? 00:26:52 Ildiko Czeller: yes 00:27:13 Ildiko Czeller: for a classification problem 00:27:13 Daniel Chen: and now we&#39;re fitting a Bayesian model on data? am I following that correctly? 00:27:33 Daniel Chen: so the packages mentioned in the beginning is for Bayesian stuff? 00:27:53 Ildiko Czeller: yes, i think stan does bayesian prediction. 00:28:24 Ildiko Czeller: probably will compute tthe equivocal zones later if you mean that by the package mentioned in the beginning 00:28:32 Daniel Chen: i mean this chapter as a whole is using Bayesian models to see how much we should &quot;trust&quot; predictions 00:28:52 Daniel Chen: yeah. ok. so this is all Bayesian specific stuff? yes stan is for Bayesian stuff 00:29:18 Ildiko Czeller: equivocal zones can be calculated for non bayesian models as well I think 00:29:45 Daniel Chen: what&#39;s the data_grid? i think i just missed it 00:30:58 Ildiko Czeller: i think it is just your simulated dataset with x, y as predictors, isn&#39;t it? 00:31:29 Daniel Chen: oh it looks like the predicted values? classes and probablilties. similar to inputs used for yardstick 00:31:33 Ildiko Czeller: the equivocal zones they are not so sophisticated for bayesian models in my understanding 00:31:44 Daniel Chen: oh no. yeah it looks like simulated data 00:32:16 Ildiko Czeller: I meant for NON bayesian models they are less sophisticated 01:02:56 Daniel Chen: can you go back to what the pca_stat value is when being compared to the Chicago data? 01:11:23 Daniel Chen: what&#39;s the pca stat values? 01:11:36 Daniel Chen: why is it 1 column of values when you have multiple PCs? 01:13:00 Daniel Chen: this is in the score function call. 01:13:43 Daniel Chen: oh it&#39;s distance from center 01:15:00 Daniel Chen: oooh it&#39;s all 9 01:15:14 Daniel Chen: ok that part makes sense that&#39;s regular PCA results 01:15:45 Daniel Chen: yeah that&#39;s good. 01:16:39 Daniel Chen: i might be moving next 2 weeks. 01:16:45 Daniel Chen: so i might not be availviale 01:17:11 Federica Gazzelloni: thanks 19.3.3 Cohort 4 Meeting chat log 00:26:35 Federica Gazzelloni: The reportable rate is calculated as (n_not_equivocal / n). "],["ensembles-of-models.html", "Chapter 20 Ensembles of models", " Chapter 20 Ensembles of models Learning objectives: Create a data stack for the {stacks} package using stacks() and add_candidates. Fit a meta-learning model using blend_predictions(). Fit the member models using fit_members(). Test the results of an ensemble model. "],["ensembling.html", "20.1 Ensembling", " 20.1 Ensembling Aggregating predictions of multiple models together to make one prediction. We’ve already seen some ensembling within 1 type of model: Random forest Bagging &amp; boosting Model stacking allows for aggregating many different types of models (lm + rf + svm, …), to assemble a new model, which generates a new prediction, informed by its members. "],["ensembling-with-stacks.html", "20.2 Ensembling with stacks!", " 20.2 Ensembling with stacks! https://stacks.tidymodels.org/articles/basics.html Model stacking process: Define some models, using all the knowledge we have from this book. We call them candidate models/members. Initialize the ensemble with stacks::stacks(), and add the members to it, using stacks::add_members() Blend the predictions of the members, using stacks::blend_predictions() (linear combination of each member’s predictions) Now that we know how to blend members, fit the members one more time on the whole training set, and predict on testing set. "],["define-some-models.html", "20.3 Define some models", " 20.3 Define some models Ensembles are formed from model definitions, which are workflows that contain model recipe &amp; spec (that has been tuned or fit_resample’d). The book recommends the racing tuning method. You’ll need to save the assessment set predictions and workflow utilized in your tune_grid(), or fit_resamples() objects by setting the control arguments save_pred = TRUE and save_workflow = TRUE. If a model has hyperparameters, then you’d be creating multiple candidate models. "],["initialize-and-add-members-to-stack..html", "20.4 Initialize and add members to stack.", " 20.4 Initialize and add members to stack. Data stacks are tibbles (with some extra attributes) that contain the response value as the first column, and the assessment set predictions for each candidate ensemble member. tree_frogs_model_st &lt;- # example from stacks vignette stacks() %&gt;% add_candidates(knn_res) %&gt;% add_candidates(lin_reg_res) %&gt;% add_candidates(svm_res) tree_frogs_model_st #&gt; # A data stack with 3 model definitions and 11 candidate members: #&gt; # knn_res: 4 model configurations #&gt; # lin_reg_res: 1 model configuration #&gt; # svm_res: 6 model configurations #&gt; # Outcome: latency (numeric) as_tibble(tree_frogs_model_st) #&gt; # A tibble: 429 x 12 #&gt; latency knn_res_1_1 knn_res_1_2 knn_res_1_3 knn_res_1_4 lin_reg_res_1_1 #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 142 -0.496 -0.478 -0.492 -0.494 114. #&gt; 2 79 -0.381 -0.446 -0.542 -0.553 78.6 #&gt; 3 50 -0.311 -0.352 -0.431 -0.438 81.5 #&gt; 4 68 -0.312 -0.368 -0.463 -0.473 78.6 #&gt; 5 64 -0.496 -0.478 -0.492 -0.494 36.5 #&gt; 6 52 -0.391 -0.412 -0.473 -0.482 124. #&gt; 7 39 -0.523 -0.549 -0.581 -0.587 35.2 #&gt; 8 46 -0.523 -0.549 -0.581 -0.587 37.1 #&gt; 9 137 -0.287 -0.352 -0.447 -0.456 78.8 #&gt; 10 73 -0.523 -0.549 -0.581 -0.587 38.8 #&gt; # … with 419 more rows, and 6 more variables: svm_res_1_1 &lt;dbl&gt;, #&gt; # svm_res_1_4 &lt;dbl&gt;, svm_res_1_3 &lt;dbl&gt;, svm_res_1_5 &lt;dbl&gt;, svm_res_1_2 &lt;dbl&gt;, #&gt; # svm_res_1_6 &lt;dbl&gt; "],["blend-fit-predict.html", "20.5 Blend, fit, predict", " 20.5 Blend, fit, predict blend_predictions() performs LASSO regularization to combine the outputs from the stack members to come up with one final prediction. Candidates with non-zero coefficients are kept. tree_frogs_model_st &lt;- tree_frogs_data_st %&gt;% blend_predictions() There’s an autoplot() function available, to see what’s going on. If you don’t like what you’re seeing, you can try blend_predictions() again, and setting your own penalty argument. Essentially, what you have, is a linear combination of each member’s prediction, to create one final prediction. With this “instruction” on how to combine candidate models, we fit the whole training set tree_frogs_model_st &lt;- tree_frogs_model_st %&gt;% fit_members() And predict on testing set tree_frogs_test &lt;- tree_frogs_test %&gt;% bind_cols(predict(tree_frogs_model_st, .)) "],["meeting-videos-21.html", "20.6 Meeting Videos", " 20.6 Meeting Videos 20.6.1 Cohort 1 Meeting chat log # 2021-05-25 Preview with Asmae Toumi 00:07:45 tan-iphone: timed with an audience 00:08:00 tan-iphone: did you get the cursive IDE set up? 00:11:00 Tony ElHabr: yup! victor mono is the secret 00:18:44 tan-iphone: foreach 00:18:47 tan-iphone: or future or something 00:18:53 Marschall Furman (he/him): parallel package 00:19:55 Jim Gruman: all_cores &lt;- parallelly::availableCores(omit = 1) future::plan(&quot;multisession&quot;, workers = all_cores) # on Windows 00:47:55 tan-iphone: &quot;you&#39;ll learn when you grow up&quot; # 2021-06-01 Preview with Tony ElHabr 00:08:56 Jon Harmon: Like tensorflow but very few insane installation issues: https://github.com/mlverse/torch 00:23:46 Daryn Ramsden: Is it multinom_reg because there’s a logistic regression under the hood? 00:25:18 Jon Harmon: I think so, as far as google is telling me. 00:27:19 Asmae Toumi: Max’s quote: “One other note that is in the upcoming ensemble chapter about racing and stacking... the stack can only use the candidates that had the complete set of resamples done. With racing, it works fast by avoiding using all of the resamples for candidates that show poor performance. If the racing method quickly gets down to one tuning parameter set, that is the only candidate that can be added to the data stack (since it has the whole set of resamples.” 00:27:26 Pavitra: Why is there a rank jump from 2 to 12? 00:27:51 Jon Harmon: By model. It&#39;s showing the top 2 of each. 00:28:00 Daryn Ramsden: ohhhh 00:28:35 Jon Harmon: Er not by model per se but by... some combo.... 00:28:41 Daryn Ramsden: So models ranked 3-11 are also rf? 00:31:58 Asmae Toumi: i’d be curious to see the stacks results without tune race anova 00:33:16 Jon Harmon: beepr always reminds me of this silliness that I had running a year and a half ago or so when Max had mentioned {tune} somewhere but the repo was still private... and it exactly solved something I was working on so I wanted to know as soon as it was public: tune_check &lt;- httr::GET( url = &quot;https://github.com/tidymodels/tune&quot; ) while (httr::status_code(tune_check) == 404) { Sys.sleep(15*60) tune_check &lt;- httr::GET( url = &quot;https://github.com/tidymodels/tune&quot; ) } beepr::beep(8) 00:58:21 Asmae Toumi: Tony hive forever 00:58:48 Asmae Toumi: Can you send the code and data? I want to understand it better too 01:00:11 Pavitra: Ledell 01:00:13 Asmae Toumi: Erin LeDell is a legend 01:00:22 Asmae Toumi: It was so funny 01:01:53 Pavitra: Thank you Tony!! This was awesome 01:01:54 Jim Gruman: thank you!! 01:02:00 Andrew G. Farina: Thank you Tony! # 2021-10-05 Presentation with Simon Couch 00:18:39 Jon Harmon (jonthegeek): It&#39;s fun watching Simon&#39;s reactions to Asmae&#39;s presentation 00:25:40 Tony ElHabr: “the literature says” = max says 00:26:00 tan: &quot;the tidyverse says&quot; = Hadley says 00:29:06 Jon Harmon (jonthegeek): Current {stacks} version has elasticnet as default, not just lasso, Max! &lt;- note to the future 00:49:53 Simon Couch: https://github.com/tidymodels/stacks/issues/91 00:52:28 Bryan Shalloway: There is parsnip::null_model() that you can use for comparing against the mean... not sure if it&#39;s possible to specify something *slightly* more sophisticated there... 00:52:47 tan: itsmesleepknee 00:52:55 Jim Gruman: thank you Asmae and Simon!!! 00:54:47 Tony ElHabr: See y’all next time… whenever that is 00:56:55 Pavitra-Dallas: Thanks asmae and Simon 🤗🤗🤗 00:59:13 tan: thanks folks! Meeting chat log: SLICED Discussion 00:37:14 Bryan Shalloway: Curious if test set is a random sample? Or if it is segment time? 20.6.2 Cohort 3 Meeting chat log 00:11:22 Daniel Chen: hi. sorry i&#39;m late 00:12:14 Federica Gazzelloni: hello 00:14:27 Daniel Chen: it&#39;s retraining all the indibidual models again? 00:14:32 Daniel Chen: before adding the weights? 00:19:51 Federica Gazzelloni: hello Jiwan you are stack 🙂 00:45:17 Daniel Chen: lm_form_fit %&gt;% extract_fit_engine() %&gt;% vcov() #&gt; (Intercept) Longitude Latitude #&gt; (Intercept) 212.621 1.6113032 -1.4686377 #&gt; Longitude 1.611 0.0168166 -0.0008695 #&gt; Latitude -1.469 -0.0008695 0.0330019 00:45:23 Daniel Chen: https://www.tmwr.org/models.html#use-the-model-results 20.6.3 Cohort 4 Meeting chat log 00:39:59 Federica Gazzelloni: https://www.sciencedirect.com/topics/computer-science/ensemble-modeling#:~:text=Ensemble%20modeling%20is%20a%20process,prediction%20for%20the%20unseen%20data. 00:44:04 Federica Gazzelloni: https://towardsdatascience.com/ensemble-models-5a62d4f4cb0c 00:45:25 Isabella Velásquez: Ship it! "],["inferential-analysis.html", "Chapter 21 Inferential analysis", " Chapter 21 Inferential analysis Learning objectives: Use broom::tidy() to interpret the results of hypothesis tests. Use the {infer} package to test hypotheses. Use rsample::reg_intervals to compute bootstrap confidence intervals. Use dplyr::mutate() and purrr::map() to analyze parameter importance. "],["dataset-used-for-demonstrating-inference.html", "21.1 Dataset used for demonstrating inference", " 21.1 Dataset used for demonstrating inference I will use TidyTuesday dataset on ultra trail running races. The data comes from Benjamin Nowak by way of International Trail Running Association (ITRA). Their original repo is available on GitHub. race &lt;- read_csv(&quot;data/21_race.csv&quot;, show_col_types = FALSE) ranking &lt;- read_csv(&quot;data/21_ultra_rankings.csv&quot;, show_col_types = FALSE) best_results &lt;- ranking %&gt;% filter(rank &lt;= 10) %&gt;% group_by(race_year_id) %&gt;% summarise(time_in_seconds = mean(time_in_seconds), top_10 = n()) %&gt;% filter(top_10 == 10) %&gt;% select(-top_10) race_top_results &lt;- race %&gt;% filter(participation == &quot;solo&quot; || participation == &quot;Solo&quot;) %&gt;% inner_join(best_results, by = &quot;race_year_id&quot;) %&gt;% mutate(avg_elevation_gain = elevation_gain / distance, avg_velocity = distance / time_in_seconds * 3600) %&gt;% filter(distance &gt; 0) glimpse(race_top_results) ## Rows: 976 ## Columns: 16 ## $ race_year_id &lt;dbl&gt; 68140, 72496, 69855, 67856, 70469, 66887, 67851, 68… ## $ event &lt;chr&gt; &quot;Peak District Ultras&quot;, &quot;UTMB®&quot;, &quot;Grand Raid des Py… ## $ race &lt;chr&gt; &quot;Millstone 100&quot;, &quot;UTMB®&quot;, &quot;Ultra Tour 160&quot;, &quot;PERSEN… ## $ city &lt;chr&gt; &quot;Castleton&quot;, &quot;Chamonix&quot;, &quot;vielle-Aure&quot;, &quot;Asenovgrad… ## $ country &lt;chr&gt; &quot;United Kingdom&quot;, &quot;France&quot;, &quot;France&quot;, &quot;Bulgaria&quot;, &quot;… ## $ date &lt;date&gt; 2021-09-03, 2021-08-27, 2021-08-20, 2021-08-20, 20… ## $ start_time &lt;time&gt; 19:00:00, 17:00:00, 05:00:00, 18:00:00, 18:00:00, … ## $ participation &lt;chr&gt; &quot;solo&quot;, &quot;Solo&quot;, &quot;solo&quot;, &quot;solo&quot;, &quot;solo&quot;, &quot;solo&quot;, &quot;so… ## $ distance &lt;dbl&gt; 166.9, 170.7, 167.0, 164.0, 159.9, 159.9, 163.8, 16… ## $ elevation_gain &lt;dbl&gt; 4520, 9930, 9980, 7490, 100, 9850, 5460, 4630, 6410… ## $ elevation_loss &lt;dbl&gt; -4520, -9930, -9980, -7500, -100, -9850, -5460, -46… ## $ aid_stations &lt;dbl&gt; 10, 11, 13, 13, 12, 15, 5, 8, 13, 13, 12, 15, 0, 14… ## $ participants &lt;dbl&gt; 150, 2300, 600, 150, 0, 300, 0, 200, 120, 300, 100,… ## $ time_in_seconds &lt;dbl&gt; 113693.8, 79380.9, 103033.1, 90816.6, 79882.1, 1088… ## $ avg_elevation_gain &lt;dbl&gt; 27.0820851, 58.1722320, 59.7604790, 45.6707317, 0.6… ## $ avg_velocity &lt;dbl&gt; 5.284721, 7.741409, 5.835018, 6.501014, 7.206120, 5… We will work with races with non-0 distance, solo participation and at least 10 participants. For each race, the avg velocity is calculated from the velocity of the top 10 racers. set.seed(345129) "],["tidy-method-from-the-broom-package.html", "21.2 Tidy method from the {broom} package", " 21.2 Tidy method from the {broom} package predictable outcome for many different models and statistical tests always a tibble consistent column names most useful for analysing / visualizing multiple models/tests easier to combine results (no rownames) also used internally by higher level functions in tidymodels packages other packages also provide tidy methods for their own data structures different models, tests will have different structures based on what makes sense, but use as similar structure as possible You can get the same outcome from many different input formats. race_top_results %&gt;% ggplot(aes(avg_elevation_gain, avg_velocity)) + geom_point(alpha = 0.3) + geom_smooth(method = &quot;lm&quot;) + expand_limits(y = 0) As makes sense intuitively, higher elevation gain per mile results in lower velocity. lm_spec &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) wf &lt;- workflow() %&gt;% add_model(lm_spec) %&gt;% add_formula(avg_velocity ~ avg_elevation_gain + distance) fitted_wf &lt;- wf %&gt;% fit(race_top_results) fitted_wf %&gt;% tidy() ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 12.7 0.760 16.7 2.95e-55 ## 2 avg_elevation_gain -0.0518 0.00248 -20.9 2.34e-80 ## 3 distance -0.0247 0.00469 -5.27 1.64e- 7 lm_spec %&gt;% fit(avg_velocity ~ avg_elevation_gain + distance, data = race_top_results) %&gt;% tidy() ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 12.7 0.760 16.7 2.95e-55 ## 2 avg_elevation_gain -0.0518 0.00248 -20.9 2.34e-80 ## 3 distance -0.0247 0.00469 -5.27 1.64e- 7 {broom} existed before {tidymodels}, it works for base R lm model object as well. fitted_wf %&gt;% extract_fit_engine() %&gt;% tidy() ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 12.7 0.760 16.7 2.95e-55 ## 2 avg_elevation_gain -0.0518 0.00248 -20.9 2.34e-80 ## 3 distance -0.0247 0.00469 -5.27 1.64e- 7 lm(avg_velocity ~ avg_elevation_gain + distance, data = race_top_results) %&gt;% tidy() ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 12.7 0.760 16.7 2.95e-55 ## 2 avg_elevation_gain -0.0518 0.00248 -20.9 2.34e-80 ## 3 distance -0.0247 0.00469 -5.27 1.64e- 7 In addition to models, we can tidy the result of tests such as correlation test or t-test. cor.test(race_top_results$avg_velocity, race_top_results$avg_elevation_gain) %&gt;% tidy() ## # A tibble: 1 × 8 ## estimate statistic p.value parameter conf.low conf.high method alternative ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 -0.561 -21.2 4.01e-82 974 -0.603 -0.517 Pearson&#39;… two.sided t.test( race_top_results %&gt;% filter(date &gt; &#39;2015-01-01&#39;) %&gt;% pull(avg_velocity), race_top_results %&gt;% filter(date &lt;= &#39;2015-01-01&#39;) %&gt;% pull(avg_velocity) ) %&gt;% tidy() ## # A tibble: 1 × 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.417 6.78 7.20 -4.19 0.0000335 489. -0.613 -0.221 ## # ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt; "],["infer-for-simple-high-level-hypothesis-testing.html", "21.3 {infer} for simple, high level hypothesis testing", " 21.3 {infer} for simple, high level hypothesis testing specify relationship and optionally hypothesis calculate statistics from simulation or based on theoretical distributions many common tests are supported for continuous and discreet variables as well 21.3.1 p value for idependence based on simulation with permutation observed &lt;- race_top_results %&gt;% specify(avg_velocity ~ avg_elevation_gain) %&gt;% calculate(stat = &quot;correlation&quot;) observed ## Response: avg_velocity (numeric) ## Explanatory: avg_elevation_gain (numeric) ## # A tibble: 1 × 1 ## stat ## &lt;dbl&gt; ## 1 -0.561 permuted &lt;- race_top_results %&gt;% specify(avg_velocity ~ avg_elevation_gain) %&gt;% hypothesise(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;) %&gt;% calculate(stat = &quot;correlation&quot;) permuted ## Response: avg_velocity (numeric) ## Explanatory: avg_elevation_gain (numeric) ## Null Hypothesis: independence ## # A tibble: 1,000 × 2 ## replicate stat ## &lt;int&gt; &lt;dbl&gt; ## 1 1 -0.0108 ## 2 2 0.0193 ## 3 3 -0.00134 ## 4 4 -0.0564 ## 5 5 -0.0386 ## 6 6 -0.0142 ## 7 7 -0.0359 ## 8 8 -0.0313 ## 9 9 -0.00306 ## 10 10 -0.0488 ## # ℹ 990 more rows permuted %&gt;% visualize() + shade_p_value(observed, direction = &quot;two_sided&quot;) get_p_value(permuted, observed, direction = &quot;two_sided&quot;) ## # A tibble: 1 × 1 ## p_value ## &lt;dbl&gt; ## 1 0 21.3.2 Confidence interval for correlation based on simulation with bootstrapping bootstrapped &lt;- race_top_results %&gt;% specify(avg_velocity ~ avg_elevation_gain) %&gt;% generate(reps = 1000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;correlation&quot;) bootstrapped %&gt;% visualize() + shade_confidence_interval(get_confidence_interval(bootstrapped)) 21.3.3 Use theory instead of simulation observed_t &lt;- race_top_results %&gt;% specify(response = avg_velocity) %&gt;% hypothesise(null = &quot;point&quot;, mu = 7) %&gt;% calculate(stat = &quot;t&quot;) race_top_results %&gt;% specify(response = avg_velocity) %&gt;% assume(&quot;t&quot;) %&gt;% visualize() + shade_p_value(observed_t, direction = &quot;two_sided&quot;) race_top_results %&gt;% specify(response = avg_velocity) %&gt;% assume(&quot;t&quot;) %&gt;% get_p_value(observed_t, &quot;two_sided&quot;) ## # A tibble: 1 × 1 ## p_value ## &lt;dbl&gt; ## 1 0.0150 21.3.4 Linear models with multiple explanatory variables my_formula &lt;- as.formula(avg_velocity ~ aid_stations + participants) observed_fit &lt;- race_top_results %&gt;% specify(my_formula) %&gt;% fit() observed_fit ## # A tibble: 3 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 intercept 6.93 ## 2 aid_stations -0.0127 ## 3 participants 0.000432 permuted_fits &lt;- race_top_results %&gt;% specify(my_formula) %&gt;% hypothesise(null = &quot;independence&quot;) %&gt;% generate(reps = 1000, type = &quot;permute&quot;, variables = c(aid_stations, participants)) %&gt;% fit() bootstrapped_fits &lt;- race_top_results %&gt;% specify(my_formula) %&gt;% generate(reps = 2000, type = &quot;bootstrap&quot;) %&gt;% fit() permuted_fits %&gt;% get_p_value(observed_fit, &quot;two_sided&quot;) ## # A tibble: 3 × 2 ## term p_value ## &lt;chr&gt; &lt;dbl&gt; ## 1 aid_stations 0.058 ## 2 intercept 0.412 ## 3 participants 0.004 visualize(permuted_fits) + shade_p_value(observed_fit, &quot;two_sided&quot;) bootstrapped_fits %&gt;% get_confidence_interval(type = &quot;percentile&quot;, point_estimate = observed_fit) ## # A tibble: 3 × 3 ## term lower_ci upper_ci ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 aid_stations -0.0278 0.00223 ## 2 intercept 6.80 7.07 ## 3 participants 0.000123 0.000890 "],["reg_intervals-from-rsample.html", "21.4 reg_intervals from {rsample}", " 21.4 reg_intervals from {rsample} Similar purpose (?) as {infer} package, few supported models and interval types reg_intervals( my_formula, race_top_results, model_fn = &quot;glm&quot;, times = 2000, type = &quot;percentile&quot;, keep_reps = TRUE ) ## # A tibble: 2 × 7 ## term .lower .estimate .upper .alpha .method .replicates ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;list&lt;tibble[,2]&gt;&gt; ## 1 aid_stations -0.0286 -0.0128 0.00200 0.05 percentile [2,000 × 2] ## 2 participants 0.000122 0.000440 0.000904 0.05 percentile [2,000 × 2] "],["inference-with-lower-level-helpers.html", "21.5 Inference with lower level helpers", " 21.5 Inference with lower level helpers Most useful for models not supported by anova, or parsnip, etc or when you want to have more control. formulas &lt;- list( &quot;full&quot; = as.formula(avg_velocity ~ avg_elevation_gain + distance + aid_stations), &quot;partial&quot; = as.formula(avg_velocity ~ avg_elevation_gain + distance), &quot;minimal&quot; = as.formula(avg_velocity ~ avg_elevation_gain) ) lm_spec &lt;- linear_reg() %&gt;% set_engine(&quot;lm&quot;) AICs &lt;- map(formulas, function(formula) { lm_spec %&gt;% fit(formula, data = race_top_results) %&gt;% extract_fit_engine() %&gt;% AIC() }) AICs ## $full ## [1] 3123.333 ## ## $partial ## [1] 3122.497 ## ## $minimal ## [1] 3148.007 How to determine whether these differences are significant? One possible solution is bootstrapping which can also be used when there are no nice theoretical properties. velocity_model_summaries &lt;- race_top_results %&gt;% bootstraps(times = 1000, apparent = TRUE) %&gt;% mutate( full_aic = map_dbl(splits, ~ fit(lm_spec, formulas[[&quot;full&quot;]], data = analysis(.x)) %&gt;% extract_fit_engine() %&gt;% AIC()), partial_aic = map_dbl(splits, ~ fit(lm_spec, formulas[[&quot;partial&quot;]], data = analysis(.x)) %&gt;% extract_fit_engine() %&gt;% AIC()), minimal_aic = map_dbl(splits, ~ fit(lm_spec, formulas[[&quot;minimal&quot;]], data = analysis(.x)) %&gt;% extract_fit_engine() %&gt;% AIC()) ) %&gt;% select(full_aic, partial_aic, minimal_aic) velocity_model_summaries %&gt;% pivot_longer(c(full_aic, partial_aic, minimal_aic)) %&gt;% ggplot(aes(x = name, y = value)) + geom_boxplot() velocity_model_summaries %&gt;% summarize( full_vs_partial = mean(full_aic &lt; partial_aic), partial_vs_minimal = mean(partial_aic &lt; minimal_aic) ) ## # A tibble: 1 × 2 ## full_vs_partial partial_vs_minimal ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.377 0.916 velocity_model_summaries %&gt;% unnest(full_coeffs) %&gt;% ggplot(aes(x = estimate)) + geom_histogram(bins = 5) + facet_wrap(~term, scales = &quot;free_x&quot;) + geom_vline(xintercept = 0, col = &quot;green&quot;) Small reps used for speed race_top_results %&gt;% bootstraps(times = 20) %&gt;% mutate( full_coeffs = map(splits, ~ fit(lm_spec, formulas[[&quot;full&quot;]], data = analysis(.x)) %&gt;% tidy()) ) %&gt;% int_pctl(full_coeffs) ## # A tibble: 4 × 6 ## term .lower .estimate .upper .alpha .method ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 (Intercept) 9.89 12.9 15.5 0.05 percentile ## 2 aid_stations -0.0139 -0.00512 0.00607 0.05 percentile ## 3 avg_elevation_gain -0.0554 -0.0515 -0.0465 0.05 percentile ## 4 distance -0.0430 -0.0261 -0.00765 0.05 percentile The key components are bootstraps() and mutate + map which give you quite a bit of flexibility to compute many statistics. "],["meeting-videos-22.html", "21.6 Meeting Videos", " 21.6 Meeting Videos 21.6.1 Cohort 3 Meeting chat log 00:17:29 Federica Gazzelloni: tidy function if you google it, this is what it pops up at first: https://www.rdocumentation.org/packages/broom/versions/0.3.4/topics/tidy 00:53:19 Federica Gazzelloni: calculate function: https://www.rdocumentation.org/packages/infer/versions/0.5.4/topics/calculate 21.6.2 Cohort 4 Meeting chat log 00:33:49 Brandon Hurr: P-value 🤏 00:43:07 Federica Gazzelloni: moderndive.com 00:43:17 Federica Gazzelloni: https://infer.tidymodels.org/ 00:57:16 Federica Gazzelloni: https://juliasilge.com/blog/rstats-vignettes/ Meeting chat log 00:23:14 Federica Gazzelloni: total # of parameters: H(P+1)+H+1 00:24:15 Federica Gazzelloni: “For this type of network model and P predictors, there are a total of H(P + 1) + H + 1 total parameters being estimated, which quickly becomes large as P increases. (APM pg.143)&quot; 00:25:39 Federica Gazzelloni: The name neural network originally derived from thinking of these hidden units as analogous to neurons in the brain (ISLR pg 405) 00:46:35 Stephen.Charlesworth: I&#39;m installing rTools right now :) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
