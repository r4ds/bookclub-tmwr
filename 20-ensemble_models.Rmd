# Ensemble models

**Learning objectives:**

- Create a data stack for the `{stacks}` package using `stacks()` and `add_candidates`.
- Fit a **meta-learning model** using `blend_predictions()`.
- Fit the **member models** using `fit_members()`.
- Test the results of an ensemble model.

## Ensembling

- Aggregating predictions of multiple models together to make one prediction.
- We've already seen some ensembling within 1 type of model:
  - Random forest
  - Bagging & boosting
  
- **Model stacking** allows for aggregating many different types of models (lm + rf + svm, ...), to assemble a new model, which generates a new prediction, informed by its *members*.

## Ensembling with `stacks`!

![](https://raw.githubusercontent.com/tidymodels/stacks/main/man/figures/logo.png){width=25%}

https://stacks.tidymodels.org/articles/basics.html

Model stacking process:

1. Define some models, using all the knowledge we have from this book. We call them *candidate models/members*.
2. Initialize the ensemble with `stacks::stacks()`, and add the members to it, using `stacks::add_members()`
3. Blend the predictions of the members, using `stacks::blend_predictions()` (linear combination of each member's predictions)
4. Now that we know how to blend members, `fit` the members one more time on the whole training set, and `predict` on testing set.

## Define some models

- Ensembles are formed from model definitions, which are `workflow`s that contain model recipe & spec (that has been tuned or fit_resample'd).
- The book recommends the racing tuning method.

![](images/20_model-definitions.PNG)

- Youâ€™ll need to save the assessment set predictions and workflow utilized in your `tune_grid()`, or `fit_resamples()` objects by setting the control arguments `save_pred = TRUE` and `save_workflow = TRUE`. 

- If a model has hyperparameters, then you'd be creating multiple candidate models.

![](images/20_model-definitions-hyperparameters.PNG)

## Initialize and add members to stack.

- Data stacks are tibbles (with some extra attributes) that contain the response value as the first column, and the assessment set predictions for each candidate ensemble member.

![](images/20_model-stack.PNG)

```{r 20-stack_setup, eval=FALSE}
tree_frogs_model_st <- # example from stacks vignette
  stacks() %>%
  add_candidates(knn_res) %>%
  add_candidates(lin_reg_res) %>%
  add_candidates(svm_res)

tree_frogs_model_st
#> # A data stack with 3 model definitions and 11 candidate members:
#> #   knn_res: 4 model configurations
#> #   lin_reg_res: 1 model configuration
#> #   svm_res: 6 model configurations
#> # Outcome: latency (numeric)

as_tibble(tree_frogs_model_st)
#> # A tibble: 429 x 12
#>    latency knn_res_1_1 knn_res_1_2 knn_res_1_3 knn_res_1_4 lin_reg_res_1_1
#>      <dbl>       <dbl>       <dbl>       <dbl>       <dbl>           <dbl>
#>  1     142      -0.496      -0.478      -0.492      -0.494           114. 
#>  2      79      -0.381      -0.446      -0.542      -0.553            78.6
#>  3      50      -0.311      -0.352      -0.431      -0.438            81.5
#>  4      68      -0.312      -0.368      -0.463      -0.473            78.6
#>  5      64      -0.496      -0.478      -0.492      -0.494            36.5
#>  6      52      -0.391      -0.412      -0.473      -0.482           124. 
#>  7      39      -0.523      -0.549      -0.581      -0.587            35.2
#>  8      46      -0.523      -0.549      -0.581      -0.587            37.1
#>  9     137      -0.287      -0.352      -0.447      -0.456            78.8
#> 10      73      -0.523      -0.549      -0.581      -0.587            38.8
#> # â€¦ with 419 more rows, and 6 more variables: svm_res_1_1 <dbl>,
#> #   svm_res_1_4 <dbl>, svm_res_1_3 <dbl>, svm_res_1_5 <dbl>, svm_res_1_2 <dbl>,
#> #   svm_res_1_6 <dbl>
```

## Blend, fit, predict

- `blend_predictions()` performs LASSO regularization to combine the outputs from the stack members to come up with one final prediction.
- Candidates with non-zero coefficients are kept.

```{r 20-stack_blend, eval=FALSE}
tree_frogs_model_st <-
  tree_frogs_data_st %>%
  blend_predictions()
```

- There's an `autoplot()` function available, to see what's going on.
- If you don't like what you're seeing, you can try `blend_predictions()` again, and setting your own penalty argument.

![](images/20_autoplot.PNG)

- Essentially, what you have, is a linear combination of each member's prediction, to create one final prediction.

![](images/20_weights.PNG)

- With this "instruction" on how to combine candidate models, we fit the whole training set

```{r 20-stack_fit_members, eval=FALSE}
tree_frogs_model_st <-
  tree_frogs_model_st %>%
  fit_members()
```

![](images/20_blend_predictions.PNG)

- And predict on testing set

```{r 20-stack_predict, eval=FALSE}
tree_frogs_test <- 
  tree_frogs_test %>%
  bind_cols(predict(tree_frogs_model_st, .))
```


## Meeting Videos

### Cohort 1

`r knitr::include_url("https://www.youtube.com/embed/J8rZ1u4PhM8")`

`r knitr::include_url("https://www.youtube.com/embed/Pdm9G8nfR1g")`

`r knitr::include_url("https://www.youtube.com/embed/bUECbzZuIDY")`

<details>
  <summary> Meeting chat log </summary>
  
```
# 2021-05-25 Preview with Asmae Toumi
00:07:45	tan-iphone:	timed with an audience
00:08:00	tan-iphone:	did you get the cursive IDE set up?
00:11:00	Tony ElHabr:	yup! victor mono is the secret
00:18:44	tan-iphone:	foreach
00:18:47	tan-iphone:	or future or something
00:18:53	Marschall Furman (he/him):	parallel package
00:19:55	Jim Gruman:	all_cores <- parallelly::availableCores(omit = 1)
future::plan("multisession", workers = all_cores) # on Windows
00:47:55	tan-iphone:	"you'll learn when you grow up"

# 2021-06-01 Preview with Tony ElHabr
00:08:56	Jon Harmon:	Like tensorflow but very few insane installation issues: https://github.com/mlverse/torch
00:23:46	Daryn Ramsden:	Is it multinom_reg because thereâ€™s a logistic regression under the hood?
00:25:18	Jon Harmon:	I think so, as far as google is telling me.
00:27:19	Asmae Toumi:	Maxâ€™s quote: â€œOne other note that is in the upcoming ensemble chapter about racing and stacking... the stack can only use the candidates that had the complete set of resamples done. With racing, it works fast by avoiding using all of the resamples for candidates that show poor performance. If the racing method quickly gets down to one tuning parameter set, that is the only candidate that can be added to the data stack (since it has the whole set of resamples.â€
00:27:26	Pavitra:	Why is there a rank jump from 2  to 12?
00:27:51	Jon Harmon:	By model. It's showing the top 2 of each.
00:28:00	Daryn Ramsden:	ohhhh
00:28:35	Jon Harmon:	Er not by model per se but by... some combo....
00:28:41	Daryn Ramsden:	So models ranked 3-11 are also rf?
00:31:58	Asmae Toumi:	iâ€™d be curious to see the stacks results without tune race anova
00:33:16	Jon Harmon:	beepr always reminds me of this silliness that I had running a year and a half ago or so when Max had mentioned {tune} somewhere but the repo was still private... and it exactly solved something I was working on so I wanted to know as soon as it was public:
tune_check <- httr::GET(
  url = "https://github.com/tidymodels/tune"
)
while (httr::status_code(tune_check) == 404) {
  Sys.sleep(15*60)
  tune_check <- httr::GET(
    url = "https://github.com/tidymodels/tune"
  )
}
beepr::beep(8)
00:58:21	Asmae Toumi:	Tony hive forever
00:58:48	Asmae Toumi:	Can you send the code and data? I want to understand it better too
01:00:11	Pavitra:	Ledell
01:00:13	Asmae Toumi:	Erin LeDell is a legend
01:00:22	Asmae Toumi:	It was so funny
01:01:53	Pavitra:	Thank you Tony!! This was awesome
01:01:54	Jim Gruman:	thank you!!
01:02:00	Andrew G. Farina:	Thank you Tony!

# 2021-10-05 Presentation with Simon Couch
00:18:39	Jon Harmon (jonthegeek):	It's fun watching Simon's reactions to Asmae's presentation
00:25:40	Tony ElHabr:	â€œthe literature saysâ€ = max says
00:26:00	tan:	"the tidyverse says" = Hadley says
00:29:06	Jon Harmon (jonthegeek):	Current {stacks} version has elasticnet as default, not just lasso, Max! <- note to the future
00:49:53	Simon Couch:	https://github.com/tidymodels/stacks/issues/91
00:52:28	Bryan Shalloway:	There is parsnip::null_model() that you can use for comparing against the mean... not sure if it's possible to specify something *slightly* more sophisticated there...
00:52:47	tan:	itsmesleepknee
00:52:55	Jim Gruman:	thank you Asmae and Simon!!!
00:54:47	Tony ElHabr:	See yâ€™all next timeâ€¦ whenever that is
00:56:55	Pavitra-Dallas:	Thanks asmae and Simon ðŸ¤—ðŸ¤—ðŸ¤—
00:59:13	tan:	thanks folks!
```
</details>

`r knitr::include_url("https://www.youtube.com/embed/j2jWX8_1tYY")`

<details>
  <summary> Meeting chat log: SLICED Discussion </summary>
```
00:37:14	Bryan Shalloway:	Curious if test set is a random sample? Or if it is segment time?
```
</details>

### Cohort 3

`r knitr::include_url("https://www.youtube.com/embed/SBl2ccerzwo")`

<details>
  <summary> Meeting chat log </summary>
  
```
00:11:22	Daniel Chen:	hi. sorry i'm late
00:12:14	Federica Gazzelloni:	hello
00:14:27	Daniel Chen:	it's retraining all the indibidual models again?
00:14:32	Daniel Chen:	before adding the weights?
00:19:51	Federica Gazzelloni:	hello Jiwan you are stack ðŸ™‚
00:45:17	Daniel Chen:	lm_form_fit %>% extract_fit_engine() %>% vcov()
#>             (Intercept)  Longitude   Latitude
#> (Intercept)     212.621  1.6113032 -1.4686377
#> Longitude         1.611  0.0168166 -0.0008695
#> Latitude         -1.469 -0.0008695  0.0330019
00:45:23	Daniel Chen:	https://www.tmwr.org/models.html#use-the-model-results
```
</details>

### Cohort 4

`r knitr::include_url("https://www.youtube.com/embed/URL")`

<details>
  <summary> Meeting chat log </summary>
  
```
LOG
```
</details>
